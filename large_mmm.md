Methods Summary of Large Multi-Modality Model
==============================

## ``Catalogue ``
* [Large Language Model](#large-language-model)
* [Large Vision Model](#large-vision-model)
* [Large Region Multimodal Model](#large-region-multimodal-model)
* [Large Image Multimodal Model](#large-image-multimodal-model)
* [Large Video Multimodal Model](#large-video-multimodal-model)
* [Large Model Distillation](#large-modal-distillation)
* [Related Survey](#related-survey)
* [Related Benchmark](#related-benchmark)


### ``*Large Language Model*``

**(*arXiv2023_LLaMA*) LLaMA: Open and Efficient Foundation Language Models.** <br>
*Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, Guillaume Lample.*<br>
[[paper]](https://arxiv.org/abs/2302.13971)
[[code]](https://github.com/facebookresearch/llama)

**(*arXiv2023_RWKV*) RWKV: Reinventing RNNs for the Transformer Era.** <br>
*Bo Peng, Eric Alcaide, Quentin Anthony, Alon Albalak, Samuel Arcadinho, Stella Biderman, Huanqi Cao, Xin Cheng, Michael Chung, Matteo Grella, Kranthi Kiran GV, Xuzheng He, Haowen Hou, Jiaju Lin, Przemyslaw Kazienko, Jan Kocon, Jiaming Kong, Bartlomiej Koptyra, Hayden Lau, Krishna Sri Ipsit Mantri, Ferdinand Mom, Atsushi Saito, Guangyu Song, Xiangru Tang, Bolun Wang, Johan S. Wind, Stanislaw Wozniak, Ruichong Zhang, Zhenyuan Zhang, Qihang Zhao, Peng Zhou, Qinghua Zhou, Jian Zhu, Rui-Jie Zhu.*<br>
[[paper]](https://arxiv.org/abs/2305.13048)
[[code]](https://github.com/BlinkDL/RWKV-LM)

**(*arXiv2023_RETNET*) Retentive Network: A Successor to Transformer for Large Language Models.** <br>
*Yutao Sun, Li Dong, Shaohan Huang, Shuming Ma, Yuqing Xia, Jilong Xue, Jianyong Wang, Furu Wei.*<br>
[[paper]](https://arxiv.org/abs/2307.08621)
[[code]](https://aka.ms/retnet)

**(*arXiv2023_Llama 2*) Llama 2: Open Foundation and Fine-Tuned Chat Models.** <br>
*Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, Thomas Scialom.*<br>
[[paper]](https://arxiv.org/abs/2307.09288)
[[code]](https://github.com/facebookresearch/llama)

**(*arXiv2023_InternLM*) InternLM: A Multilingual Language Model with Progressively Enhanced Capabilities.** <br>
*InternLM Team.*<br>
[[paper]](https://github.com/InternLM/InternLM-techreport/blob/main/InternLM.pdf)
[[code]](https://github.com/InternLM/InternLM)

**(*arXiv2023_Mamba*) Mamba: Linear-Time Sequence Modeling with Selective State Spaces.** <br>
*Albert Gu, Tri Dao.*<br>
[[paper]](https://arxiv.org/abs/2312.00752)
[[code]](https://github.com/state-spaces/mamba)


### ``*Large Vision Model*``

**(*ICCV2021_ViViT*) ViViT: A Video Vision Transformer.** <br>
*Anurag Arnab, Mostafa Dehghani, Georg Heigold, Chen Sun, Mario Lučić, Cordelia Schmid.*<br>
[[paper]](https://arxiv.org/abs/2103.15691)
[[code]](https://github.com/google-research/scenic/tree/main/scenic/projects/vivit)

**(*ICLR2023_ToME*) Token Merging: Your ViT But Faster.** <br>
*Daniel Bolya, Cheng-Yang Fu, Xiaoliang Dai, Peizhao Zhang, Christoph Feichtenhofer, Judy Hoffman.*<br>
[[paper]](https://arxiv.org/abs/2210.09461)
[[code]](https://github.com/facebookresearch/tome)

**(*CVPR2023_EVA*) EVA: Exploring the Limits of Masked Visual Representation Learning at Scale.** <br>
*Yuxin Fang, Wen Wang, Binhui Xie, Quan Sun, Ledell Wu, Xinggang Wang, Tiejun Huang, Xinlong Wang, Yue Cao.*<br>
[[paper]](https://arxiv.org/abs/2211.07636)
[[code]](https://github.com/baaivision/EVA)

**(*CVPR2023_Painter*) Images Speak in Images: A Generalist Painter for In-Context Visual Learning.** <br>
*Xinlong Wang, Wen Wang, Yue Cao, Chunhua Shen, Tiejun Huang.*<br>
[[paper]](https://arxiv.org/abs/2212.02499)
[[code]](https://github.com/baaivision/Painter)

**(*CVPR2023_MAGVIT*) MAGVIT: Masked Generative Video Transformer.** <br>
*Lijun Yu, Yong Cheng, Kihyuk Sohn, José Lezama, Han Zhang, Huiwen Chang, Alexander G. Hauptmann, Ming-Hsuan Yang, Yuan Hao, Irfan Essa, Lu Jiang.*<br>
[[paper]](https://arxiv.org/abs/2212.05199)
[[code]](https://magvit.cs.cmu.edu/)

**(*arXiv2023_EVA-02*) EVA-02: A Visual Representation for Neon Genesis.** <br>
*Yuxin Fang, Quan Sun, Xinggang Wang, Tiejun Huang, Xinlong Wang, Yue Cao.*<br>
[[paper]](https://arxiv.org/abs/2303.11331)
[[code]](https://github.com/baaivision/EVA/tree/master/EVA-02)

**(*arXiv2023_EVA-CLIP*) EVA-CLIP: Improved Training Techniques for CLIP at Scale.** <br>
*Quan Sun, Yuxin Fang, Ledell Wu, Xinlong Wang, Yue Cao.*<br>
[[paper]](https://arxiv.org/abs/2303.15389)
[[code]](https://github.com/baaivision/EVA/tree/master/EVA-CLIP)

**(*CVPR2023_VideoMAEv2*) VideoMAE V2: Scaling Video Masked Autoencoders with Dual Masking.** <br>
*Limin Wang, Bingkun Huang, Zhiyu Zhao, Zhan Tong, Yinan He, Yi Wang, Yali Wang, Yu Qiao.*<br>
[[paper]](https://arxiv.org/abs/2303.16727)
[[code]](https://github.com/OpenGVLab/VideoMAEv2)

**(*ICCV2023_SegGPT*) SegGPT: Segmenting Everything In Context.** <br>
*Xinlong Wang, Xiaosong Zhang, Yue Cao, Wen Wang, Chunhua Shen, Tiejun Huang.*<br>
[[paper]](https://arxiv.org/abs/2304.03284)
[[code]](https://github.com/baaivision/Painter)

**(*ICLR2024_MAGVITv2*) Language Model Beats Diffusion -- Tokenizer is Key to Visual Generation.** <br>
*Lijun Yu, José Lezama, Nitesh B. Gundavarapu, Luca Versari, Kihyuk Sohn, David Minnen, Yong Cheng, Agrim Gupta, Xiuye Gu, Alexander G. Hauptmann, Boqing Gong, Ming-Hsuan Yang, Irfan Essa, David A. Ross, Lu Jiang.*<br>
[[paper]](https://arxiv.org/abs/2310.05737)

**(*arXiv2024_AIM*) Scalable Pre-training of Large Autoregressive Image Models.** <br>
*Alaaeldin El-Nouby, Michal Klein, Shuangfei Zhai, Miguel Angel Bautista, Alexander Toshev, Vaishaal Shankar, Joshua M Susskind, Armand Joulin.*<br>
[[paper]](https://arxiv.org/abs/2401.08541)
[[code]](https://github.com/apple/ml-aim)

**(*arXiv2024_VIM*) Vision Mamba: Efficient Visual Representation Learning with Bidirectional State Space Model.** <br>
*Lianghui Zhu, Bencheng Liao, Qian Zhang, Xinlong Wang, Wenyu Liu, Xinggang Wang.*<br>
[[paper]](https://arxiv.org/abs/2401.09417)
[[code]](https://github.com/hustvl/Vim)

**(*arXiv2024_EVA-CLIP-18B*) EVA-CLIP-18B: Scaling CLIP to 18 Billion Parameters.** <br>
*Quan Sun, Jinsheng Wang, Qiying Yu, Yufeng Cui, Fan Zhang, Xiaosong Zhang, Xinlong Wang.*<br>
[[paper]](https://arxiv.org/abs/2402.04252)
[[code]](https://github.com/baaivision/EVA/tree/master/EVA-CLIP-18B)

**(*arXiv2024_VisionLLaMA*) VisionLLaMA: A Unified LLaMA Interface for Vision Tasks.** <br>
*Xiangxiang Chu, Jianlin Su, Bo Zhang, Chunhua Shen.*<br>
[[paper]](https://arxiv.org/abs/2403.00522)
[[code]](https://github.com/Meituan-AutoML/VisionLLaMA)

**(*arXiv2024_Vision-RWKV*) Vision-RWKV: Efficient and Scalable Visual Perception with RWKV-Like Architectures.** <br>
*Yuchen Duan, Weiyun Wang, Zhe Chen, Xizhou Zhu, Lewei Lu, Tong Lu, Yu Qiao, Hongsheng Li, Jifeng Dai, Wenhai Wang.*<br>
[[paper]](https://arxiv.org/abs/2403.02308)
[[code]](https://github.com/OpenGVLab/Vision-RWKV)

**(*arXiv2024_VideoMamba*) VideoMamba: State Space Model for Efficient Video Understanding.** <br>
*Kunchang Li, Xinhao Li, Yi Wang, Yinan He, Yali Wang, Limin Wang, Yu Qiao.*<br>
[[paper]](https://arxiv.org/abs/2403.06977)
[[code]](https://github.com/opengvlab/videomamba)

**(*arXiv2024_VAR*) Visual Autoregressive Modeling: Scalable Image Generation via Next-Scale Prediction.** <br>
*Keyu Tian, Yi Jiang, Zehuan Yuan, Bingyue Peng, Liwei Wang.*<br>
[[paper]](https://arxiv.org/abs/2404.02905)
[[code]](https://github.com/FoundationVision/VAR)

**(*arXiv2024_Ctrl-Adapter*) Ctrl-Adapter: An Efficient and Versatile Framework for Adapting Diverse Controls to Any Diffusion Model.** <br>
*Han Lin, Jaemin Cho, Abhay Zala, Mohit Bansal.*<br>
[[paper]](https://arxiv.org/abs/2404.09967)
[[code]](https://ctrl-adapter.github.io/)


### ``*Large Region Multimodal Model*``

**(*arXiv2023_SoM*) Set-of-Mark Prompting Unleashes Extraordinary Visual Grounding in GPT-4V.** <br>
*Jianwei Yang, Hao Zhang, Feng Li, Xueyan Zou, Chunyuan Li, Jianfeng Gao.*<br>
[[paper]](https://arxiv.org/abs/2310.11441)
[[code]](https://github.com/microsoft/SoM)

**(*CVPR2024_GLaMM*) GLaMM: Pixel Grounding Large Multimodal Model.** <br>
*Hanoona Rasheed, Muhammad Maaz, Sahal Shaji Mullappilly, Abdelrahman Shaker, Salman Khan, Hisham Cholakkal, Rao M. Anwer, Erix Xing, Ming-Hsuan Yang, Fahad S. Khan.*<br>
[[paper]](https://arxiv.org/abs/2311.03356)
[[code]](https://mbzuai-oryx.github.io/groundingLMM)

**(*arXiv2023_PG-Video-LLaVA*) PG-Video-LLaVA: Pixel Grounding Large Video-Language Models.** <br>
*Shehan Munasinghe, Rusiru Thushara, Muhammad Maaz, Hanoona Abdul Rasheed, Salman Khan, Mubarak Shah, Fahad Khan.*<br>
[[paper]](https://arxiv.org/abs/2311.13435)
[[code]](https://github.com/mbzuai-oryx/Video-LLaVA)

**(*arXiv2023_DINOv*) Visual In-Context Prompting.** <br>
*Feng Li, Qing Jiang, Hao Zhang, Tianhe Ren, Shilong Liu, Xueyan Zou, Huaizhe Xu, Hongyang Li, Chunyuan Li, Jianwei Yang, Lei Zhang, Jianfeng Gao.*<br>
[[paper]](https://arxiv.org/abs/2311.13601)
[[code]](https://github.com/UX-Decoder/DINOv)

**(*arXiv2023_TAP*) Tokenize Anything via Prompting.** <br>
*Ting Pan, Lulu Tang, Xinlong Wang, Shiguang Shan.*<br>
[[paper]](https://arxiv.org/abs/2312.09128)
[[code]](https://github.com/baaivision/tokenize-anything)

**(*CVPR2024_Emu2*) Generative Multimodal Models are In-Context Learners.** <br>
*Quan Sun, Yufeng Cui, Xiaosong Zhang, Fan Zhang, Qiying Yu, Zhengxiong Luo, Yueze Wang, Yongming Rao, Jingjing Liu, Tiejun Huang, Xinlong Wang.*<br>
[[paper]](https://arxiv.org/abs/2312.13286)
[[code]](https://baaivision.github.io/emu2)


### ``*Large Image Multimodal Model*``

**(*ICCV2023_DiT*) Scalable Diffusion Models with Transformers.** <br>
*William Peebles, Saining Xie.*<br>
[[paper]](https://arxiv.org/abs/2212.09748)
[[code]](https://www.wpeebles.com/DiT)

**(*ICML2023_mPLUG-2*) mPLUG-2: A Modularized Multi-modal Foundation Model Across Text, Image and Video.** <br>
*Haiyang Xu, Qinghao Ye, Ming Yan, Yaya Shi, Jiabo Ye, Yuanhong Xu, Chenliang Li, Bin Bi, Qi Qian, Wei Wang, Guohai Xu, Ji Zhang, Songfang Huang, Fei Huang, Jingren Zhou.*<br>
[[paper]](https://arxiv.org/abs/2302.00402v1)
[[code]](https://github.com/alibaba/AliceMind)

**(*ICCV2023_ControlNet*) Adding Conditional Control to Text-to-Image Diffusion Models.** <br>
*Lvmin Zhang, Anyi Rao, Maneesh Agrawala.*<br>
[[paper]](https://arxiv.org/abs/2302.05543)
[[code]](https://github.com/lllyasviel/ControlNet)

**(*ICLR2023_Emu*) Generative Pretraining in Multimodality.** <br>
*Quan Sun, Qiying Yu, Yufeng Cui, Fan Zhang, Xiaosong Zhang, Yueze Wang, Hongcheng Gao, Jingjing Liu, Tiejun Huang, Xinlong Wang.*<br>
[[paper]](https://arxiv.org/abs/2307.05222)
[[code]](https://github.com/baaivision/Emu)

**(*arXiv2023_Qwen-VL*) Qwen-VL: A Versatile Vision-Language Model for Understanding, Localization, Text Reading, and Beyond.** <br>
*Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, Jingren Zhou.*<br>
[[paper]](https://arxiv.org/abs/2308.12966)
[[code]](https://github.com/QwenLM/Qwen-VL)

**(*arXiv2023_InternLM-XComposer*) InternLM-XComposer: A Vision-Language Large Model for Advanced Text-image Comprehension and Composition.** <br>
*Pan Zhang, Xiaoyi Dong, Bin Wang, Yuhang Cao, Chao Xu, Linke Ouyang, Zhiyuan Zhao, Haodong Duan, Songyang Zhang, Shuangrui Ding, Wenwei Zhang, Hang Yan, Xinyue Zhang, Wei Li, Jingwen Li, Kai Chen, Conghui He, Xingcheng Zhang, Yu Qiao, Dahua Lin, Jiaqi Wang.*<br>
[[paper]](https://arxiv.org/abs/2309.15112)
[[code]](https://github.com/InternLM/InternLM-XComposer)

**(*arXiv2023_LLaVA1.5*) Improved Baselines with Visual Instruction Tuning.** <br>
*Haotian Liu, Chunyuan Li, Yuheng Li, Yong Jae Lee.*<br>
[[blog]](https://arxiv.org/abs/2310.03744)
[[code]](https://llava-vl.github.io/)

**(*CVPR2024_CapsFusion*) CapsFusion: Rethinking Image-Text Data at Scale.** <br>
*Qiying Yu, Quan Sun, Xiaosong Zhang, Yufeng Cui, Fan Zhang, Yue Cao, Xinlong Wang, Jingjing Liu.*<br>
[[blog]](https://arxiv.org/abs/2310.20550)
[[code]](https://github.com/baaivision/CapsFusion)

**(*blog2023_Fuyu-8B*) Fuyu-8B: A Multimodal Architecture for AI Agents.** <br>
*Rohan Bavishi, Erich Elsen, Curtis Hawthorne, Maxwell Nye, Augustus Odena, Arushi Somani, Sağnak Taşırlar.*<br>
[[blog]](https://www.adept.ai/blog/fuyu-8b)

**(*arXiv2023_CogVLM*) CogVLM: Visual Expert for Pretrained Language Models.** <br>
*Weihan Wang, Qingsong Lv, Wenmeng Yu, Wenyi Hong, Ji Qi, Yan Wang, Junhui Ji, Zhuoyi Yang, Lei Zhao, Xixuan Song, Jiazheng Xu, Bin Xu, Juanzi Li, Yuxiao Dong, Ming Ding, Jie Tang.*<br>
[[paper]](https://arxiv.org/abs/2311.03079)
[[code]](https://github.com/THUDM/CogVLM)

**(*arXiv2023_OtterHD*) OtterHD: A High-Resolution Multi-modality Model.** <br>
*Bo Li, Peiyuan Zhang, Jingkang Yang, Yuanhan Zhang, Fanyi Pu, Ziwei Liu.*<br>
[[paper]](https://arxiv.org/abs/2311.04219)
[[code]](https://github.com/Luodian/Otter)

**(*CVPR2024_Monkey*) Monkey: Image Resolution and Text Label Are Important Things for Large Multi-modal Models.** <br>
*Zhang Li, Biao Yang, Qiang Liu, Zhiyin Ma, Shuo Zhang, Jingxu Yang, Yabo Sun, Yuliang Liu, Xiang Bai.*<br>
[[paper]](https://arxiv.org/abs/2311.06607)
[[code]](https://github.com/Yuliang-Liu/Monkey)

**(*arXiv2023_LVIS-Instruct4V*) To See is to Believe: Prompting GPT-4V for Better Visual Instruction Tuning.** <br>
*Junke Wang, Lingchen Meng, Zejia Weng, Bo He, Zuxuan Wu, Yu-Gang Jiang.*<br>
[[paper]](https://arxiv.org/abs/2311.07574)
[[code]](https://github.com/X2FD/LVIS-INSTRUCT4V)

**(*arXiv2023_ShareGPT4V*) ShareGPT4V: Improving Large Multi-Modal Models with Better Captions.** <br>
*Lin Chen, Jinsong Li, Xiaoyi Dong, Pan Zhang, Conghui He, Jiaqi Wang, Feng Zhao, Dahua Lin.*<br>
[[paper]](https://arxiv.org/abs/2311.12793)
[[code]](https://sharegpt4v.github.io/)

**(*CVPR2024_Powers-of-Ten*) Generative Powers of Ten.** <br>
*Xiaojuan Wang, Janne Kontkanen, Brian Curless, Steve Seitz, Ira Kemelmacher, Ben Mildenhall, Pratul Srinivasan, Dor Verbin, Aleksander Holynski.*<br>
[[paper]](https://arxiv.org/abs/2312.02149)
[[code]](https://powers-of-10.github.io/)

**(*CVPR2024_OneLLM*) OneLLM: One Framework to Align All Modalities with Language.** <br>
*Jiaming Han, Kaixiong Gong, Yiyuan Zhang, Jiaqi Wang, Kaipeng Zhang, Dahua Lin, Yu Qiao, Peng Gao, Xiangyu Yue.*<br>
[[paper]](https://arxiv.org/abs/2312.03700)
[[code]](https://github.com/csuhan/OneLLM)

**(*CVPR2024_Honeybee*) Honeybee: Locality-enhanced Projector for Multimodal LLM.** <br>
*Junbum Cha, Wooyoung Kang, Jonghwan Mun, Byungseok Roh.*<br>
[[paper]](https://arxiv.org/abs/2312.06742)
[[code]](https://github.com/kakaobrain/honeybee)

**(*arXiv2023_CogAgent*) CogAgent: A Visual Language Model for GUI Agents.** <br>
*Wenyi Hong, Weihan Wang, Qingsong Lv, Jiazheng Xu, Wenmeng Yu, Junhui Ji, Yan Wang, Zihan Wang, Yuxuan Zhang, Juanzi Li, Bin Xu, Yuxiao Dong, Ming Ding, Jie Tang.*<br>
[[paper]](https://arxiv.org/abs/2312.08914)
[[code]](https://github.com/THUDM/CogVLM)

**(*arXiv2024_InternLM-XComposer2*) InternLM-XComposer2: Mastering Free-form Text-Image Composition and Comprehension in Vision-Language Large Model.** <br>
*Xiaoyi Dong, Pan Zhang, Yuhang Zang, Yuhang Cao, Bin Wang, Linke Ouyang, Xilin Wei, Songyang Zhang, Haodong Duan, Maosong Cao, Wenwei Zhang, Yining Li, Hang Yan, Yang Gao, Xinyue Zhang, Wei Li, Jingwen Li, Kai Chen, Conghui He, Xingcheng Zhang, Yu Qiao, Dahua Lin, Jiaqi Wang.*<br>
[[paper]](https://arxiv.org/abs/2401.16420)
[[code]](https://github.com/InternLM/InternLM-XComposer)

**(*arXiv2024_CoBSAT*) Can MLLMs Perform Text-to-Image In-Context Learning?.** <br>
*Yuchen Zeng, Wonjun Kang, Yicong Chen, Hyung Il Koo, Kangwook Lee.*<br>
[[paper]](https://arxiv.org/abs/2402.01293)
[[code]](https://github.com/UW-Madison-Lee-Lab/CoBSAT)

**(*arXiv2024_Bunny*) Efficient Multimodal Learning from Data-centric Perspective.** <br>
*Muyang He, Yexin Liu, Boya Wu, Jianhao Yuan, Yueze Wang, Tiejun Huang, Bo Zhao.*<br>
[[paper]](https://arxiv.org/abs/2402.11530)
[[code]](https://github.com/BAAI-DCAI/Bunny)

**(*arXiv2024_LLaVA-PruMerge*) LLaVA-PruMerge: Adaptive Token Reduction for Efficient Large Multimodal Models.** <br>
*Yuzhang Shang, Mu Cai, Bingxin Xu, Yong Jae Lee, Yan Yan.*<br>
[[paper]](https://arxiv.org/abs/2403.15388)
[[code]](https://llava-prumerge.github.io/)

**(*arXiv2024_InternLM-XComposer2-4KHD*) InternLM-XComposer2-4KHD: A Pioneering Large Vision-Language Model Handling Resolutions from 336 Pixels to 4K HD.** <br>
*Xiaoyi Dong, Pan Zhang, Yuhang Zang, Yuhang Cao, Bin Wang, Linke Ouyang, Songyang Zhang, Haodong Duan, Wenwei Zhang, Yining Li, Hang Yan, Yang Gao, Zhe Chen, Xinyue Zhang, Wei Li, Jingwen Li, Wenhai Wang, Kai Chen, Conghui He, Xingcheng Zhang, Jifeng Dai, Yu Qiao, Dahua Lin, Jiaqi Wang.*<br>
[[paper]](https://arxiv.org/abs/2404.06512)
[[code]](https://github.com/InternLM/InternLM-XComposer)

**(*arXiv2024_BRAVE*) BRAVE: Broadening the visual encoding of vision-language models.** <br>
*Oğuzhan Fatih Kar, Alessio Tonioni, Petra Poklukar, Achin Kulshrestha, Amir Zamir, Federico Tombari.*<br>
[[paper]](https://arxiv.org/abs/2404.07204)
[[code]](https://brave-vlms.epfl.ch/)


### ``*Large Video Multimodal Model*``

**(*arXiv2022_InternVideo*) InternVideo: General Video Foundation Models via Generative and Discriminative Learning.** <br>
*Yi Wang, Kunchang Li, Yizhuo Li, Yinan He, Bingkun Huang, Zhiyu Zhao, Hongjie Zhang, Jilan Xu, Yi Liu, Zun Wang, Sen Xing, Guo Chen, Junting Pan, Jiashuo Yu, Yali Wang, Limin Wang, Yu Qiao.*<br>
[[paper]](https://arxiv.org/abs/2212.03191)
[[code]](https://github.com/OpenGVLab/InternVideo)

**(*arXiv2022_VideoCoCa*) VideoCoCa: Video-Text Modeling with Zero-Shot Transfer from Contrastive Captioners.** <br>
*Shen Yan, Tao Zhu, Zirui Wang, Yuan Cao, Mi Zhang, Soham Ghosh, Yonghui Wu, Jiahui Yu.*<br>
[[paper]](https://arxiv.org/abs/2212.04979v3)

**(*arXiv2023_VideoChat*) VideoChat: Chat-Centric Video Understanding.** <br>
*KunChang Li, Yinan He, Yi Wang, Yizhuo Li, Wenhai Wang, Ping Luo, Yali Wang, Limin Wang, Yu Qiao.*<br>
[[paper]](https://arxiv.org/abs/2305.06355)
[[code]](https://github.com/opengvlab/ask-anything)

**(*EMNLP2023_Video-LLaMA*) Video-LLaMA: An Instruction-tuned Audio-Visual Language Model for Video Understanding.** <br>
*Hang Zhang, Xin Li, Lidong Bing.*<br>
[[paper]](https://arxiv.org/abs/2306.02858)
[[code]](https://github.com/damo-nlp-sg/video-llama)

**(*arXiv2023_Video-ChatGPT*) Video-ChatGPT: Towards Detailed Video Understanding via Large Vision and Language Models.** <br>
*Muhammad Maaz, Hanoona Rasheed, Salman Khan, Fahad Shahbaz Khan.*<br>
[[paper]](https://arxiv.org/abs/2306.05424)
[[code]](https://github.com/mbzuai-oryx/Video-ChatGPT)

**(*arXiv2023_Valley*) Valley: Video Assistant with Large Language model Enhanced abilitY.** <br>
*Ruipu Luo, Ziwang Zhao, Min Yang, Junwei Dong, Da Li, Pengcheng Lu, Tao Wang, Linmei Hu, Minghui Qiu, Zhongyu Wei.*<br>
[[paper]](https://arxiv.org/abs/2306.07207)
[[code]](https://github.com/rupertluo/valley)

**(*CVPR2024_MovieChat*) MovieChat: From Dense Token to Sparse Memory for Long Video Understanding.** <br>
*Enxin Song, Wenhao Chai, Guanhong Wang, Yucheng Zhang, Haoyang Zhou, Feiyang Wu, Haozhe Chi, Xun Guo, Tian Ye, Yanting Zhang, Yan Lu, Jenq-Neng Hwang, Gaoang Wang.*<br>
[[paper]](https://arxiv.org/abs/2307.16449)
[[code]](https://github.com/rese1f/MovieChat)

**(*EMNLP2023_TESTA*) TESTA: Temporal-Spatial Token Aggregation for Long-form Video-Language Understanding.** <br>
*Shuhuai Ren, Sishuo Chen, Shicheng Li, Xu Sun, Lu Hou.*<br>
[[paper]](https://arxiv.org/abs/2310.19060)
[[code]](https://github.com/RenShuhuai-Andy/TESTA)

**(*CVPR2024_Chat-UniVi*) Chat-UniVi: Unified Visual Representation Empowers Large Language Models with Image and Video Understanding.** <br>
*Peng Jin, Ryuichi Takanobu, Caiwan Zhang, Xiaochun Cao, Li Yuan.*<br>
[[paper]](https://arxiv.org/abs/2311.08046)
[[code]](https://github.com/PKU-YuanGroup/Chat-UniVi)

**(*arXiv2023_VideoChat2*) MVBench: A Comprehensive Multi-modal Video Understanding Benchmark.** <br>
*Kunchang Li, Yali Wang, Yinan He, Yizhuo Li, Yi Wang, Yi Liu, Zun Wang, Jilan Xu, Guo Chen, Ping Luo, Limin Wang, Yu Qiao.*<br>
[[paper]](https://arxiv.org/abs/2311.17005)
[[code]](https://github.com/OpenGVLab/Ask-Anything)

**(*arXiv2023_LLaMA-VID*) LLaMA-VID: An Image is Worth 2 Tokens in Large Language Models.** <br>
*Yanwei Li, Chengyao Wang, Jiaya Jia.*<br>
[[paper]](https://arxiv.org/abs/2311.17043)
[[code]](https://github.com/dvlab-research/LLaMA-VID)

**(*arXiv2024_LSTP*) LSTP: Language-guided Spatial-Temporal Prompt Learning for Long-form Video-Text Understanding.** <br>
*Yuxuan Wang, Yueqian Wang, Pengfei Wu, Jianxin Liang, Dongyan Zhao, Zilong Zheng.*<br>
[[paper]](https://arxiv.org/abs/2402.16050)
[[code]](https://github.com/bigai-nlco/LSTP-Chat)


### ``*Large Model Distillation*``

**(*EMNLP2016_Seq-KD*) Sequence-Level Knowledge Distillation.** <br>
*Yoon Kim, Alexander M. Rush.*<br>
[[paper]](https://arxiv.org/abs/1606.07947)
[[code]](https://github.com/harvardnlp/seq2seq-attn)

**(*arXiv2020_ImitKD*) Autoregressive Knowledge Distillation through Imitation Learning.** <br>
*Alexander Lin, Jeremy Wohlwend, Howard Chen, Tao Lei.*<br>
[[paper]](https://arxiv.org/abs/2009.07253)
[[code]](https://github.com/asappresearch/imitkd)

**(*ICLR2024_MINILLM*) MINILLM: Knowledge Distillation of Large Language Models.** <br>
*Yuxian Gu, Li Dong, Furu Wei, Minlie Huang.*<br>
[[paper]](https://arxiv.org/abs/2306.08543)
[[code]](https://aka.ms/MiniLLM)

**(*ICLR2024_GKD*) On-Policy Distillation of Language Models: Learning from Self-Generated Mistakes.** <br>
*Rishabh Agarwal, Nino Vieillard, Yongchao Zhou, Piotr Stanczyk, Sabela Ramos, Matthieu Geist, Olivier Bachem.*<br>
[[paper]](https://arxiv.org/abs/2306.13649)
[[code]](https://github.com/microsoft/LMOps/tree/main/minillm)

**(*ACL2023_f-DISTILL*) f-Divergence Minimization for Sequence-Level Knowledge Distillation.** <br>
*Yuqiao Wen, Zichao Li, Wenyu Du, Lili Mou.*<br>
[[paper]](https://arxiv.org/abs/2307.15190)
[[code]](https://github.com/MANGA-UOFA/fdistill)

**(*arXiv2023_DistillSpec*) DistillSpec: Improving Speculative Decoding via Knowledge Distillation.** <br>
*Yongchao Zhou, Kaifeng Lyu, Ankit Singh Rawat, Aditya Krishna Menon, Afshin Rostamizadeh, Sanjiv Kumar, Jean-François Kagy, Rishabh Agarwal.*<br>
[[paper]](https://arxiv.org/abs/2310.08461)

**(*arXiv2023_MiniMA*) Towards the Law of Capacity Gap in Distilling Language Models.** <br>
*Chen Zhang, Dawei Song, Zheyu Ye, Yan Gao.*<br>
[[paper]](https://arxiv.org/abs/2311.07052)
[[code]](https://github.com/GeneZC/MiniMA)

**(*arXiv2024_Self-Rewarding*) Self-Rewarding Language Models.** <br>
*Weizhe Yuan, Richard Yuanzhe Pang, Kyunghyun Cho, Sainbayar Sukhbaatar, Jing Xu, Jason Weston.*<br>
[[paper]](https://arxiv.org/abs/2401.10020)


### ``*Related Survey*``

**(*arXiv2020_Survey*) Efficient Transformers: A Survey.** <br>
*Yi Tay, Mostafa Dehghani, Dara Bahri, Donald Metzler.*<br>
[[paper]](https://arxiv.org/abs/2009.06732)

**(*arXiv2023_Survey*) A Survey of Chain of Thought Reasoning: Advances, Frontiers and Future.** <br>
*Zheng Chu, Jingchang Chen, Qianglong Chen, Weijiang Yu, Tao He, Haotian Wang, Weihua Peng, Ming Liu, Bing Qin, Ting Liu.*<br>
[[paper]](https://arxiv.org/abs/2309.15402)
[[code]](https://github.com/zchuz/CoT-Reasoning-Survey)


### ``*Related Benchmark*``

**(*arXiv2023_MagnifierBench*) OtterHD: A High-Resolution Multi-modality Model.** <br>
*Bo Li, Peiyuan Zhang, Jingkang Yang, Yuanhan Zhang, Fanyi Pu, Ziwei Liu.*<br>
[[paper]](https://arxiv.org/abs/2311.04219)
[[code]](https://huggingface.co/datasets/Otter-AI/MagnifierBench)

**(*arXiv2023_Video-Bench*) Video-Bench: A Comprehensive Benchmark and Toolkit for Evaluating Video-based Large Language Models.** <br>
*Munan Ning, Bin Zhu, Yujia Xie, Bin Lin, Jiaxi Cui, Lu Yuan, Dongdong Chen, Li Yuan.*<br>
[[paper]](https://arxiv.org/abs/2311.16103)
[[code]](https://github.com/PKU-YuanGroup/Video-Bench)

**(*arXiv2023_MVBench*) MVBench: A Comprehensive Multi-modal Video Understanding Benchmark.** <br>
*Kunchang Li, Yali Wang, Yinan He, Yizhuo Li, Yi Wang, Yi Liu, Zun Wang, Jilan Xu, Guo Chen, Ping Luo, Limin Wang, Yu Qiao.*<br>
[[paper]](https://arxiv.org/abs/2311.17005)
[[code]](https://github.com/OpenGVLab/Ask-Anything)

**(*arXiv2024_VL-ICL*) VL-ICL Bench: The Devil in the Details of Benchmarking Multimodal In-Context Learning.** <br>
*Yongshuo Zong, Ondrej Bohdal, Timothy Hospedales.*<br>
[[paper]](https://arxiv.org/abs/2403.13164)
[[code]](https://github.com/ys-zong/VL-ICL)