Methods Summary of Vision-Language Pretraining
==============================

## ``Catalogue ``
* [Image-Language Pretraining](#image-language-pretraining)
* [Video-Language Pretraining](#video-language-pretraining)
* [Image-Language Datasets](#image-language-datasets)
* [Video-Language Datasets](#video-language-datasets)


### ``*Image-Language Pretraining*``

**(*NeurIPS2019_ViLBERT*) ViLBERT: Pretraining Task-Agnostic Visiolinguistic Representations for Vision-and-Language Tasks.** <br>
*Jiasen Lu, Dhruv Batra, Devi Parikh, Stefan Lee.*<br>
[[paper]](https://arxiv.org/abs/1908.02265)
[[code]](https://github.com/jiasenlu/vilbert_beta)

**(*ACL2020_VisualBERT*) VisualBERT: A Simple and Performant Baseline for Vision and Language.** <br>
*Liunian Harold Li, Mark Yatskar, Da Yin, Cho-Jui Hsieh, Kai-Wei Chang.*<br>
[[paper]](https://arxiv.org/abs/1908.03557)
[[code]](https://github.com/uclanlp/visualbert)

**(*EMNLP2019_B2T2*) Fusion of Detected Objects in Text for Visual Question Answering.** <br>
*Chris Alberti, Jeffrey Ling, Michael Collins, David Reitter.*<br>
[[paper]](https://arxiv.org/abs/1908.05054)
[[code]](https://github.com/google-research/language/tree/master/language/question_answering/b2t2)

**(*AAAI2020_Unicoder-VL*) Unicoder-VL: A Universal Encoder for Vision and Language by Cross-modal Pre-training.** <br>
*Gen Li, Nan Duan, Yuejian Fang, Ming Gong, Daxin Jiang, Ming Zhou.*<br>
[[paper]](https://arxiv.org/abs/1908.06066)

**(*EMNLP2019_LXMERT*) LXMERT: Learning Cross-Modality Encoder Representations from Transformers.** <br>
*Hao Tan, Mohit Bansal.*<br>
[[paper]](https://arxiv.org/abs/1908.07490)
[[code]](https://github.com/airsplay/lxmert)

**(*ICLR2020_VL-BERT*) VL-BERT: Pre-training of Generic Visual-Linguistic Representations.** <br>
*Weijie Su, Xizhou Zhu, Yue Cao, Bin Li, Lewei Lu, Furu Wei, Jifeng Dai.*<br>
[[paper]](https://arxiv.org/abs/1908.08530)
[[code]](https://github.com/jackroos/VL-BERT)

**(*AAAI2020_Unified-VLP*) Unified Vision-Language Pre-Training for Image Captioning and VQA.** <br>
*Luowei Zhou, Hamid Palangi, Lei Zhang, Houdong Hu, Jason J. Corso, Jianfeng Gao.*<br>
[[paper]](https://arxiv.org/abs/1909.11059)
[[code]](https://github.com/LuoweiZhou/VLP)

**(*ECCV2020_UNITER*) UNITER: UNiversal Image-TExt Representation Learning.** <br>
*Yen-Chun Chen, Linjie Li, Licheng Yu, Ahmed El Kholy, Faisal Ahmed, Zhe Gan, Yu Cheng, Jingjing Liu.*<br>
[[paper]](https://arxiv.org/abs/1909.11740)
[[code]](https://github.com/ChenRocks/UNITER)

**(*CVPR2020_M4C*) Iterative Answer Prediction with Pointer-Augmented Multimodal Transformers for TextVQA.** <br>
*Ronghang Hu, Amanpreet Singh, Trevor Darrell, Marcus Rohrbach.*<br>
[[paper]](https://arxiv.org/abs/1911.06258)

**(*CVPR2020_12-in-1*) 12-in-1: Multi-Task Vision and Language Representation Learning.** <br>
*Jiasen Lu, Vedanuj Goswami, Marcus Rohrbach, Devi Parikh, Stefan Lee.*<br>
[[paper]](https://arxiv.org/abs/1912.02315)
[[code]](https://github.com/facebookresearch/vilbert-multi-task)

**(*ECCV2020_VisDial-BERT*) Large-scale Pretraining for Visual Dialog: A Simple State-of-the-Art Baseline.** <br>
*Vishvak Murahari, Dhruv Batra, Devi Parikh, Abhishek Das.*<br>
[[paper]](https://arxiv.org/abs/1912.02379)
[[code]](https://github.com/vmurahari3/visdial-bert)

**(*arXiv2020_ImageBERT*) ImageBERT: Cross-modal Pre-training with Large-scale Weak-supervised Image-Text Data.** <br>
*Di Qi, Lin Su, Jia Song, Edward Cui, Taroon Bharti, Arun Sacheti.*<br>
[[paper]](https://arxiv.org/abs/2001.07966)

**(*NAACL2021_MSB*) Measuring Social Biases in Grounded Vision and Language Embeddings.** <br>
*Candace Ross, Boris Katz, Andrei Barbu.*<br>
[[paper]](https://arxiv.org/abs/2002.08911)
[[code]](https://github.com/candacelax/bias-in-vision-and-language)

**(*CVPR2020_PREVALENT*) Towards Learning a Generic Agent for Vision-and-Language Navigation via Pre-training.** <br>
*Weituo Hao, Chunyuan Li, Xiujun Li, Lawrence Carin, Jianfeng Gao.*<br>
[[paper]](https://arxiv.org/abs/2002.10638)
[[code]](https://github.com/weituo12321/PREVALENT)

**(*INLG2020_VQG-BERT*) What BERT Sees: Cross-Modal Transfer for Visual Question Generation.** <br>
*Thomas Scialom, Patrick Bordes, Paul-Alexis Dray, Jacopo Staiano, Patrick Gallinari.*<br>
[[paper]](https://arxiv.org/abs/2002.10832)

**(*NLPCC2021_XGPT*) XGPT: Cross-modal Generative Pre-Training for Image Captioning.** <br>
*Qiaolin Xia, Haoyang Huang, Nan Duan, Dongdong Zhang, Lei Ji, Zhifang Sui, Edward Cui, Taroon Bharti, Xin Liu, Ming Zhou.*<br>
[[paper]](https://arxiv.org/abs/2003.01473)

**(*arXiv2020_InterBERT*) InterBERT: Vision-and-Language Interaction for Multi-modal Pretraining.** <br>
*Junyang Lin, An Yang, Yichang Zhang, Jie Liu, Jingren Zhou, Hongxia Yang.*<br>
[[paper]](https://arxiv.org/abs/2003.13198)

**(*arXiv2020_Pixel-BERT*) Pixel-BERT: Aligning Image Pixels with Text by Deep Multi-Modal Transformers.** <br>
*Zhicheng Huang, Zhaoyang Zeng, Bei Liu, Dongmei Fu, Jianlong Fu.*<br>
[[paper]](https://arxiv.org/abs/2004.00849)

**(*ECCV2020_Oscar*) Oscar: Object-Semantics Aligned Pre-training for Vision-Language Tasks.** <br>
*Xiujun Li, Xi Yin, Chunyuan Li, Pengchuan Zhang, Xiaowei Hu, Lei Zhang, Lijuan Wang, Houdong Hu, Li Dong, Furu Wei, Yejin Choi, Jianfeng Gao.*<br>
[[paper]](https://arxiv.org/abs/2004.06165)
[[code]](https://github.com/microsoft/Oscar)

**(*arXiv2020_MMF*) Are we pretraining it right? Digging deeper into visio-linguistic pretraining.** <br>
*Amanpreet Singh, Vedanuj Goswami, Devi Parikh.*<br>
[[paper]](https://arxiv.org/abs/2004.08744)
[[code]](https://github.com/facebookresearch/mmf)

**(*ACMMM2020_MMNas*) Deep Multimodal Neural Architecture Search.** <br>
*Zhou Yu, Yuhao Cui, Jun Yu, Meng Wang, Dacheng Tao, Qi Tian.*<br>
[[paper]](https://arxiv.org/abs/2004.12070)
[[code]](https://github.com/MILVLG/mmnas/)

**(*EMNLP2020_VD-BERT*) VD-BERT: A Unified Vision and Dialog Transformer with BERT.** <br>
*Yue Wang, Shafiq Joty, Michael R. Lyu, Irwin King, Caiming Xiong, Steven C.H. Hoi.*<br>
[[paper]](https://arxiv.org/abs/2004.13278)
[[code]](https://github.com/salesforce/VD-BERT)

**(*ECCV2020_VALUE*) Behind the Scene: Revealing the Secrets of Pre-trained Vision-and-Language Models.** <br>
*Jize Cao, Zhe Gan, Yu Cheng, Licheng Yu, Yen-Chun Chen, Jingjing Liu.*<br>
[[paper]](https://arxiv.org/abs/2005.07310)
[[code]](https://github.com/JizeCao/VALUE)

**(*ACLSRW2020_AT*) Adaptive Transformers for Learning Multimodal Representations.** <br>
*Prajjwal Bhargava.*<br>
[[paper]](https://arxiv.org/abs/2005.07486)
[[code]](https://github.com/prajjwal1/adaptive_transformer)

**(*NeurIPS2020_VILLA*) Large-Scale Adversarial Training for Vision-and-Language Representation Learning.** <br>
*Zhe Gan, Yen-Chun Chen, Linjie Li, Chen Zhu, Yu Cheng, Jingjing Liu.*<br>
[[paper]](https://arxiv.org/abs/2006.06195)
[[code]](https://github.com/zhegan27/VILLA)

**(*CVPR2021_VirTex*) VirTex: Learning Visual Representations from Textual Annotations.** <br>
*Karan Desai, Justin Johnson.*<br>
[[paper]](https://arxiv.org/abs/2006.06666)
[[code]](https://github.com/kdexd/virtex)

**(*AAAI2021_ERNIE-ViL*) ERNIE-ViL: Knowledge Enhanced Vision-Language Representations Through Scene Graph.** <br>
*Fei Yu, Jiji Tang, Weichong Yin, Yu Sun, Hao Tian, Hua Wu, Haifeng Wang.*<br>
[[paper]](https://arxiv.org/abs/2006.16934)

**(*ACMMM2020_DeVLBert*) DeVLBert: Learning Deconfounded Visio-Linguistic Representations.** <br>
*Shengyu Zhang, Tan Jiang, Tan Wang, Kun Kuang, Zhou Zhao, Jianke Zhu, Jin Yu, Hongxia Yang, Fei Wu.*<br>
[[paper]](https://arxiv.org/abs/2008.06884)
[[code]](https://github.com/shengyuzhang/DeVLBert)

**(*Access2021_RVL-BERT*) Visual Relationship Detection With Visual-Linguistic Knowledge From Multimodal Representations.** <br>
*Meng-Jiun Chiou, Roger Zimmermann, Jiashi Feng.*<br>
[[paper]](https://arxiv.org/abs/2009.04965)
[[code]](https://github.com/coldmanck/RVL-BERT)

**(*EMNLP2020_X-LXMERT*) X-LXMERT: Paint, Caption and Answer Questions with Multi-Modal Transformers.** <br>
*Jaemin Cho, Jiasen Lu, Dustin Schwenk, Hannaneh Hajishirzi, Aniruddha Kembhavi.*<br>
[[paper]](https://arxiv.org/abs/2009.11278)
[[code]](https://prior.allenai.org/projects/x-lxmert)

**(*arXiv2020_CAPT*) CAPT: Contrastive Pre-Training for Learning Denoised Sequence Representations.** <br>
*Fuli Luo, Pengcheng Yang, Shicheng Li, Xuancheng Ren, Xu Sun.*<br>
[[paper]](https://arxiv.org/abs/2010.06351)

**(*EMNLP2020_STL-CQA*) STL-CQA: Structure-based Transformers with Localization and Encoding for Chart Question Answering.** <br>
*Hrituraj Singh, Sumit Shekhar.*<br>
[[paper]](https://aclanthology.org/2020.emnlp-main.264.pdf)

**(*CVPR2021_DenseCL*) Dense Contrastive Learning for Self-Supervised Visual Pre-Training.** <br>
*Xinlong Wang, Rufeng Zhang, Chunhua Shen, Tao Kong, Lei Li.*<br>
[[paper]](https://arxiv.org/abs/2011.09157)
[[code]](https://git.io/DenseCL)

**(*TACL2021_MPU*) Multimodal Pretraining Unmasked: A Meta-Analysis and a Unified Framework of Vision-and-Language BERTs.** <br>
*Emanuele Bugliarello, Ryan Cotterell, Naoaki Okazaki, Desmond Elliott.*<br>
[[paper]](https://arxiv.org/abs/2011.15124)
[[code]](https://github.com/e-bug/mpre-unmasked)

**(*arXiv2020_LAMP*) LAMP: Label Augmented Multimodal Pretraining.** <br>
*Jia Guo, Chen Zhu, Yilun Zhao, Heda Wang, Yao Hu, Xiaofei He, Deng Cai.*<br>
[[paper]](https://arxiv.org/abs/2012.04446)

**(*arXiv2020_MiniVLM*) MiniVLM: A Smaller and Faster Vision-Language Model.** <br>
*Jianfeng Wang, Xiaowei Hu, Pengchuan Zhang, Xiujun Li, Lijuan Wang, Lei Zhang, Jianfeng Gao, Zicheng Liu.*<br>
[[paper]](https://arxiv.org/abs/2012.06946)

**(*arXiv2020_MANGO*) A Closer Look at the Robustness of Vision-and-Language Pre-trained Models.** <br>
*Linjie Li, Zhe Gan, Jingjing Liu.*<br>
[[paper]](https://arxiv.org/abs/2012.08673)

**(*ACL2021_UNIMO*) UNIMO: Towards Unified-Modal Understanding and Generation via Cross-Modal Contrastive Learning.** <br>
*Wei Li, Can Gao, Guocheng Niu, Xinyan Xiao, Hao Liu, Jiachen Liu, Hua Wu, Haifeng Wang.*<br>
[[paper]](https://arxiv.org/abs/2012.15409)
[[code]](https://github.com/PaddlePaddle/Research/tree/master/NLP/UNIMO)

**(*CVPR2021_VinVL*) VinVL: Revisiting Visual Representations in Vision-Language Models.** <br>
*Pengchuan Zhang, Xiujun Li, Xiaowei Hu, Jianwei Yang, Lei Zhang, Lijuan Wang, Yejin Choi, Jianfeng Gao.*<br>
[[paper]](https://arxiv.org/abs/2101.00529)
[[code]](https://github.com/pzzhang/VinVL)

**(*AAAI2021_VisualMRC*) VisualMRC: Machine Reading Comprehension on Document Images.** <br>
*Ryota Tanaka, Kyosuke Nishida, Sen Yoshida.*<br>
[[paper]](https://arxiv.org/abs/2101.11272)
[[code]](https://github.com/nttmdlab-nlp/VisualMRC)

**(*AAAI2021_TDEN*) Scheduled Sampling in Vision-Language Pretraining with Decoupled Encoder-Decoder Network.** <br>
*Yehao Li, Yingwei Pan, Ting Yao, Jingwen Chen, Tao Mei.*<br>
[[paper]](https://arxiv.org/abs/2101.11562)
[[code]](https://github.com/YehLi/TDEN)

**(*ICML2021_VL-BART*) Unifying Vision-and-Language Tasks via Text Generation.** <br>
*Jaemin Cho, Jie Lei, Hao Tan, Mohit Bansal.*<br>
[[paper]](https://arxiv.org/abs/2102.02779)
[[code]](https://github.com/j-min/VL-T5)

**(*ICML2021_ViLT*) ViLT: Vision-and-Language Transformer Without Convolution or Region Supervision.** <br>
*Wonjae Kim, Bokyung Son, Ildoo Kim.*<br>
[[paper]](https://arxiv.org/abs/2102.03334)
[[code]](https://github.com/dandelin/vilt)

**(*ICML2021_ALIGN*) Scaling Up Visual and Vision-Language Representation Learning With Noisy Text Supervision.** <br>
*Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc V. Le, Yunhsuan Sung, Zhen Li, Tom Duerig.*<br>
[[paper]](https://arxiv.org/abs/2102.05918)
[[blog]](https://ai.googleblog.com/2021/05/align-scaling-up-visual-and-vision.html)

**(*ICCV2021_UniT*) UniT: Multimodal Multitask Learning with a Unified Transformer.** <br>
*Ronghang Hu, Amanpreet Singh.*<br>
[[paper]](https://arxiv.org/abs/2102.10772)
[[code]](https://github.com/facebookresearch/mmf/tree/main/projects/unit)

**(*ICML2021_CLIP*) Learning Transferable Visual Models From Natural Language Supervision.** <br>
*Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever.*<br>
[[paper]](https://arxiv.org/abs/2103.00020)
[[code]](https://github.com/OpenAI/CLIP)

**(*arXiv2021_SemVLP*) SemVLP: Vision-Language Pre-training by Aligning Semantics at Multiple Levels.** <br>
*Chenliang Li, Ming Yan, Haiyang Xu, Fuli Luo, Wei Wang, Bin Bi, Songfang Huang.*<br>
[[paper]](https://arxiv.org/abs/2103.07829)

**(*NAACL2021_LightningDOT*) LightningDOT: Pre-training Visual-Semantic Embeddings for Real-Time Image-Text Retrieval** <br>
*Siqi Sun, Yen-Chun Chen, Linjie Li, Shuohang Wang, Yuwei Fang, Jingjing Liu.*<br>
[[paper]](https://arxiv.org/abs/2103.08784)
[[code]](https://github.com/intersun/LightningDOT)

**(*CVPR2021_Fast&Slow*) Thinking Fast and Slow: Efficient Text-to-Visual Retrieval with Transformers.** <br>
*Antoine Miech, Jean-Baptiste Alayrac, Ivan Laptev, Josef Sivic, Andrew Zisserman.*<br>
[[paper]](https://arxiv.org/abs/2103.16553)

**(*CVPR2021_UC2*) UC2: Universal Cross-lingual Cross-modal Vision-and-Language Pre-training.** <br>
*Mingyang Zhou, Luowei Zhou, Shuohang Wang, Yu Cheng, Linjie Li, Zhou Yu, Jingjing Liu.*<br>
[[paper]](https://arxiv.org/abs/2104.00332)

**(*ICCV2021_DistillVLM*) Compressing Visual-linguistic Model via Knowledge Distillation.** <br>
*Zhiyuan Fang, Jianfeng Wang, Xiaowei Hu, Lijuan Wang, Yezhou Yang, Zicheng Liu.*<br>
[[paper]](https://arxiv.org/abs/2104.02096)

**(*CVPR2021_SOHO*) Seeing Out of tHe bOx: End-to-End Pre-training for Vision-Language Representation Learning.** <br>
*Zhicheng Huang, Zhaoyang Zeng, Yupan Huang, Bei Liu, Dongmei Fu, Jianlong Fu.*<br>
[[paper]](https://arxiv.org/abs/2104.03135)
[[code]](https://github.com/researchmm/soho)

**(*EMNLP2021_GLUE*) Effect of Visual Extensions on Natural Language Understanding in Vision-and-Language Models.** <br>
*Taichi Iki, Akiko Aizawa.*<br>
[[paper]](https://arxiv.org/abs/2104.08066)
[[code]](https://github.com/alab-nii/eval_vl_glue)

**(*ICCV2021_MDETR*) MDETR -- Modulated Detection for End-to-End Multi-Modal Understanding.** <br>
*Aishwarya Kamath, Mannat Singh, Yann LeCun, Gabriel Synnaeve, Ishan Misra, Nicolas Carion.*<br>
[[paper]](https://arxiv.org/abs/2104.12763)
[[code]](https://github.com/ashkamath/mdetr)

**(*CVPR2021_MCT*) Multimodal Contrastive Training for Visual Representation Learning.** <br>
*Xin Yuan, Zhe Lin, Jason Kuen, Jianming Zhang, Yilin Wang, Michael Maire, Ajinkya Kale, Baldo Faieta.*<br>
[[paper]](https://arxiv.org/abs/2104.12836)

**(*ACL2021_IAIS*) Learning Relation Alignment for Calibrated Cross-modal Retrieval.** <br>
*Shuhuai Ren, Junyang Lin, Guangxiang Zhao, Rui Men, An Yang, Jingren Zhou, Xu Sun, Hongxia Yang.*<br>
[[paper]](https://arxiv.org/abs/2105.13868)
[[code]](https://github.com/lancopku/IAIS)

**(*ICLR2022_CLIP-ViL*) How Much Can CLIP Benefit Vision-and-Language Tasks?.** <br>
*Sheng Shen, Liunian Harold Li, Hao Tan, Mohit Bansal, Anna Rohrbach, Kai-Wei Chang, Zhewei Yao, Kurt Keutzer.*<br>
[[paper]](https://arxiv.org/abs/2107.06383)
[[code]](https://github.com/clip-vil/CLIP-ViL)

**(*SIGIR2021_GilBERT*) GilBERT: Generative Vision-Language Pre-Training for Image-Text Retrieval.** <br>
*Weixiang Hong, Kaixiang Ji, Jiajia Liu, Jian Wang, Jingdong Chen, Wei Chu.*<br>
[[paper]](https://dl.acm.org/doi/pdf/10.1145/3404835.3462838)

**(*NeurIPS2021_ALBEF*) Align before Fuse: Vision and Language Representation Learning with Momentum Distillation.** <br>
*Junnan Li, Ramprasaath R. Selvaraju, Akhilesh Deepak Gotmare, Shafiq Joty, Caiming Xiong, Steven Hoi.*<br>
[[paper]](https://arxiv.org/abs/2107.07651)
[[code]](https://github.com/salesforce/ALBEF)

**(*NeurIPS2021_Frozen*) Multimodal Few-Shot Learning with Frozen Language Models.** <br>
*Maria Tsimpoukelli, Jacob Menick, Serkan Cabi, S. M. Ali Eslami, Oriol Vinyals, Felix Hill.*<br>
[[paper]](https://openreview.net/pdf?id=WtmMyno9Tq2)
[[project]](https://fh295.github.io/frozen.html)

**(*ICLR2022_SimVLM*) SimVLM: Simple Visual Language Model Pretraining with Weak Supervision.** <br>
*Zirui Wang, Jiahui Yu, Adams Wei Yu, Zihang Dai, Yulia Tsvetkov, Yuan Cao.*<br>
[[paper]](https://arxiv.org/abs/2108.10904)

**(*arXiv2021_MURAL*) MURAL: Multimodal, Multitask Retrieval Across Languages.** <br>
*Aashi Jain, Mandy Guo, Krishna Srinivasan, Ting Chen, Sneha Kudugunta, Chao Jia, Yinfei Yang, Jason Baldridge.*<br>
[[paper]](https://arxiv.org/abs/2109.05125)

**(*NAACL2022_KD-VLP*) KD-VLP: Improving End-to-End Vision-and-Language Pretraining with Object Knowledge Distillation.** <br>
*Yongfei Liu, Chenfei Wu, Shao-yen Tseng, Vasudev Lal, Xuming He, Nan Duan.*<br>
[[paper]](https://arxiv.org/abs/2109.10504)

**(*CIKM2021_TDMR*) Student Can Also be a Good Teacher: Extracting Knowledge from Vision-and-Language Model for Cross-Modal Retrieval.** <br>
*Jun Rao, Tao Qian, Shuhan Qi, Yulin Wu, Qing Liao, Xuan Wang.*<br>
[[paper]](https://dl.acm.org/doi/pdf/10.1145/3459637.3482194)

**(*ICCV2021_COOKIE*) COOKIE: Contrastive Cross-Modal Knowledge Sharing Pre-Training for Vision-Language Representation.** <br>
*Keyu Wen, Jin Xia, Yuanyuan Huang, Linyang Li, Jiayan Xu, Jie Shao.*<br>
[[paper]](https://openaccess.thecvf.com/content/ICCV2021/papers/Wen_COOKIE_Contrastive_Cross-Modal_Knowledge_Sharing_Pre-Training_for_Vision-Language_Representation_ICCV_2021_paper.pdf)
[[code]](https://github.com/kywen1119/COOKIE)

**(*ICLR2022_DeCLIP*) Supervision Exists Everywhere: A Data Efficient Contrastive Language-Image Pre-training Paradigm.** <br>
*Yangguang Li, Feng Liang, Lichen Zhao, Yufeng Cui, Wanli Ouyang, Jing Shao, Fengwei Yu, Junjie Yan.*<br>
[[paper]](https://arxiv.org/abs/2110.05208)

**(*arXiv2021_VLDeformer*) VLDeformer: Learning Visual-Semantic Embeddings by Vision-Language Transformer Decomposing.** <br>
*Lisai Zhang, Hongfa Wu, Qingcai Chen, Yimeng Deng, Zhonghua Li, Dejiang Kong, Zhao Cao, Joanna Siebert, Yunpeng Han.*<br>
[[paper]](https://arxiv.org/abs/2110.11338)

**(*NeurIPS2022_VLMo*) VLMo: Unified Vision-Language Pre-Training with Mixture-of-Modality-Experts.** <br>
*Wenhui Wang, Hangbo Bao, Li Dong, Furu Wei.*<br>
[[paper]](https://arxiv.org/abs/2111.02358)
[[code]](https://github.com/microsoft/unilm)

**(*CVPR2022_METER*) An Empirical Study of Training End-to-End Vision-and-Language Transformers.** <br>
*Zi-Yi Dou, Yichong Xu, Zhe Gan, Jianfeng Wang, Shuohang Wang, Lijuan Wang, Chenguang Zhu, Pengchuan Zhang, Lu Yuan, Nanyun Peng, Zicheng Liu, Michael Zeng.*<br>
[[paper]](https://arxiv.org/abs/2111.02387)
[[code]](https://github.com/zdou0830/METER)

**(*NAACL2022_TAGS*) Negative Sample is Negative in Its Own Way: Tailoring Negative Sentences for Image-Text Retrieval.** <br>
*Zhihao Fan, Zhongyu Wei, Zejun Li, Siyuan Wang, Jianqing Fan.*<br>
[[paper]](https://arxiv.org/abs/2111.03349)
[[code]](https://github.com/LibertFan/TAGS)

**(*ICLR2022_FILIP*) FILIP: Fine-grained Interactive Language-Image Pre-Training.** <br>
*Lewei Yao, Runhui Huang, Lu Hou, Guansong Lu, Minzhe Niu, Hang Xu, Xiaodan Liang, Zhenguo Li, Xin Jiang, Chunjing Xu.*<br>
[[paper]](https://arxiv.org/abs/2111.07783)

**(*CVPR2022_LiT*) LiT: Zero-Shot Transfer with Locked-image Text Tuning.** <br>
*Xiaohua Zhai, Xiao Wang, Basil Mustafa, Andreas Steiner, Daniel Keysers, Alexander Kolesnikov, Lucas Beyer.*<br>
[[paper]](https://arxiv.org/abs/2111.07991)

**(*ICML2022_X-VLM*) Multi-Grained Vision Language Pre-Training: Aligning Texts with Visual Concepts.** <br>
*Yan Zeng, Xinsong Zhang, Hang Li.*<br>
[[paper]](https://arxiv.org/abs/2111.08276)
[[code]](https://github.com/zengyan-97/x-vlm)

**(*arXiv2021_Florence*) Florence: A New Foundation Model for Computer Vision.** <br>
*Lu Yuan, Dongdong Chen, Yi-Ling Chen, Noel Codella, Xiyang Dai, Jianfeng Gao, Houdong Hu, Xuedong Huang, Boxin Li, Chunyuan Li, Ce Liu, Mengchen Liu, Zicheng Liu, Yumao Lu, Yu Shi, Lijuan Wang, Jianfeng Wang, Bin Xiao, Zhen Xiao, Jianwei Yang, Michael Zeng, Luowei Zhou, Pengchuan Zhang.*<br>
[[paper]](https://arxiv.org/abs/2111.11432)

**(*CVPR2022_GLIP*) Grounded Language-Image Pre-training.** <br>
*Liunian Harold Li, Pengchuan Zhang, Haotian Zhang, Jianwei Yang, Chunyuan Li, Yiwu Zhong, Lijuan Wang, Lu Yuan, Lei Zhang, Jenq-Neng Hwang, Kai-Wei Chang, Jianfeng Gao.*<br>
[[paper]](https://arxiv.org/abs/2112.03857)
[[code]](https://github.com/microsoft/GLIP)

**(*arXiv2021_ViT-BERT*) Towards a Unified Foundation Model: Jointly Pre-Training Transformers on Unpaired Images and Text.** <br>
*Qing Li, Boqing Gong, Yin Cui, Dan Kondratyuk, Xianzhi Du, Ming-Hsuan Yang, Matthew Brown.*<br>
[[paper]](https://arxiv.org/abs/2112.07074)

**(*ECCV2022_SLIP*) SLIP: Self-supervision meets Language-Image Pre-training.** <br>
*Norman Mu, Alexander Kirillov, David Wagner, Saining Xie.*<br>
[[paper]](https://arxiv.org/abs/2112.12750)
[[code]](https://github.com/facebookresearch/slip)

**(*CVPR2022_QB-NORM*) Cross Modal Retrieval with Querybank Normalisation.** <br>
*Simion-Vlad Bogolin, Ioana Croitoru, Hailin Jin, Yang Liu, Samuel Albanie.*<br>
[[paper]](https://arxiv.org/abs/2112.12777)
[[code]](https://github.com/ioanacroi/qb-norm)

**(*ACLARR_PromptFuse*) Prompting as Multimodal Fusing.** <br>
[[paper]](https://openreview.net/pdf?id=wWZCNLkK-FK)

**(*TCSVT2022_CSIC*) Image-Text Retrieval with Cross-Modal Semantic Importance Consistency.** <br>
*Zejun Liu, Fanglin Chen, Jun Xu, Wenjie Pei, Guangming Lu.*<br>
[[paper]](https://ieeexplore.ieee.org/abstract/document/9940913)

**(*PMLR2022_VLUE*) VLUE: A Multi-Task Benchmark for Evaluating Vision-Language Models.** <br>
*Wangchunshu Zhou, Yan Zeng, Shizhe Diao, Xinsong Zhang.*<br>
[[paper]](https://proceedings.mlr.press/v162/zhou22n/zhou22n.pdf)
[[code]](https://github.com/MichaelZhouwang/VLUE)

**(*CVPR2022_TCL*) Vision-Language Pre-Training with Triple Contrastive Learning.** <br>
*Jinyu Yang, Jiali Duan, Son Tran, Yi Xu, Sampath Chanda, Liqun Chen, Belinda Zeng, Trishul Chilimbi, Junzhou Huang.*<br>
[[paper]](https://arxiv.org/abs/2202.10401v3)
[[code]](https://github.com/uta-smile/TCL)

**(*CVPR2022_CODIS*) Multi-modal Alignment using Representation Codebook.** <br>
*Jiali Duan, Liqun Chen, Son Tran, Jinyu Yang, Yi Xu, Belinda Zeng, Trishul Chilimbi.*<br>
[[paper]](https://arxiv.org/abs/2203.00048)

**(*arXiv2022_LoopITR*) LoopITR: Combining Dual and Cross Encoder Architectures for Image-Text Retrieval.** <br>
*Jie Lei, Xinlei Chen, Ning Zhang, Mengjiao Wang, Mohit Bansal, Tamara L. Berg, Licheng Yu.*<br>
[[paper]](https://arxiv.org/abs/2203.05465)

**(*ACL2022_VLKD*) Enabling Multimodal Generation on CLIP via Vision-Language Knowledge Distillation.** <br>
*Wenliang Dai, Lu Hou, Lifeng Shang, Xin Jiang, Qun Liu, Pascale Fung.*<br>
[[paper]](https://arxiv.org/abs/2203.06386)

**(*ACL2022_CMKT*) Leveraging Visual Knowledge in Language Tasks: An Empirical Study on Intermediate Pre-training for Cross-modal Knowledge Transfer.** <br>
*Woojeong Jin, Dong-Ho Lee, Chenguang Zhu, Jay Pujara, Xiang Ren.*<br>
[[paper]](https://arxiv.org/abs/2203.07519)
[[code]](https://github.com/INK-USC/CMKT)

**(*CVPR2022_ViSTA*) ViSTA: Vision and Scene Text Aggregation for Cross-Modal Retrieval.** <br>
*Mengjun Cheng, Yipeng Sun, Longchao Wang, Xiongwei Zhu, Kun Yao, Jie Chen, Guoli Song, Junyu Han, Jingtuo Liu, Errui Ding, Jingdong Wang.*<br>
[[paper]](https://arxiv.org/abs/2203.16778)

**(*CVPR2022_UniCL*) Unified Contrastive Learning in Image-Text-Label Space.** <br>
*Jianwei Yang, Chunyuan Li, Pengchuan Zhang, Bin Xiao, Ce Liu, Lu Yuan, Jianfeng Gao.*<br>
[[paper]](https://arxiv.org/abs/2204.03610)
[[code]](https://github.com/microsoft/UniCL)

**(*CVPR2022_PSD*) Robust Cross-Modal Representation Learning with Progressive Self-Distillation.** <br>
*Alex Andonian, Shixing Chen, Raffay Hamid.*<br>
[[paper]](https://arxiv.org/abs/2204.04588)

**(*CVPR2022_COTS*) COTS: Collaborative Two-Stream Vision-Language Pre-Training Model for Cross-Modal Retrieval.** <br>
*Haoyu Lu, Nanyi Fei, Yuqi Huo, Yizhao Gao, Zhiwu Lu, Ji-Rong Wen.*<br>
[[paper]](https://arxiv.org/abs/2204.07441)

**(*TMLR2023_LTD*) Reducing Predictive Feature Suppression in Resource-Constrained Contrastive Image-Caption Retrieval.** <br>
*Maurits Bleeker, Andrew Yates, Maarten de Rijke.*<br>
[[paper]](https://arxiv.org/abs/2204.13382)
[[code]](https://github.com/mauritsbleeker/reducing-predictive-feature-suppression)

**(*NeurIPS2022_PyramidCLIP*) PyramidCLIP: Hierarchical Feature Alignment for Vision-language Model Pretraining.** <br>
*Yuting Gao, Jinfeng Liu, Zihan Xu, Jun Zhang, Ke Li, Rongrong Ji, Chunhua Shen.*<br>
[[paper]](https://arxiv.org/abs/2204.14095)

**(*arXiv2022_HiVLP*) HiVLP: Hierarchical Vision-Language Pre-Training for Fast Image-Text Retrieval.** <br>
*Feilong Chen, Xiuyi Chen, Jiaxin Shi, Duzhen Zhang, Jianlong Chang, Qi Tian.*<br>
[[paper]](https://arxiv.org/abs/2205.12105)

**(*arXiv2022_COOKIE*) Contrastive Cross-Modal Knowledge Sharing Pre-training for Vision-Language Representation Learning and Retrieval.** <br>
*Keyu Wen, Zhenshan Tan, Qingrong Cheng, Cheng Chen, Xiaodong Gu.*<br>
[[paper]](https://arxiv.org/abs/2207.00733)

**(*CBMI2022_ALADIN*) ALADIN: Distilling Fine-grained Alignment Scores for Efficient Image-Text Matching and Retrieval.** <br>
*Nicola Messina, Matteo Stefanini, Marcella Cornia, Lorenzo Baraldi, Fabrizio Falchi, Giuseppe Amato, Rita Cucchiara.*<br>
[[paper]](https://arxiv.org/abs/2207.14757)
[[code]](https://github.com/mesnico/ALADIN)

**(*NeurIPS2022_LOUPE*) Fine-Grained Semantically Aligned Vision-Language Pre-Training.** <br>
*Juncheng Li, Xin He, Longhui Wei, Long Qian, Linchao Zhu, Lingxi Xie, Yueting Zhuang, Qi Tian, Siliang Tang.*<br>
[[paper]](https://arxiv.org/abs/2208.02515)
[[code]](https://github.com/yyjmjc/loupe)

**(*ECCV2022_GRIT-VLP*) GRIT-VLP: Grouped Mini-batch Sampling for Efficient Vision and Language Pre-training.** <br>
*Jaeseok Byun, Taebaek Hwang, Jianlong Fu, Taesup Moon.*<br>
[[paper]](https://arxiv.org/abs/2208.04060)
[[code]](https://github.com/jaeseokbyun/GRIT-VLP)

**(*arXiv2022_TokenFlow*) TokenFlow: Rethinking Fine-grained Cross-modal Alignment in Vision-Language Retrieval.** <br>
*Xiaohan Zou, Changqiao Wu, Lele Cheng, Zhongyuan Wang.*<br>
[[paper]](https://arxiv.org/abs/2209.13822)

**(*NeurIPS2022_Knowledge-CLIP*) Contrastive Language-Image Pre-Training with Knowledge Graphs.** <br>
*Xuran Pan, Tianzhu Ye, Dongchen Han, Shiji Song, Gao Huang.*<br>
[[paper]](https://arxiv.org/abs/2210.08901)

**(*CVPR2023_xCLIP*) Non-Contrastive Learning Meets Language-Image Pre-Training.** <br>
*Jinghao Zhou, Li Dong, Zhe Gan, Lijuan Wang, Furu Wei.*<br>
[[paper]](https://arxiv.org/abs/2210.09304)

**(*arXiv2022_X2-VLM*) X2-VLM: All-In-One Pre-trained Model For Vision-Language Tasks.** <br>
*Yan Zeng, Xinsong Zhang, Hang Li, Jiawei Wang, Jipeng Zhang, Wangchunshu Zhou.*<br>
[[paper]](https://arxiv.org/abs/2211.12402v2)
[[code]](https://github.com/zengyan-97/x2-vlm)

**(*BMVC2022_ViCHA*) Efficient Vision-Language Pretraining with Visual Concepts and Hierarchical Alignment.** <br>
*Mustafa Shukor, Guillaume Couairon, Matthieu Cord.*<br>
[[paper]](https://hal.science/hal-03811336v1/file/Long_version_arxiv.pdf)
[[code]](https://github.com/mshukor/ViCHA)

**(*ACMMM2022_CMAL*) CMAL: A Novel Cross-Modal Associative Learning Framework for Vision-Language Pre-Training.** <br>
*Zhiyuan Ma, Jianjun Li, Guohui Li, Kaiyan Huang.*<br>
[[paper]](https://dl.acm.org/doi/abs/10.1145/3503161.3548292?casa_token=hi-6v_U02LUAAAAA%3A9bC8zagp-Strds7Ik1iST5VWvADRENtFa_R_vh2gBKfjxyTg1o-7LxTFhM16Q00mdg6l_7-vaWQOqNE)

**(*ACMMM2022_MVPTR*) MVPTR: Multi-Level Semantic Alignment for Vision-Language Pre-Training via Multi-Stage Learning.** <br>
*Zejun Li, Zhihao Fan, Huaixiao Tou, Jingjing Chen, Zhongyu Wei, Xuanjing Huang.*<br>
[[paper]](https://dl.acm.org/doi/abs/10.1145/3503161.3548341?casa_token=VWAkYtj9sQ8AAAAA%3A76l66PyWE1IXcXSHMTXzVndjPS61VGwPpd1RgCid8oKoJJ-f_6qtIDDmDzG16TVj20GUjTAPkElk53Q)

**(*CVPR2022_CLIP-Event*) CLIP-Event: Connecting Text and Images with Event Structures.** <br>
*Manling Li, Ruochen Xu, Shuohang Wang, Luowei Zhou, Xudong Lin, Chenguang Zhu, Michael Zeng, Heng Ji, Shih-Fu Chang.*<br>
[[paper]](https://openaccess.thecvf.com/content/CVPR2022/papers/Li_CLIP-Event_Connecting_Text_and_Images_With_Event_Structures_CVPR_2022_paper.pdf)
[[code]](https://github.com/limanling/clip-event)

**(*CVPR2023_TCL*) Learning to Generate Text-grounded Mask for Open-world Semantic Segmentation from Only Image-Text Pairs.** <br>
*Junbum Cha, Jonghwan Mun, Byungseok Roh.*<br>
[[paper]](https://arxiv.org/abs/2212.00785)
[[code]](https://github.com/kakaobrain/tcl)

**(*AAAI2023_NLIP*) NLIP: Noise-robust Language-Image Pre-training.** <br>
*Runhui Huang, Yanxin Long, Jianhua Han, Hang Xu, Xiwen Liang, Chunjing Xu, Xiaodan Liang.*<br>
[[paper]](https://arxiv.org/abs/2212.07086)

**(*ECIR2023_HADA*) HADA: A Graph-based Amalgamation Framework in Image-text Retrieval.** <br>
*Manh-Duy Nguyen, Binh T. Nguyen, Cathal Gurrin.*<br>
[[paper]](https://arxiv.org/abs/2301.04742)
[[code]](https://github.com/m2man/HADA)

**(*ICCV2023_LexLIP*) LexLIP: Lexicon-Bottlenecked Language-Image Pre-Training for Large-Scale Image-Text Retrieval.** <br>
*Ziyang luo, Pu Zhao, Can Xu, Xiubo Geng, Tao Shen, Chongyang Tao, Jing Ma, Qingwen lin, Daxin Jiang.*<br>
[[paper]](https://arxiv.org/abs/2302.02908)
[[code]](https://github.com/chiyeunglaw/lexlip-iccv23)

**(*arXiv2023_VITR*) VITR: Augmenting Vision Transformers with Relation-Focused Learning for Cross-Modal Information Retrieval.** <br>
*Yan Gong, Georgina Cosma, Axel Finke.*<br>
[[paper]](https://arxiv.org/abs/2302.06350)

**(*arXiv2023_UKnow*) UKnow: A Unified Knowledge Protocol for Common-Sense Reasoning and Vision-Language Pre-training.** <br>
*Biao Gong, Xiaoying Xie, Yutong Feng, Yiliang Lv, Yujun Shen, Deli Zhao.*<br>
[[paper]](https://arxiv.org/abs/2302.06891)
[[code]](https://github.com/Gongggg/UKnow)

**(*CVPR2023_SCL*) Seeing What You Miss: Vision-Language Pre-training with Semantic Completion Learning.** <br>
*Yatai Ji, Rongcheng Tu, Jie Jiang, Weijie Kong, Chengfei Cai, Wenzhe Zhao, Hongfa Wang, Yujiu Yang, Wei Liu.*<br>
[[paper]](https://openaccess.thecvf.com/content/CVPR2023/papers/Ji_Seeing_What_You_Miss_Vision-Language_Pre-Training_With_Semantic_Completion_Learning_CVPR_2023_paper.pdf)

**(*CVPR2023_RO-ViT*) Region-Aware Pretraining for Open-Vocabulary Object Detection with Vision Transformers.** <br>
*Dahun Kim, Anelia Angelova, Weicheng Kuo.*<br>
[[paper]](https://openaccess.thecvf.com/content/CVPR2023/papers/Kim_Region-Aware_Pretraining_for_Open-Vocabulary_Object_Detection_With_Vision_Transformers_CVPR_2023_paper.pdf)

**(*ICCV2023_EqSim*) Equivariant Similarity for Vision-Language Foundation Models.** <br>
*Tan Wang, Kevin Lin, Linjie Li, Chung-Ching Lin, Zhengyuan Yang, Hanwang Zhang, Zicheng Liu, Lijuan Wang.*<br>
[[paper]](https://arxiv.org/abs/2303.14465)
[[code]](https://github.com/Wangt-CN/EqBen)

**(*ICCV2023_SigLIP*) Sigmoid Loss for Language Image Pre-Training.** <br>
*Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, Lucas Beyer.*<br>
[[paper]](https://arxiv.org/abs/2303.15343)
[[code]](https://github.com/google-research/big_vision)

**(*arXiv2023_CAVL*) CAVL: Learning Contrastive and Adaptive Representations of Vision and Language.** <br>
*Shentong Mo, Jingfei Xia, Ihor Markevych.*<br>
[[paper]](https://arxiv.org/abs/2304.04399)

**(*ICML2023_MERU*) Hyperbolic Image-Text Representations.** <br>
*Karan Desai, Maximilian Nickel, Tanmay Rajpurohit, Justin Johnson, Ramakrishna Vedantam.*<br>
[[paper]](https://arxiv.org/abs/2304.09172)
[[code]](https://github.com/facebookresearch/meru)

**(*ACL2023_MI*) Vision Language Pre-training by Contrastive Learning with Cross-Modal Similarity Regulation.** <br>
*Chaoya Jiang, Wei Ye, Haiyang Xu, Miang yan, Shikun Zhang, Jie Zhang, Fei Huang.*<br>
[[paper]](https://arxiv.org/abs/2305.04474)

**(*arXiv2023_Boon*) Boon: A Neural Search Engine for Cross-Modal Information Retrieval.** <br>
*Yan Gong, Georgina Cosma.*<br>
[[paper]](https://arxiv.org/abs/2307.14240)

**(*ACMMM2023_COPA*) COPA: Efficient Vision-Language Pre-training Through Collaborative Object- and Patch-Text Alignment.** <br>
*Chaoya Jiang, Haiyang Xu, Wei Ye, Qinghao Ye, Chenliang Li, Ming Yan, Bin Bi, Shikun Zhang, Ji Zhang, Fei Huang.*<br>
[[paper]](https://arxiv.org/abs/2308.03475)

**(*AAAI2024_EVE*) EVE: Efficient Vision-Language Pre-training with Masked Prediction and Modality-Aware MoE.** <br>
*Junyi Chen, Longteng Guo, Jia Sun, Shuai Shao, Zehuan Yuan, Liang Lin, Dongyu Zhang.*<br>
[[paper]](https://arxiv.org/abs/2308.11971)

**(*NeurIPS2023_PAU*) Prototype-based Aleatoric Uncertainty Quantification for Cross-modal Retrieval.** <br>
*Hao Li, Jingkuan Song, Lianli Gao, Xiaosu Zhu, Heng Tao Shen.*<br>
[[paper]](https://arxiv.org/abs/2309.17093v3)
[[code]](https://github.com/leolee99/PAU)

**(*arXiv2023_TiC-CLIP*) TiC-CLIP: Continual Training of CLIP Models.** <br>
*Saurabh Garg, Mehrdad Farajtabar, Hadi Pouransari, Raviteja Vemulapalli, Sachin Mehta, Oncel Tuzel, Vaishaal Shankar, Fartash Faghri.*<br>
[[paper]](https://arxiv.org/abs/2310.16226)

**(*arXiv2023_MCAD*) MCAD: Multi-teacher Cross-modal Alignment Distillation for Efficient Image-text Retrieval.**<br>
*Youbo Lei, Feifei He, Chen Chen, Yingbin Mo, Si Jia Li, Defeng Xie, Haonan Lu.*<br>
[[paper]](https://arxiv.org/abs/2310.19654)

**(*arXiv2023_MLLMs-Augmented*) MLLMs-Augmented Visual-Language Representation Learning.**<br>
*Yanqing Liu, Kai Wang, Wenqi Shao, Ping Luo, Yu Qiao, Mike Zheng Shou, Kaipeng Zhang, Yang You.*<br>
[[paper]](https://arxiv.org/abs/2311.18765)
[[code]](https://github.com/lyq312318224/MLLMs-Augmented)

**(*CVPR2024_MAFA*) MAFA: Managing False Negatives for Vision-Language Pre-training.**<br>
*Jaeseok Byun, Dohoon Kim, Taesup Moon.*<br>
[[paper]](https://openaccess.thecvf.com/content/CVPR2024/papers/Byun_MAFA_Managing_False_Negatives_for_Vision-Language_Pre-training_CVPR_2024_paper.pdf)
[[code]](https://github.com/jaeseokbyun/MAFA)


### ``*Video-Language Pretraining*``

**(*ICCV2019_VideoBERT*) VideoBERT: A Joint Model for Video and Language Representation Learning.** <br>
*Chen Sun, Austin Myers, Carl Vondrick, Kevin Murphy, Cordelia Schmid.*<br>
[[paper]](https://arxiv.org/abs/1904.01766)

**(*ICCV2019_HowTo100M*) HowTo100M: Learning a Text-Video Embedding by Watching Hundred Million Narrated Video Clips.** <br>
*Antoine Miech, Dimitri Zhukov, Jean-Baptiste Alayrac, Makarand Tapaswi, Ivan Laptev, Josef Sivic.*<br>
[[paper]](https://arxiv.org/abs/1906.03327)
[[code]](http://www.di.ens.fr/willow/research/howto100m/)

**(*arXiv2019_CBT*) Learning Video Representations using Contrastive Bidirectional Transformer.** <br>
*Chen Sun, Fabien Baradel, Kevin Murphy, Cordelia Schmid.*<br>
[[paper]](https://arxiv.org/abs/1906.05743)

**(*EMNLP2020_HERO*) HERO: Hierarchical Encoder for Video+Language Omni-representation Pre-training.** <br>
*Linjie Li, Yen-Chun Chen, Yu Cheng, Zhe Gan, Licheng Yu, Jingjing Liu.*<br>
[[paper]](https://arxiv.org/abs/2005.00200)
[[code]](https://github.com/linjieli222/HERO)

**(*CVPR2020_ActBERT*) ActBERT: Learning Global-Local Video-Text Representations.** <br>
*Linchao Zhu, Yi Yang.*<br>
[[paper]](https://arxiv.org/abs/2011.07231)

**(*CVPR2021_ClipBERT*) Less is More: ClipBERT for Video-and-Language Learning via Sparse Sampling.** <br>
*Jie Lei, Linjie Li, Luowei Zhou, Zhe Gan, Tamara L. Berg, Mohit Bansal, Jingjing Liu.*<br>
[[paper]](https://arxiv.org/abs/2102.06183)
[[code]](https://github.com/jayleicn/ClipBERT)

**(*CVPRW2021_MDMMT*) MDMMT: Multidomain Multimodal Transformer for Video Retrieval** <br>
*Maksim Dzabraev, Maksim Kalashnikov, Stepan Komkov, Aleksandr Petiushko.*<br>
[[paper]](https://arxiv.org/abs/2103.10699v1)
[[code]](https://github.com/papermsucode/mdmmt)

**(*ICCV2021_Frozen*) Frozen in Time: A Joint Video and Image Encoder for End-to-End Retrieval.** <br>
*Max Bain, Arsha Nagrani, Gül Varol, Andrew Zisserman.*<br>
[[paper]](https://arxiv.org/abs/2104.00650)
[[code]](https://github.com/m-bain/frozen-in-time)

**(*ICCV2021_TEACHTEXT*) TEACHTEXT: CrossModal Generalized Distillation for Text-Video Retrieval.** <br>
*Ioana Croitoru, Simion-Vlad Bogolin, Marius Leordeanu, Hailin Jin, Andrew Zisserman, Samuel Albanie, Yang Liu.*<br>
[[paper]](https://arxiv.org/abs/2104.08271)
[[code]](https://github.com/albanie/collaborative-experts)

**(*Neurocomputing2022_CLIP4Clip*) CLIP4Clip: An Empirical Study of CLIP for End to End Video Clip Retrieval.** <br>
*Huaishao Luo, Lei Ji, Ming Zhong, Yang Chen, Wen Lei, Nan Duan, Tianrui Li.*<br>
[[paper]](https://arxiv.org/abs/2104.08860)
[[code]](https://github.com/ArrowLuo/CLIP4Clip)

**(*ACL2021_VLM*) VLM: Task-agnostic Video-Language Model Pre-training for Video Understanding.** <br>
*Hu Xu, Gargi Ghosh, Po-Yao Huang, Prahal Arora, Masoumeh Aminzadeh, Christoph Feichtenhofer, Florian Metze, Luke Zettlemoyer.*<br>
[[paper]](https://arxiv.org/abs/2105.09996v3)
[[code]](https://github.com/facebookresearch/fairseq/tree/main/examples/MMPT)

**(*arXiv2021_CLIP2Video*) CLIP2Video: Mastering Video-Text Retrieval via Image CLIP.** <br>
*Han Fang, Pengfei Xiong, Luhui Xu, Yu Chen.*<br>
[[paper]](https://arxiv.org/abs/2106.11097)
[[code]](https://github.com/CryhanFang/CLIP2Video)

**(*ICCV2021_TACo*) TACo: Token-aware Cascade Contrastive Learning for Video-Text Alignment.** <br>
*Jianwei Yang, Yonatan Bisk, Jianfeng Gao.*<br>
[[paper]](https://arxiv.org/abs/2108.09980v1)

**(*arXiv2021_CAMoE*) Improving Video-Text Retrieval by Multi-Stream Corpus Alignment and Dual Softmax Loss.** <br>
*Xing Cheng, Hezheng Lin, Xiangyu Wu, Fan Yang, Dong Shen.*<br>
[[paper]](https://arxiv.org/abs/2109.04290)

**(*EMNLP2021_VideoCLIP*) VideoCLIP: Contrastive Pre-training for Zero-shot Video-Text Understanding.** <br>
*Hu Xu, Gargi Ghosh, Po-Yao Huang, Dmytro Okhonko, Armen Aghajanyan, Florian Metze, Luke Zettlemoyer, Christoph Feichtenhofer.*<br>
[[paper]](https://arxiv.org/abs/2109.14084v2)
[[code]](https://github.com/pytorch/fairseq/tree/main/examples/MMPT)

**(*arXiv2021_CLIP2TV*) CLIP2TV: An Empirical Study on Transformer-based Methods for Video-Text Retrieval.** <br>
*Zijian Gao, Jingyu Liu, Sheng Chen, Dedan Chang, Hao Zhang, Jinwei Yuan.*<br>
[[paper]](https://arxiv.org/abs/2111.05610)

**(*CVPR2022_OA-Transformer*) Object-aware Video-language Pre-training for Retrieval.** <br>
*Alex Jinpeng Wang, Yixiao Ge, Guanyu Cai, Rui Yan, Xudong Lin, Ying Shan, Xiaohu Qie, Mike Zheng Shou.*<br>
[[paper]](https://arxiv.org/abs/2112.00656)
[[code]](https://github.com/FingerRec/OA-Transformer)

**(*AAAI2023_RegionLearner*) Video-Text Pre-training with Learned Regions.** <br>
*Rui Yan, Mike Zheng Shou, Yixiao Ge, Alex Jinpeng Wang, Xudong Lin, Guanyu Cai, Jinhui Tang.*<br>
[[paper]](https://arxiv.org/abs/2112.01194)
[[code]](https://github.com/ruiyan1995/Region_Learner)

**(*ECCV2022_LAFF*) Lightweight Attentional Feature Fusion: A New Baseline for Text-to-Video Retrieval.** <br>
*Fan Hu, Aozhu Chen, Ziyue Wang, Fangming Zhou, Jianfeng Dong, Xirong Li.*<br>
[[paper]](https://arxiv.org/abs/2112.01832)
[[code]](https://github.com/ruc-aimc-lab/laff)

**(*ACMMM2021_CoCo-BERT*) CoCo-BERT: Improving Video-Language Pre-training with Contrastive Cross-modal Matching and Denoising.** <br>
*Jianjie Luo, Yehao Li, Yingwei Pan, Ting Yao, Hongyang Chao, Tao Mei.*<br>
[[paper]](https://arxiv.org/abs/2112.07515)

**(*CVPR2022_ALPRO*) Align and Prompt: Video-and-Language Pre-training with Entity Prompts.** <br>
*Dongxu Li, Junnan Li, Hongdong Li, Juan Carlos Niebles, Steven C.H. Hoi.*<br>
[[paper]](https://arxiv.org/abs/2112.09583)
[[code]](https://github.com/salesforce/alpro)

**(*CVPR2022_MCQ*) Bridging Video-text Retrieval with Multiple Choice Questions.** <br>
*Yuying Ge, Yixiao Ge, Xihui Liu, Dian Li, Ying Shan, Xiaohu Qie, Ping Luo.*<br>
[[paper]](https://arxiv.org/abs/2201.04850)
[[code]](https://github.com/TencentARC/MCQ)

**(*arXiv2022_MDMMT-2*) MDMMT-2: Multidomain Multimodal Transformer for Video Retrieval, One More Step Towards Generalization.** <br>
*Alexander Kunitsyn, Maksim Kalashnikov, Maksim Dzabraev, Andrei Ivaniuta.*<br>
[[paper]](https://arxiv.org/abs/2203.07086)

**(*arXiv2022_DRL*) Disentangled Representation Learning for Text-Video Retrieval.** <br>
*Qiang Wang, Yanhao Zhang, Yun Zheng, Pan Pan, Xian-Sheng Hua.*<br>
[[paper]](https://arxiv.org/abs/2203.07111)
[[code]](https://github.com/foolwood/DRL)

**(*CVPR2023_All-in-One*) All in One: Exploring Unified Video-Language Pre-training.** <br>
*Alex Jinpeng Wang, Yixiao Ge, Rui Yan, Yuying Ge, Xudong Lin, Guanyu Cai, Jianping Wu, Ying Shan, Xiaohu Qie, Mike Zheng Shou.*<br>
[[paper]](https://arxiv.org/abs/2203.07303v1)
[[code]](https://github.com/showlab/all-in-one)

**(*arXiv2022_MDMMT-2*) MDMMT-2: Multidomain Multimodal Transformer for Video Retrieval, One More Step Towards Generalization.** <br>
*Alexander Kunitsyn, Maksim Kalashnikov, Maksim Dzabraev, Andrei Ivaniuta.*<br>
[[paper]](https://arxiv.org/abs/2203.07086)

**(*arXiv2022_DRL*) Disentangled Representation Learning for Text-Video Retrieval.** <br>
*Qiang Wang, Yanhao Zhang, Yun Zheng, Pan Pan, Xian-Sheng Hua.*<br>
[[paper]](https://arxiv.org/abs/2203.07111)
[[code]](https://github.com/foolwood/DRL)

**(*CVPR2023_All-in-One*) All in One: Exploring Unified Video-Language Pre-training.** <br>
*Alex Jinpeng Wang, Yixiao Ge, Rui Yan, Yuying Ge, Xudong Lin, Guanyu Cai, Jianping Wu, Ying Shan, Xiaohu Qie, Mike Zheng Shou.*<br>
[[paper]](https://arxiv.org/abs/2203.07303v1)
[[code]](https://github.com/showlab/all-in-one)

**(*arXiv2022_DemoVLP*) Revitalize Region Feature for Democratizing Video-Language Pre-training.** <br>
*Guanyu Cai, Yixiao Ge, Alex Jinpeng Wang, Rui Yan, Xudong Lin, Ying Shan, Lianghua He, Xiaohu Qie, Jianping Wu, Mike Zheng Shou.*<br>
[[paper]](https://arxiv.org/abs/2203.07720)
[[code]](https://github.com/showlab/DemoVLP)

**(*CVPR2022_X-Pool*) X-Pool: Cross-Modal Language-Video Attention for Text-Video Retrieval.** <br>
*Satya Krishna Gorti, Noel Vouitsis, Junwei Ma, Keyvan Golestan, Maksims Volkovs, Animesh Garg, Guangwei Yu.*<br>
[[paper]](https://arxiv.org/abs/2203.15086)
[[code]](https://github.com/layer6ai-labs/xpool)
[[project]](https://layer6ai-labs.github.io/xpool/)

**(*CVPR2022_TAN*) Temporal Alignment Networks for Long-term Video.** <br>
*Tengda Han, Weidi Xie, Andrew Zisserman.*<br>
[[paper]](https://arxiv.org/abs/2204.02968)
[[code]](https://github.com/tengdahan/temporalalignnet)

**(*arXiv2022_HCMI*) Tencent Text-Video Retrieval: Hierarchical Cross-Modal Interactions with Multi-Level Representations.** <br>
*Jie Jiang, Shaobo Min, Weijie Kong, Dihong Gong, Hongfa Wang, Zhifeng Li, Wei Liu.*<br>
[[paper]](https://arxiv.org/abs/2204.03382v8)

**(*CVPR2022_MILES*) MILES: Visual BERT Pre-training with Injected Language Semantics for Video-text Retrieval.** <br>
*Yuying Ge, Yixiao Ge, Xihui Liu, Alex Jinpeng Wang, Jianping Wu, Ying Shan, Xiaohu Qie, Ping Luo.*<br>
[[paper]](https://arxiv.org/abs/2204.12408)
[[code]](https://github.com/tencentarc/mcq)

**(*SIGIR2022_CenterCLIP*) CenterCLIP: Token Clustering for Efficient Text-Video Retrieval.** <br>
*Shuai Zhao, Linchao Zhu, Xiaohan Wang, Yi Yang.*<br>
[[paper]](https://arxiv.org/abs/2205.00823)
[[code]](https://github.com/mzhaoshuai/CenterCLIP)

**(*arXiv2022_VidIL*) Language Models with Image Descriptors are Strong Few-Shot Video-Language Learners.** <br>
*Zhenhailong Wang, Manling Li, Ruochen Xu, Luowei Zhou, Jie Lei, Xudong Lin, Shuohang Wang, Ziyi Yang, Chenguang Zhu, Derek Hoiem, Shih-Fu Chang, Mohit Bansal, Heng Ji.*<br>
[[paper]](https://arxiv.org/abs/2205.10747)
[[code]](https://github.com/MikeWangWZHL/VidIL)

**(*arXiv2022_VL-BEiT*) VL-BEiT: Generative Vision-Language Pretraining.** <br>
*Hangbo Bao, Wenhui Wang, Li Dong, Furu Wei.*<br>
[[paper]](https://arxiv.org/abs/2206.01127)

**(*ACL2023_Singularity*) Revealing Single Frame Bias for Video-and-Language Learning.** <br>
*Jie Lei, Tamara L. Berg, Mohit Bansal.*<br>
[[paper]](https://arxiv.org/abs/2206.03428v1)
[[code]](https://github.com/jayleicn/singularity)

**(*CVPR2023_LAVENDER*) LAVENDER: Unifying Video-Language Understanding as Masked Language Modeling.** <br>
*Linjie Li, Zhe Gan, Kevin Lin, Chung-Ching Lin, Zicheng Liu, Ce Liu, Lijuan Wang.*<br>
[[paper]](https://arxiv.org/abs/2206.07160)
[[code]](https://github.com/microsoft/LAVENDER)

**(*NeurIPS2022_FrozenBiLM*) Zero-Shot Video Question Answering via Frozen Bidirectional Language Models.** <br>
*Antoine Yang, Antoine Miech, Josef Sivic, Ivan Laptev, Cordelia Schmid.*<br>
[[paper]](https://arxiv.org/abs/2206.08155)
[[code]](https://github.com/antoyang/FrozenBiLM)

**(*arXiv2022_LaT*) LaT: Latent Translation with Cycle-Consistency for Video-Text Retrieval.** <br>
*Jinbin Bai, Chunhui Liu, Feiyue Ni, Haofan Wang, Mengying Hu, Xiaofeng Guo, Lele Cheng.*<br>
[[paper]](https://arxiv.org/abs/2207.04858)

**(*ACMMM2022_X-CLIP*) X-CLIP: End-to-End Multi-grained Contrastive Learning for Video-Text Retrieval.** <br>
*Yiwei Ma, Guohai Xu, Xiaoshuai Sun, Ming Yan, Ji Zhang, Rongrong Ji.*<br>
[[paper]](https://arxiv.org/abs/2207.07285v2)
[[code]](https://github.com/xuguohai/X-CLIP)

**(*ECCV2022_TS2-Net*) TS2-Net: Token Shift and Selection Transformer for Text-Video Retrieval.** <br>
*Yuqi Liu, Pengfei Xiong, Luhui Xu, Shengming Cao, Qin Jin.*<br>
[[paper]](https://arxiv.org/abs/2207.07852v1)
[[code]](https://github.com/yuqi657/ts2_net)

**(*CVPR2023_Clover*) Clover: Towards A Unified Video-Language Alignment and Fusion Model.** <br>
*Jingjia Huang, Yinan Li, Jiashi Feng, Xinglong Wu, Xiaoshuai Sun, Rongrong Ji.*<br>
[[paper]](https://arxiv.org/abs/2207.07885v3)
[[code]](https://github.com/LeeYN-43/Clover)

**(*SIGIR2022_CRET*) CRET: Cross-Modal Retrieval Transformer for Efficient Text-Video Retrieval.** <br>
*Kaixiang Ji, Jiajia Liu, Weixiang Hong, Liheng Zhong, Jian Wang, Jingdong Chen, Wei Chu.*<br>
[[paper]](https://dl.acm.org/doi/abs/10.1145/3477495.3531960)

**(*ECCV2022_LocVTP*) LocVTP: Video-Text Pre-training for Temporal Localization.** <br>
*Meng Cao, Tianyu Yang, Junwu Weng, Can Zhang, Jue Wang, Yuexian Zou.*<br>
[[paper]](https://arxiv.org/abs/2207.10362)
[[code]](https://github.com/mengcaopku/LocVTP)

**(*ICLR2023_CLIP-ViP*) CLIP-ViP: Adapting Pre-trained Image-Text Model to Video-Language Representation Alignment.** <br>
*Hongwei Xue, Yuchong Sun, Bei Liu, Jianlong Fu, Ruihua Song, Houqiang Li, Jiebo Luo.*<br>
[[paper]](https://arxiv.org/abs/2209.06430v4)
[[code]](https://github.com/microsoft/XPretrain/tree/main/CLIP-ViP)

**(*NeurIPS2022_OmniVL*) OmniVL: One Foundation Model for Image-Language and Video-Language Tasks.** <br>
*Junke Wang, Dongdong Chen, Zuxuan Wu, Chong Luo, Luowei Zhou, Yucheng Zhao, Yujia Xie, Ce Liu, Yu-Gang Jiang, Lu Yuan.*<br>
[[paper]](https://arxiv.org/abs/2209.07526)

**(*NeurIPS2022_LGDN*) LGDN: Language-Guided Denoising Network for Video-Language Modeling.** <br>
*Haoyu Lu, Mingyu Ding, Nanyi Fei, Yuqi Huo, Zhiwu Lu.*<br>
[[paper]](https://arxiv.org/abs/2209.11388)

**(*NeurIPS2022_EMCL*) Expectation-Maximization Contrastive Learning for Compact Video-and-Language Representations.** <br>
*Peng Jin, Jinfa Huang, Fenglin Liu, Xian Wu, Shen Ge, Guoli Song, David A. Clifton, Jie Chen.*<br>
[[paper]](https://arxiv.org/abs/2211.11427v1)
[[code]](https://github.com/jpthu17/emcl)

**(*arXiv2022_MAC*) Masked Contrastive Pre-Training for Efficient Video-Text Retrieval.** <br>
*Fangxun Shu, Biaolong Chen, Yue Liao, Shuwen Xiao, Wenyu Sun, Xiaobo Li, Yousong Zhu, Jinqiao Wang, Si Liu.*<br>
[[paper]](https://arxiv.org/abs/2212.00986)
[[code]](https://github.com/shufangxun/MAC)

**(*CVPR2023_VindLU*) VindLU: A Recipe for Effective Video-and-Language Pretraining.** <br>
*Feng Cheng, Xizi Wang, Jie Lei, David Crandall, Mohit Bansal, Gedas Bertasius.*<br>
[[paper]](https://arxiv.org/abs/2212.05051v2)
[[code]](https://github.com/klauscc/VindLU)

**(*ICCV2023_HiTeA*) HiTeA: Hierarchical Temporal-Aware Video-Language Pre-training.** <br>
*Qinghao Ye, Guohai Xu, Ming Yan, Haiyang Xu, Qi Qian, Ji Zhang, Fei Huang.*<br>
[[paper]](https://arxiv.org/abs/2212.14546v1)

**(*CVPR2023_BIKE*) Bidirectional Cross-Modal Knowledge Exploration for Video Recognition with Pre-trained Vision-Language Models.** <br>
*Wenhao Wu, Xiaohan Wang, Haipeng Luo, Jingdong Wang, Yi Yang, Wanli Ouyang.*<br>
[[paper]](https://arxiv.org/abs/2301.00182)
[[code]](https://github.com/whwu95/BIKE)

**(*CVPR2023_Cap4Video*) Cap4Video: What Can Auxiliary Captions Do for Text-Video Retrieval?.** <br>
*Wenhao Wu, Haipeng Luo, Bo Fang, Jingdong Wang, Wanli Ouyang.*<br>
[[paper]](https://arxiv.org/abs/2301.00184)
[[code]](https://github.com/whwu95/Cap4Video)

**(*CVPR2023_STAN*) Revisiting Temporal Modeling for CLIP-based Image-to-Video Knowledge Transferring.** <br>
*Ruyang Liu, Jingjia Huang, Ge Li, Jiashi Feng, Xinglong Wu, Thomas H. Li.*<br>
[[paper]](https://arxiv.org/abs/2301.11116v1)
[[code]](https://github.com/farewellthree/STAN)

**(*EMNLP2023_S3MA*) Video-Text Retrieval by Supervised Sparse Multi-Grained Learning.** <br>
*Yimu Wang, Peng Shi.*<br>
[[paper]](https://arxiv.org/abs/2302.09473v2)
[[code]](https://github.com/yimuwangcs/Better_Cross_Modal_Retrieval)

**(*AAAI2023_STOA-VLP*) STOA-VLP: Spatial-Temporal Modeling of Object and Action for Video-Language Pre-training.** <br>
*Weihong Zhong, Mao Zheng, Duyu Tang, Xuan Luo, Heng Gong, Xiaocheng Feng, Bing Qin.*<br>
[[paper]](https://arxiv.org/abs/2302.09736)

**(*CVPR2023_Vid2Seq*) Vid2Seq: Large-Scale Pretraining of a Visual Language Model for Dense Video Captioning.** <br>
*Antoine Yang, Arsha Nagrani, Paul Hongsuck Seo, Antoine Miech, Jordi Pont-Tuset, Ivan Laptev, Josef Sivic, Cordelia Schmid.*<br>
[[paper]](https://arxiv.org/abs/2302.14115)
[[code]](https://antoyang.github.io/vid2seq.html)

**(*AAAI2024_MuLTI*) MuLTI: Efficient Video-and-Language Understanding with Text-Guided MultiWay-Sampler and Multiple Choice Modeling.** <br>
*Jiaqi Xu, Bo Liu, Yunkuo Chen, Mengli Cheng, Xing Shi.*<br>
[[paper]](https://arxiv.org/abs/2303.05707v2)

**(*CVPRW2023_Cali-NCE*) Cali-NCE: Boosting Cross-modal Video Representation Learning with Calibrated Alignment.** <br>
*Nanxuan Zhao, Jianbo Jiao, Weidi Xie, Dahua Lin.*<br>
[[paper]](https://openaccess.thecvf.com/content/CVPR2023W/WFM/papers/Zhao_Cali-NCE_Boosting_Cross-Modal_Video_Representation_Learning_With_Calibrated_Alignment_CVPRW_2023_paper.pdf)
[[code]](https://github.com/nanxuanzhao/Cali-NCE)

**(*CVPR2023_CLIPPING*) CLIPPING: Distilling CLIP-Based Models with a Student Base for Video-Language Retrieval.** <br>
*Renjing Pei, Jianzhuang Liu, Weimian Li, Bin Shao, Songcen Xu, Peng Dai, Juwei Lu, Youliang Yan.*<br>
[[paper]](https://openaccess.thecvf.com/content/CVPR2023/papers/Pei_CLIPPING_Distilling_CLIP-Based_Models_With_a_Student_Base_for_Video-Language_CVPR_2023_paper.pdf)

**(*CVPR2023_SViTT*) SViTT: Temporal Learning of Sparse Video-Text Transformers.** <br>
*Yi Li, Kyle Min, Subarna Tripathi, Nuno Vasconcelos.*<br>
[[paper]](https://openaccess.thecvf.com/content/CVPR2023/papers/Li_SViTT_Temporal_Learning_of_Sparse_Video-Text_Transformers_CVPR_2023_paper.pdf)
[[code]](http://svcl.ucsd.edu/projects/svitt)

**(*CVPR2023_LAVENDER*) LAVENDER: Unifying Video-Language Understanding as Masked Language Modeling.** <br>
*Linjie Li, Zhe Gan, Kevin Lin, Chung-Ching Lin, Zicheng Liu, Ce Liu, Lijuan Wang.*<br>
[[paper]](https://openaccess.thecvf.com/content/CVPR2023/papers/Li_LAVENDER_Unifying_Video-Language_Understanding_As_Masked_Language_Modeling_CVPR_2023_paper.pdf)
[[code]](https://github.com/microsoft/LAVENDER)

**(*ICCV2023_DiffusionRet*) DiffusionRet: Generative Text-Video Retrieval with Diffusion Model.** <br>
*Peng Jin, Hao Li, Zesen Cheng, Kehan Li, Xiangyang Ji, Chang Liu, Li Yuan, Jie Chen.*<br>
[[paper]](https://arxiv.org/abs/2303.09867v2)
[[code]](https://github.com/jpthu17/DiffusionRet)

**(*CVPR2023_MELTR*) MELTR: Meta Loss Transformer for Learning to Fine-tune Video Foundation Models.** <br>
*Dohwan Ko, Joonmyung Choi, Hyeong Kyu Choi, Kyoung-Woon On, Byungseok Roh, Hyunwoo J. Kim.*<br>
[[paper]](https://arxiv.org/abs/2303.13009v1)
[[code]](https://github.com/mlvlab/MELTR)

**(*CVPR2023_HBI*) Video-Text as Game Players: Hierarchical Banzhaf Interaction for Cross-Modal Representation Learning.** <br>
*Peng Jin, Jinfa Huang, Pengfei Xiong, Shangxuan Tian, Chang Liu, Xiangyang Ji, Li Yuan, Jie Chen.*<br>
[[paper]](https://arxiv.org/abs/2303.14369v1)
[[code]](https://github.com/jpthu17/HBI)

**(*arXiv2023_VLAB*) VLAB: Enhancing Video Language Pre-training by Feature Adapting and Blending.** <br>
*Xingjian He, Sihan Chen, Fan Ma, Zhicheng Huang, Xiaojie Jin, Zikang Liu, Dongmei Fu, Yi Yang, Jing Liu, Jiashi Feng.*<br>
[[paper]](https://arxiv.org/abs/2305.13167)

**(*ICCV2023_PIDRo*) PIDRo: Parallel Isomeric Attention with Dynamic Routing for Text-Video Retrieval.** <br>
*Peiyan Guan, Renjing Pei, Bin Shao, Jianzhuang Liu2, Weimian Li, Jiaxi Gu, Hang Xu, Songcen Xu, Youliang Yan, Edmund Y. Lam.*<br>
[[paper]](https://openaccess.thecvf.com//content/ICCV2023/papers/Guan_PIDRo_Parallel_Isomeric_Attention_with_Dynamic_Routing_for_Text-Video_Retrieval_ICCV_2023_paper.pdf)

**(*ICCV2023_Eventful-Transformers*) Eventful Transformers: Leveraging Temporal Redundancy in Vision Transformers.** <br>
*Matthew Dutson, Yin Li, Mohit Gupta.*<br>
[[paper]](https://arxiv.org/abs/2308.13494)
[[code]](https://github.com/WISION-Lab/eventful-transformer)

**(*ICCV2023_UCoFiA*) Unified Coarse-to-Fine Alignment for Video-Text Retrieval.** <br>
*Ziyang Wang, Yi-Lin Sung, Feng Cheng, Gedas Bertasius, Mohit Bansal.*<br>
[[paper]](https://arxiv.org/abs/2309.10091)
[[code]](https://github.com/Ziyang412/UCoFiA)

**(*ACMMM2023_DMAE*) Dual-Modal Attention-Enhanced Text-Video Retrieval with Triplet Partial Margin Contrastive Learning.** <br>
*Chen Jiang, Hong Liu, Xuzheng Yu, Qing Wang, Yuan Cheng, Jia Xu, Zhongyi Liu, Qingpei Guo, Wei Chu, Ming Yang, Yuan Qi.*<br>
[[paper]](https://arxiv.org/abs/2309.11082v3)
[[code]](https://github.com/alipay/Ant-Multi-Modal-Framework)

**(*ICLR2024_Norton*) Multi-granularity Correspondence Learning from Long-term Noisy Videos.**<br>
*Yijie Lin, Jie Zhang, Zhenyu Huang, Jia Liu, Zujie Wen, Xi Peng.*<br>
[[paper]](https://openreview.net/pdf?id=9Cu8MRmhq2)
[[code]](https://github.com/XLearning-SCU/2024-ICLR-Norton)

**(*COLING2024_UNIFY*) Unifying Latent and Lexicon Representations for Effective Video-Text Retrieval.**<br>
*Haowei Liu, Yaya Shi, Haiyang Xu, Chunfeng Yuan, Qinghao Ye, Chenliang Li, Ming Yan, Ji Zhang, Fei Huang, Bing Li, Weiming Hu.*<br>
[[paper]](https://arxiv.org/abs/2402.16769)


### ``*Image-Language Datasets*``

**(*NIPS2011_SBU*) Im2Text: Describing Images Using 1 Million Captioned Photographs.** <br>
*Vicente Ordonez, Girish Kulkarni, Tamara Berg.*<br>
[[paper]](https://proceedings.neurips.cc/paper/2011/file/5dd9db5e033da9c6fb5ba83c7a7ebea9-Paper.pdf)

**(*CACM2016_YFCC100M*) YFCC100M: The New Data in Multimedia Research.** <br>
*Bart Thomee, David A. Shamma, Gerald Friedland, Benjamin Elizalde, Karl Ni, Douglas Poland, Damian Borth, Li-Jia Li.*<br>
[[paper]](https://arxiv.org/abs/1503.01817v2)

**(*IJCV2017_VG*) Visual Genome: Connecting Language and Vision Using Crowdsourced Dense Image Annotations.** <br>
*Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua Kravitz, Stephanie Chen, Yannis Kalantidis, Li-Jia Li, David A. Shamma, Michael S. Bernstein, Fei-Fei Li.*<br>
[[paper]](https://arxiv.org/abs/1602.07332)

**(*ICCV2017_JFT-300M*) Revisiting Unreasonable Effectiveness of Data in Deep Learning Era.** <br>
*Chen Sun, Abhinav Shrivastava, Saurabh Singh, Abhinav Gupta.*<br>
[[paper]](https://arxiv.org/abs/1707.02968v2)

**(*ECCV2020_TextCaps*) TextCaps: a Dataset for Image Captioning with Reading Comprehension.** <br>
*Oleksii Sidorov, Ronghang Hu, Marcus Rohrbach, Amanpreet Singh.*<br>
[[paper]](https://arxiv.org/abs/2003.12462)
[[code]](https://textvqa.org/textcaps/)

**(*SIGIR2021_WIT*) WIT: Wikipedia-based Image Text Dataset for Multimodal Multilingual Machine Learning.** <br>
*Krishna Srinivasan, Karthik Raman, Jiecao Chen, Michael Bendersky, Marc Najork.*<br>
[[paper]](https://arxiv.org/abs/2103.01913)
[[code]](https://github.com/google-research-datasets/wit)

**(*CVPR2021_CC-12M*) Conceptual 12M: Pushing Web-Scale Image-Text Pre-Training To Recognize Long-Tail Visual Concepts.** <br>
*Soravit Changpinyo, Piyush Sharma, Nan Ding, Radu Soricut.*<br>
[[paper]](https://arxiv.org/abs/2102.08981)
[[code]](https://github.com/google-research-datasets/conceptual-12m)

**(*NeurIPS2022_VLMo*) VLMo: Unified Vision-Language Pre-Training with Mixture-of-Modality-Experts.** <br>
*Hangbo Bao, Wenhui Wang, Li Dong, Qiang Liu, Owais Khan Mohammed, Kriti Aggarwal, Subhojit Som, Furu Wei.*<br>
[[paper]](https://arxiv.org/abs/2111.02358)
[[code]](https://aka.ms/vlmo)

**(*CVPR2022_LiT*) LiT: Zero-Shot Transfer with Locked-image text Tuning.** <br>
*Xiaohua Zhai, Xiao Wang, Basil Mustafa, Andreas Steiner, Daniel Keysers, Alexander Kolesnikov, Lucas Beyer.*<br>
[[paper]](https://arxiv.org/abs/2111.07991)
[[code]](https://github.com/google-research/vision_transformer#lit-models)

**(*NeurIPS2021_RedCaps*) RedCaps: web-curated image-text data created by the people, for the people.** <br>
*Karan Desai, Gaurav Kaul, Zubin Aysola, Justin Johnson.*<br>
[[paper]](https://arxiv.org/abs/2111.11431)
[[code]](https://redcaps.xyz)

**(*CVPR2022_ALT200M*) Scaling Up Vision-Language Pre-training for Image Captioning.** <br>
*Xiaowei Hu, Zhe Gan, Jianfeng Wang, Zhengyuan Yang, Zicheng Liu, Yumao Lu, Lijuan Wang.*<br>
[[paper]](https://arxiv.org/abs/2111.12233)
[[code]](https://github.com/xiaoweihu/ALT200M)

**(*TMLR2022_GIT*) GIT: A Generative Image-to-text Transformer for Vision and Language.** <br>
*Jianfeng Wang, Zhengyuan Yang, Xiaowei Hu, Linjie Li, Kevin Lin, Zhe Gan, Zicheng Liu, Ce Liu, Lijuan Wang.*<br>
[[paper]](https://arxiv.org/abs/2205.14100)
[[code]](https://github.com/microsoft/GenerativeImage2Text)

**(*ICLR2023_WebLI*) PaLI: A Jointly-Scaled Multilingual Language-Image Model.** <br>
*Xi Chen, Xiao Wang, Soravit Changpinyo, AJ Piergiovanni, Piotr Padlewski, Daniel Salz, Sebastian Goodman, Adam Grycner, Basil Mustafa, Lucas Beyer, Alexander Kolesnikov, Joan Puigcerver, Nan Ding, Keran Rong, Hassan Akbari, Gaurav Mishra, Linting Xue, Ashish Thapliyal, James Bradbury, Weicheng Kuo, Mojtaba Seyedhosseini, Chao Jia, Burcu Karagol Ayan, Carlos Riquelme, Andreas Steiner, Anelia Angelova, Xiaohua Zhai, Neil Houlsby, Radu Soricut.*<br>
[[paper]](https://arxiv.org/abs/2209.06794)

**(*NeurIPS2022_LAION-5B*) LAION-5B: An open large-scale dataset for training next generation image-text models.** <br>
*Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, Patrick Schramowski, Srivatsa Kundurthy, Katherine Crowson, Ludwig Schmidt, Robert Kaczmarczyk, Jenia Jitsev.*<br>
[[paper]](https://arxiv.org/abs/2210.08402)
[[code]](https://github.com/mlfoundations/open_clip)

**(*Github2022_COYO-700M*) COYO-700M: Image-Text Pair Dataset.** <br>
*Byeon, Minwoo and Park, Beomhee and Kim, Haecheon and Lee, Sungjun and Baek, Woonhyuk and Kim, Saehoon.*<br>
[[code]](https://github.com/kakaobrain/coyo-dataset)

**(*Blog2023_LAION POP*) LAION POP: 600,000 High-resolution Images with Detailed Descriptions.** <br>
*Christoph Schuhmann, Peter Bevan.*<br>
[[code]](https://laion.ai/blog/laion-pop/)

**(*SIGIR2023_COCO-F30K-FG*) Rethinking Benchmarks for Cross-modal Image-text Retrieval.** <br>
*Weijing Chen, Linli Yao, Qin Jin.*<br>
[[paper]](https://arxiv.org/abs/2304.10824)
[[code]](https://github.com/cwj1412/MSCOCO-Flikcr30K_FG)

**(*arXiv2023_OBELICS*) OBELICS: An Open Web-Scale Filtered Dataset of Interleaved Image-Text Documents.** <br>
*Hugo Laurençon, Lucile Saulnier, Léo Tronchon, Stas Bekman, Amanpreet Singh, Anton Lozhkov, Thomas Wang, Siddharth Karamcheti, Alexander M. Rush, Douwe Kiela, Matthieu Cord, Victor Sanh.*<br>
[[paper]](https://arxiv.org/abs/2306.16527)
[[code]](https://github.com/huggingface/obelics)



### ``*Video-Language Datasets*``

**(*ACL2011_MSVD*) Collecting Highly Parallel Data for Paraphrase Evaluation.** <br>
*David L. Chen, William B. Dolan.*<br>
[[paper]](https://aclanthology.org/P11-1020.pdf)

**(*CVPR2015_ActivityNet*) ActivityNet: A Large-Scale Video Benchmark for Human Activity Understanding.** <br>
*Fabian Caba Heilbron, Victor Escorcia, Bernard Ghanem, Juan Carlos Niebles.*<br>
[[paper]](https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Heilbron_ActivityNet_A_Large-Scale_2015_CVPR_paper.pdf)

**(*CVPR2016_MSR-VTT*) MSR-VTT: A Large Video Description Dataset for Bridging Video and Language.** <br>
*Jun Xu, Tao Mei, Ting Yao, Yong Rui.*<br>
[[paper]](https://openaccess.thecvf.com/content_cvpr_2016/papers/Xu_MSR-VTT_A_Large_CVPR_2016_paper.pdf)

**(*CVPR2016_TGIF*) TGIF: A New Dataset and Benchmark on Animated GIF Description.** <br>
*Yuncheng Li, Yale Song, Liangliang Cao, Joel Tetreault, Larry Goldberg, Alejandro Jaimes, Jiebo Luo.*<br>
[[paper]](https://arxiv.org/abs/1604.02748)

**(*IJCV2017_LSMDC*) Movie Description.** <br>
*Anna Rohrbach, Atousa Torabi, Marcus Rohrbach, Niket Tandon, Christopher Pal, Hugo Larochelle, Aaron Courville, Bernt Schiele.*<br>
[[paper]](https://arxiv.org/abs/1605.03705)

**(*arXiv2016_YouTube-8M*) YouTube-8M: A Large-Scale Video Classification Benchmark.** <br>
*Sami Abu-El-Haija, Nisarg Kothari, Joonseok Lee, Paul Natsev, George Toderici, Balakrishnan Varadarajan, Sudheendra Vijayanarasimhan.*<br>
[[paper]](https://arxiv.org/abs/1609.08675)

**(*AAAI2018_YouCook2*) Towards Automatic Learning of Procedures from Web Instructional Videos.** <br>
*Luowei Zhou, Chenliang Xu, Jason J. Corso.*<br>
[[paper]](https://arxiv.org/abs/1703.09788)

**(*CVPR2017_Kinetics-400*) Quo Vadis, Action Recognition? A New Model and the Kinetics Dataset.** <br>
*Joao Carreira, Andrew Zisserman.*<br>
[[paper]](https://arxiv.org/abs/1705.07750)

**(*CVPR2018_AVA*) AVA: A Video Dataset of Spatio-temporally Localized Atomic Visual Actions.** <br>
*Chunhui Gu, Chen Sun, David A. Ross, Carl Vondrick, Caroline Pantofaru, Yeqing Li, Sudheendra Vijayanarasimhan, George Toderici, Susanna Ricco, Rahul Sukthankar, Cordelia Schmid, Jitendra Malik.*<br>
[[paper]](https://arxiv.org/abs/1705.08421)

**(*ICCV2017_Something-Something V2*) The "something something" video database for learning and evaluating visual common sense.** <br>
*Raghav Goyal, Samira Ebrahimi Kahou, Vincent Michalski, Joanna Materzyńska, Susanne Westphal, Heuna Kim, Valentin Haenel, Ingo Fruend, Peter Yianilos, Moritz Mueller-Freitag, Florian Hoppe, Christian Thurau, Ingo Bax, Roland Memisevic.*<br>
[[paper]](https://arxiv.org/abs/1706.04261)

**(*ICCV2017_DiDeMo*) Localizing Moments in Video with Natural Language.** <br>
*Lisa Anne Hendricks, Oliver Wang, Eli Shechtman, Josef Sivic, Trevor Darrell, Bryan Russell.*<br>
[[paper]](https://arxiv.org/abs/1708.01641v1)

**(*arXiv2018_Kinetics-600*) A Short Note about Kinetics-600.** <br>
*Joao Carreira, Eric Noland, Andras Banki-Horvath, Chloe Hillier, Andrew Zisserman.*<br>
[[paper]](https://arxiv.org/abs/1808.01340)

**(*arXiv2018_How2*) How2: A Large-scale Dataset for Multimodal Language Understanding.** <br>
*Ramon Sanabria, Ozan Caglayan, Shruti Palaskar, Desmond Elliott, Loïc Barrault, Lucia Specia, Florian Metze.*<br>
[[paper]](https://arxiv.org/abs/1811.00347)

**(*ICCV2019_VATEX*) VATEX: A Large-Scale, High-Quality Multilingual Dataset for Video-and-Language Research.** <br>
*Xin Wang, Jiawei Wu, Junkun Chen, Lei Li, Yuan-Fang Wang, William Yang Wang.*<br>
[[paper]](https://arxiv.org/abs/1904.03493)

**(*arXiv2019_Kinetics-700*) A Short Note on the Kinetics-700 Human Action Dataset.** <br>
*Joao Carreira, Eric Noland, Chloe Hillier, Andrew Zisserman.*<br>
[[paper]](https://arxiv.org/abs/1907.06987)

**(*ICCV2019_HowTo100M*) HowTo100M: Learning a Text-Video Embedding by Watching Hundred Million Narrated Video Clips.** <br>
*Antoine Miech, Dimitri Zhukov, Jean-Baptiste Alayrac, Makarand Tapaswi, Ivan Laptev, Josef Sivic.*<br>
[[paper]](https://arxiv.org/abs/1906.03327)

**(*arXiv2020_WTS70M*) Learning Video Representations from Textual Web Supervision.** <br>
*Jonathan C. Stroud, Zhichao Lu, Chen Sun, Jia Deng, Rahul Sukthankar, Cordelia Schmid, David A. Ross.*<br>
[[paper]](https://arxiv.org/abs/2007.14937)

**(*ICCV2021_WebVid10M*) Frozen in Time: A Joint Video and Image Encoder for End-to-End Retrieval.** <br>
*Max Bain, Arsha Nagrani, Gül Varol, Andrew Zisserman.*<br>
[[paper]](https://arxiv.org/abs/2104.00650)

**(*NeurIPS2021_YT-Temporal-180M*) MERLOT: Multimodal Neural Script Knowledge Models.** <br>
*Rowan Zellers, Ximing Lu, Jack Hessel, Youngjae Yu, Jae Sung Park, Jize Cao, Ali Farhadi, Yejin Choi.*<br>
[[paper]](https://arxiv.org/abs/2106.02636)

**(*CVPR2022_HD-VILA-100M*) Advancing High-Resolution Video-Language Representation with Large-Scale Video Transcriptions.** <br>
*Hongwei Xue, Tiankai Hang, Yanhong Zeng, Yuchong Sun, Bei Liu, Huan Yang, Jianlong Fu, Baining Guo.*<br>
[[paper]](https://arxiv.org/abs/2111.10337)

**(*ECCV2022_VideoCC3M*) Learning Audio-Video Modalities from Image Captions.** <br>
*Arsha Nagrani, Paul Hongsuck Seo, Bryan Seybold, Anja Hauth, Santiago Manen, Chen Sun, Cordelia Schmid.*<br>
[[paper]](https://arxiv.org/abs/2204.00679)

**(*CVPR2022_Tencent-MVSE*) Tencent-MVSE: A Large-Scale Benchmark Dataset for Multi-Modal Video Similarity Evaluation.** <br>
*Zhaoyang Zeng, Yongsheng Luo, Zhenhua Liu, Fengyun Rao, Dian Li, Weidong Guo, Zhen Wen.*<br>
[[paper]](https://openaccess.thecvf.com/content/CVPR2022/papers/Zeng_Tencent-MVSE_A_Large-Scale_Benchmark_Dataset_for_Multi-Modal_Video_Similarity_Evaluation_CVPR_2022_paper.pdf)

**(*CVPR2023_CNVid-3.5M*) CNVid-3.5M: Build, Filter, and Pre-train the Large-scale Public Chinese Video-text Dataset.** <br>
*Tian Gan, Qing Wang, Xingning Dong, Xiangyuan Ren, Liqiang Nie, Qingpei Guo.*<br>
[[paper]](https://openaccess.thecvf.com/content/CVPR2023/papers/Gan_CNVid-3.5M_Build_Filter_and_Pre-Train_the_Large-Scale_Public_Chinese_Video-Text_CVPR_2023_paper.pdf)

**(*arXiv2023_Youku-mPLUG*) Youku-mPLUG: A 10 Million Large-scale Chinese Video-Language Dataset for Pre-training and Benchmarks.** <br>
*Haiyang Xu, Qinghao Ye, Xuan Wu, Ming Yan, Yuan Miao, Jiabo Ye, Guohai Xu, Anwen Hu, Yaya Shi, Guangwei Xu, Chenliang Li, Qi Qian, Maofei Que, Ji Zhang, Xiao Zeng, Fei Huang.*<br>
[[paper]](https://arxiv.org/abs/2306.04362)

**(*arXiv2023_InternVid-10M*) InternVid: A Large-scale Video-Text Dataset for Multimodal Understanding and Generation.** <br>
*Yi Wang, Yinan He, Yizhuo Li, Kunchang Li, Jiashuo Yu, Xin Ma, Xinyuan Chen, Yaohui Wang, Ping Luo, Ziwei Liu, Yali Wang, Limin Wang, Yu Qiao.*<br>
[[paper]](https://arxiv.org/abs/2307.06942)

**(*arXiv2023_VideoCon*) VideoCon: Robust Video-Language Alignment via Contrast Captions.** <br>
*Hritik Bansal, Yonatan Bitton, Idan Szpektor, Kai-Wei Chang, Aditya Grover.*<br>
[[paper]](https://arxiv.org/abs/2311.10111)
