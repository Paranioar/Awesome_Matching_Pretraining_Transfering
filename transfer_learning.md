Methods Summary of Parameter-Efficient Finetuning
==============================

## ``Catalogue ``
* [Prompt Tuning](#prompt-tuning)
* [Adapter Tuning](#adapter-tuning)
* [Partially Tuning](#partially-tuning)
* [Side Tuning](#side-tuning)
* [Unified Tuning](#unified-tuning)
* [Posted in](#posted-in)


### ``*Prompt Tuning*``

**(*ACL2021_Prefix-Tuning*) Prefix-Tuning: Optimizing Continuous Prompts for Generation.** <br>
*Xiang Lisa Li, Percy Liang.*<br>
[[paper]](https://arxiv.org/abs/2101.00190)

**(*EMNLP2021_PEPT*) The Power of Scale for Parameter-Efficient Prompt Tuning.** <br>
*Brian Lester, Rami Al-Rfou, Noah Constant.*<br>
[[paper]](https://arxiv.org/abs/2104.08691)

**(*NeurIPS2021_Frozen*) Multimodal Few-Shot Learning with Frozen Language Models.** <br>
*Maria Tsimpoukelli, Jacob Menick, Serkan Cabi, S. M. Ali Eslami, Oriol Vinyals, Felix Hill.*<br>
[[paper]](https://arxiv.org/abs/2106.13884)

**(*IJCV2022_CoOp*) Learning to Prompt for Vision-Language Models.** <br>
*Kaiyang Zhou, Jingkang Yang, Chen Change Loy, Ziwei Liu.*<br>
[[paper]](https://arxiv.org/abs/2109.01134)
[[code]](https://github.com/KaiyangZhou/CoOp)

**(*ACL2022_PPT*) PPT: Pre-trained Prompt Tuning for Few-shot Learning.** <br>
*Yuxian Gu, Xu Han, Zhiyuan Liu, Minlie Huang.*<br>
[[paper]](https://arxiv.org/abs/2109.04332)
[[code]](https://github.com/thu-coai/PPT)

**(*arXiv2021_CPT*) CPT: Colorful Prompt Tuning for Pre-trained Vision-Language Models.** <br>
*Yuan Yao, Ao Zhang, Zhengyan Zhang, Zhiyuan Liu, Tat-Seng Chua, Maosong Sun.*<br>
[[paper]](https://arxiv.org/abs/2109.11797)
[[code]](https://github.com/thunlp/CPT)

**(*CVPR2022_DenseCLIP*) DenseCLIP: Language-Guided Dense Prediction with Context-Aware Prompting.** <br>
*Yongming Rao, Wenliang Zhao, Guangyi Chen, Yansong Tang, Zheng Zhu, Guan Huang, Jie Zhou, Jiwen Lu.*<br>
[[paper]](https://arxiv.org/abs/2112.01518)
[[code]](https://github.com/raoyongming/DenseCLIP)

**(*ECCV2022_Efficient-Prompt*) Prompting Visual-Language Models for Efficient Video Understanding.** <br>
*Chen Ju, Tengda Han, Kunhao Zheng, Ya Zhang, Weidi Xie.*<br>
[[paper]](https://arxiv.org/abs/2112.04478)
[[code]](https://github.com/ju-chen/Efficient-Prompt)

**(*CVPR2022_L2P*) Learning to Prompt for Continual Learning.** <br>
*Zifeng Wang, Zizhao Zhang, Chen-Yu Lee, Han Zhang, Ruoxi Sun, Xiaoqi Ren, Guolong Su, Vincent Perot, Jennifer Dy, Tomas Pfister.*<br>
[[paper]](https://arxiv.org/abs/2112.08654)
[[code]](https://github.com/google-research/l2p)

**(*ICML2022_Language-Planners*) Language Models as Zero-Shot Planners: Extracting Actionable Knowledge for Embodied Agents.** <br>
*Wenlong Huang, Pieter Abbeel, Deepak Pathak, Igor Mordatch.*<br>
[[paper]](https://arxiv.org/abs/2201.07207)
[[code]](https://wenlong.page/language-planner/)

**(*NeurIPS2022_CoT*) Chain-of-Thought Prompting Elicits Reasoning in Large Language Models.** <br>
*Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, Denny Zhou.*<br>
[[paper]](https://arxiv.org/abs/2201.11903)

**(*CVPR2022_CoCoOp*) Conditional Prompt Learning for Vision-Language Models.** <br>
*Kaiyang Zhou, Jingkang Yang, Chen Change Loy, Ziwei Liu.*<br>
[[paper]](https://arxiv.org/abs/2203.05557)
[[code]](https://github.com/KaiyangZhou/CoOp)

**(*ECCV2022_VPT*) Visual Prompt Tuning.** <br>
*Menglin Jia, Luming Tang, Bor-Chun Chen, Claire Cardie, Serge Belongie, Bharath Hariharan, Ser-Nam Lim.*<br>
[[paper]](https://arxiv.org/abs/2203.12119)
[[code]](https://github.com/kmnp/vpt)

**(*arXiv2022_Visual-Prompting*) Exploring Visual Prompts for Adapting Large-Scale Models.** <br>
*Hyojin Bahng, Ali Jahanian, Swami Sankaranarayanan, Phillip Isola.*<br>
[[paper]](https://arxiv.org/abs/2203.17274)
[[code]](http://hjbahng.github.io/visual_prompting)

**(*ECCV2022_DualPrompt*) DualPrompt: Complementary Prompting for Rehearsal-free Continual Learning.** <br>
*Zifeng Wang, Zizhao Zhang, Sayna Ebrahimi, Ruoxi Sun, Han Zhang, Chen-Yu Lee, Xiaoqi Ren, Guolong Su, Vincent Perot, Jennifer Dy, Tomas Pfister.*<br>
[[paper]](https://arxiv.org/abs/2204.04799)
[[code]](https://github.com/google-research/l2p)

**(*EMNLP2022_ATTEMPT*) ATTEMPT: Parameter-Efficient Multi-task Tuning via Attentional Mixtures of Soft Prompts.** <br>
*Akari Asai, Mohammadreza Salehi, Matthew E. Peters, Hannaneh Hajishirzi.*<br>
[[paper]](https://arxiv.org/abs/2205.11961)
[[code]](https://github.com/AkariAsai/ATTEMPT)

**(*NeurIPS2022_P2P*) P2P: Tuning Pre-trained Image Models for Point Cloud Analysis with Point-to-Pixel Prompting.** <br>
*Ziyi Wang, Xumin Yu, Yongming Rao, Jie Zhou, Jiwen Lu.*<br>
[[paper]](https://arxiv.org/abs/2208.02812)
[[code]](https://github.com/wangzy22/P2P)

**(*NeurIPS2022_PromptGen*) Generative Visual Prompt: Unifying Distributional Control of Pre-Trained Generative Models.** <br>
*Chen Henry Wu, Saman Motamed, Shaunak Srivastava, Fernando De la Torre.*<br>
[[paper]](https://arxiv.org/abs/2209.06970)
[[code]](https://github.com/chenwu98/generative-visual-prompt)

**(*NeurIPS2022_ScienceQA*) Learn to Explain: Multimodal Reasoning via Thought Chains for Science Question Answering.** <br>
*Pan Lu, Swaroop Mishra, Tony Xia, Liang Qiu, Kai-Wei Chang, Song-Chun Zhu, Oyvind Tafjord, Peter Clark, Ashwin Kalyan.*<br>
[[paper]](https://arxiv.org/abs/2209.09513)
[[code]](https://github.com/lupantech/ScienceQA)

**(*ICML2022_HyperPrompt*) HyperPrompt: Prompt-based Task-Conditioning of Transformers.** <br>
*Yun He, Steven Zheng, Yi Tay, Jai Gupta, Yu Du, Vamsi Aribandi, Zhe Zhao, Yaguang Li, Zhao Chen, Donald Metzler, Heng-Tze Cheng, Ed H. Chi.*<br>
[[paper]](https://proceedings.mlr.press/v162/he22f.html)

**(*ICLR2023_Promptagator*) Promptagator: Few-shot Dense Retrieval From 8 Examples.** <br>
*Zhuyun Dai, Vincent Y. Zhao, Ji Ma, Yi Luan, Jianmo Ni, Jing Lu, Anton Bakalov, Kelvin Guu, Keith B. Hall, Ming-Wei Chang.*<br>
[[paper]](https://arxiv.org/abs/2209.11755)

**(*ICLR2023_PromptPG*) Dynamic Prompt Learning via Policy Gradient for Semi-structured Mathematical Reasoning.** <br>
*Pan Lu, Liang Qiu, Kai-Wei Chang, Ying Nian Wu, Song-Chun Zhu, Tanmay Rajpurohit, Peter Clark, Ashwin Kalyan.*<br>
[[paper]](https://arxiv.org/abs/2209.14610)
[[code]](https://github.com/lupantech/PromptPG)

**(*CVPR2023_VPT-GTL*) Visual Prompt Tuning for Generative Transfer Learning.** <br>
*Kihyuk Sohn, Yuan Hao, José Lezama, Luisa Polania, Huiwen Chang, Han Zhang, Irfan Essa, Lu Jiang.*<br>
[[paper]](https://arxiv.org/abs/2210.00990)
[[code]](https://github.com/google-research/generative_transfer)

**(*ICLR2023_LPT*) LPT: Long-tailed Prompt Tuning for Image Classification.** <br>
*Bowen Dong, Pan Zhou, Shuicheng Yan, Wangmeng Zuo.*<br>
[[paper]](https://arxiv.org/abs/2210.01033)

**(*ICLR2023_PLOT*) PLOT: Prompt Learning with Optimal Transport for Vision-Language Models.** <br>
*Guangyi Chen, Weiran Yao, Xiangchen Song, Xinyue Li, Yongming Rao, Kun Zhang.*<br>
[[paper]](https://arxiv.org/abs/2210.01253)
[[code]](https://github.com/CHENGY12/PLOT)

**(*CVPR2023_MaPLe*) MaPLe: Multi-modal Prompt Learning.** <br>
*Muhammad Uzair Khattak, Hanoona Rasheed, Muhammad Maaz, Salman Khan, Fahad Shahbaz Khan.*<br>
[[paper]](https://arxiv.org/abs/2210.03117)
[[code]](https://github.com/muzairkhattak/multimodal-prompt-learning)

**(*arXiv2023_DePT*) Visual Prompt Tuning for Test-time Domain Adaptation.** <br>
*Yunhe Gao, Xingjian Shi, Yi Zhu, Hao Wang, Zhiqiang Tang, Xiong Zhou, Mu Li, Dimitris N. Metaxas.*<br>
[[paper]](https://arxiv.org/abs/2210.04831)

**(*ICLR2023_Description*) Visual Classification via Description from Large Language Models.** <br>
*Sachit Menon, Carl Vondrick.*<br>
[[paper]](https://arxiv.org/abs/2210.07183)

**(*ICLR2023_reliability*) Prompting GPT-3 To Be Reliable.** <br>
*Chenglei Si, Zhe Gan, Zhengyuan Yang, Shuohang Wang, Jianfeng Wang, Jordan Boyd-Graber, Lijuan Wang.*<br>
[[paper]](https://arxiv.org/abs/2210.09150)
[[code]](https://github.com/NoviScl/GPT3-Reliability)

**(*arXiv2022_ProSFDA*) ProSFDA: Prompt Learning based Source-free Domain Adaptation for Medical Image Segmentation.** <br>
*Shishuai Hu, Zehui Liao, Yong Xia.*<br>
[[paper]](https://arxiv.org/abs/2211.11514)
[[code]](https://github.com/ShishuaiHu/ProSFDA)

**(*CVPR2023_ILM-VP*) Understanding and Improving Visual Prompting: A Label-Mapping Perspective.** <br>
*Aochuan Chen, Yuguang Yao, Pin-Yu Chen, Yihua Zhang, Sijia Liu.*<br>
[[paper]](https://arxiv.org/abs/2211.11635)
[[code]](https://github.com/OPTML-Group/ILM-VP)

**(*CVPR2023_TaI-DPT*) Texts as Images in Prompt Tuning for Multi-Label Image Recognition.** <br>
*Zixian Guo, Bowen Dong, Zhilong Ji, Jinfeng Bai, Yiwen Guo, Wangmeng Zuo.*<br>
[[paper]](https://arxiv.org/abs/2211.12739)
[[code]](https://github.com/guozix/tai-dpt)

**(*CVPR2023_VoP*) VoP: Text-Video Co-operative Prompt Tuning for Cross-Modal Retrieval.** <br>
*Siteng Huang, Biao Gong, Yulin Pan, Jianwen Jiang, Yiliang Lv, Yuyuan Li, Donglin Wang.*<br>
[[paper]](https://arxiv.org/abs/2211.12764)
[[code]](https://github.com/bighuang624/VoP)

**(*AAAI2023_CLIP-ReID*) CLIP-ReID: Exploiting Vision-Language Model for Image Re-Identification without Concrete Text Labels.** <br>
*Siyuan Li, Li Sun, Qingli Li.*<br>
[[paper]](https://arxiv.org/abs/2211.13977)
[[code]](https://github.com/Syliz517/CLIP-ReID)

**(*CVPR2023_Painter*) Images Speak in Images: A Generalist Painter for In-Context Visual Learning.** <br>
*Xinlong Wang, Wen Wang, Yue Cao, Chunhua Shen, Tiejun Huang.*<br>
[[paper]](https://arxiv.org/abs/2212.02499)
[[code]](https://github.com/baaivision/Painter)

**(*AAAI2023_VDP*) Decorate the Newcomers: Visual Domain Prompt for Continual Test Time Adaptation.** <br>
*Yulu Gan, Yan Bai, Yihang Lou, Xianzheng Ma, Renrui Zhang, Nian Shi, Lin Luo.*<br>
[[paper]](https://arxiv.org/abs/2212.04145)

**(*CVPR2023_PIVOT*) PIVOT: Prompting for Video Continual Learning.** <br>
*Andrés Villa, Juan León Alcázar, Motasem Alfarra, Kumail Alhamoud, Julio Hurtado, Fabian Caba Heilbron, Alvaro Soto, Bernard Ghanem.*<br>
[[paper]](https://arxiv.org/abs/2212.04842)

**(*TMLR2024_EVP*) Unleashing the Power of Visual Prompting At the Pixel Level.** <br>
*Junyang Wu, Xianhang Li, Chen Wei, Huiyu Wang, Alan Yuille, Yuyin Zhou, Cihang Xie.*<br>
[[paper]](https://arxiv.org/abs/2212.10556)
[[code]](https://github.com/UCSC-VLAA/EVP)

**(*ACL2023_OFA-PT*) Prompt Tuning for Unified Multimodal Pretrained Models.** <br>
*Hao Yang, Junyang Lin, An Yang, Peng Wang, Chang Zhou.*<br>
[[paper]](https://aclanthology.org/2023.findings-acl.27.pdf)
[[code]](https://github.com/OFA-Sys/OFA)

**(*arXiv2023_MM-CoT*) Multimodal Chain-of-Thought Reasoning in Language Models.** <br>
*Zhuosheng Zhang, Aston Zhang, Mu Li, Hai Zhao, George Karypis, Alex Smola.*<br>
[[paper]](https://arxiv.org/abs/2302.00923)
[[code]](https://github.com/amazon-science/mm-cot)

**(*ICCV2023_PTUnifier*) Towards Unifying Medical Vision-and-Language Pre-training via Soft Prompts.** <br>
*Zhihong Chen, Shizhe Diao, Benyou Wang, Guanbin Li, Xiang Wan.*<br>
[[paper]](https://arxiv.org/abs/2302.08958)
[[code]](https://github.com/zhjohnchan/PTUnifier)

**(*CVPR2023_HiPro*) Hierarchical Prompt Learning for Multi-Task Learning.** <br>
*Yajing Liu, Yuning Lu, Hao Liu, Yaozu An, Zhuoran Xu, Zhuokun Yao, Baofeng Zhang, Zhiwei Xiong, Chenguang Gui.*<br>
[[paper]](https://openaccess.thecvf.com/content/CVPR2023/html/Liu_Hierarchical_Prompt_Learning_for_Multi-Task_Learning_CVPR_2023_paper.html)

**(*NeurIPS2023_CVP*) Convolutional Visual Prompt for Robust Visual Perception.** <br>
*Yun-Yun Tsai, Chengzhi Mao, Junfeng Yang.*<br>
[[paper]](https://arxiv.org/abs/2303.00198)

**(*CVPR2023_VE-Prompt*) Visual Exemplar Driven Task-Prompting for Unified Perception in Autonomous Driving.** <br>
*Xiwen Liang, Minzhe Niu, Jianhua Han, Hang Xu, Chunjing Xu, Xiaodan Liang.*<br>
[[paper]](https://arxiv.org/abs/2303.01788)

**(*CVPR2023_CaFo*) Prompt, Generate, then Cache: Cascade of Foundation Models makes Strong Few-shot Learners.** <br>
*Renrui Zhang, Xiangfei Hu, Bohao Li, Siyuan Huang, Hanqiu Deng, Hongsheng Li, Yu Qiao, Peng Gao.*<br>
[[paper]](https://arxiv.org/abs/2303.02151)
[[code]](https://github.com/ZrrSkywalker/CaFo)

**(*arXiv2023_ComPro*) Learning Combinatorial Prompts for Universal Controllable Image Captioning.** <br>
*Zhen Wang, Jun Xiao, Yueting Zhuang, Fei Gao, Jian Shao, Long Chen.*<br>
[[paper]](https://arxiv.org/abs/2303.06338)

**(*CVPR2023_DAM-VP*) Diversity-Aware Meta Visual Prompting.** <br>
*Qidong Huang, Xiaoyi Dong, Dongdong Chen, Weiming Zhang, Feifei Wang, Gang Hua, Nenghai Yu.*<br>
[[paper]](https://arxiv.org/abs/2303.08138)
[[code]](https://github.com/shikiw/DAM-VP)

**(*AAAI2024_LION*) LION: Implicit Vision Prompt Tuning.** <br>
*Haixin Wang, Jianlong Chang, Xiao Luo, Jinan Sun, Zhouchen Lin, Qi Tian.*<br>
[[paper]](https://arxiv.org/abs/2303.09992)

**(*CVPR2023_ViPT*) Visual Prompt Multi-Modal Tracking.** <br>
*Jiawen Zhu, Simiao Lai, Xin Chen, Dong Wang, Huchuan Lu.*<br>
[[paper]](https://arxiv.org/abs/2303.10826)
[[code]](https://github.com/jiawen-zhu/vipt)

**(*arXiv2023_DPL*) Decomposed Prototype Learning for Few-Shot Scene Graph Generation.** <br>
*Xingchen Li, Long Chen, Guikun Chen, Yinfu Feng, Yi Yang, Jun Xiao.*<br>
[[paper]](https://arxiv.org/abs/2303.10863)

**(*CVPR2023_EVP*) Explicit Visual Prompting for Low-Level Structure Segmentations.** <br>
*Weihuang Liu, Xi Shen, Chi-Man Pun, Xiaodong Cun.*<br>
[[paper]](https://arxiv.org/abs/2303.10883)
[[code]](https://github.com/NiFangBaAGe/Explicit-Visual-Prompt)

**(*CVPR2023_SP*) Semantic Prompt for Few-Shot Image Recognition.** <br>
*Wentao Chen, Chenyang Si, Zhang Zhang, Liang Wang, Zilei Wang, Tieniu Tan.*<br>
[[paper]](https://arxiv.org/abs/2303.14123)

**(*ICCV2023_SegGPT*) SegGPT: Segmenting Everything In Context.** <br>
*Xinlong Wang, Xiaosong Zhang, Yue Cao, Wen Wang, Chunhua Shen, Tiejun Huang.*<br>
[[paper]](https://arxiv.org/abs/2304.03284)
[[code]](https://github.com/baaivision/Painter)

**(*CVPR2023_Vita-CLIP*) Vita-CLIP: Video and text adaptive CLIP via Multimodal Prompting.** <br>
*Syed Talal Wasim, Muzammal Naseer, Salman Khan, Fahad Shahbaz Khan, Mubarak Shah.*<br>
[[paper]](https://arxiv.org/abs/2304.03307)
[[code]](https://github.com/TalalWasim/Vita-CLIP)

**(*ICCV2023_IDPT*) Instance-aware Dynamic Prompt Tuning for Pre-trained Point Cloud Models.** <br>
*Yaohua Zha, Jinpeng Wang, Tao Dai, Bin Chen, Zhi Wang, Shu-Tao Xia.*<br>
[[paper]](https://arxiv.org/abs/2304.07221)
[[code]](https://github.com/zyh16143998882/ICCV23-IDPT)

**(*NeurIPS2023_VPGTrans*) VPGTrans: Transfer Visual Prompt Generator across LLMs.** <br>
*Ao Zhang, Hao Fei, Yuan Yao, Wei Ji, Li Li, Zhiyuan Liu, Tat-Seng Chua.*<br>
[[paper]](https://arxiv.org/abs/2305.01278)
[[code]](https://github.com/vpgtrans/vpgtrans)

**(*arXiv2023_TreePrompt*) TreePrompt: Learning to Compose Tree Prompts for Explainable Visual Grounding.** <br>
*Chenchi Zhang, Jun Xiao, Lei Chen, Jian Shao, Long Chen.*<br>
[[paper]](https://arxiv.org/abs/2305.11497)

**(*ACL2023_APT*) Towards Adaptive Prefix Tuning for Parameter-Efficient Language Model Fine-tuning.** <br>
*Zhen-Ru Zhang, Chuanqi Tan, Haiyang Xu, Chengyu Wang, Jun Huang, Songfang Huang.*<br>
[[paper]](https://arxiv.org/abs/2305.15212)

**(*arXiv2023_APT*) Approximated Prompt Tuning for Vision-Language Pre-trained Models.** <br>
*Qiong Wu, Shubin Huang, Yiyi Zhou, Pingyang Dai, Annan Shu, Guannan Jiang, Rongrong Ji.*<br>
[[paper]](https://arxiv.org/abs/2306.15706)

**(*ICLR2024_LRR*) Look, Remember and Reason: Visual Reasoning with Grounded Rationales.** <br>
*Apratim Bhattacharyya, Sunny Panchal, Mingu Lee, Reza Pourreza, Pulkit Madan, Roland Memisevic.*<br>
[[paper]](https://arxiv.org/abs/2306.17778)

**(*ACMMM2023_Self-PT*) Self-PT: Adaptive Self-Prompt Tuning for Low-Resource Visual Question Answering.** <br>
*Bowen Yuan, Sisi You, Bing-Kun Bao.*<br>
[[paper]](https://dl.acm.org/doi/abs/10.1145/3581783.3612222)
[[code]](https://github.com/NJUPT-MCC/Self-PT)

**(*ICCV2023_E2VPT*) E^2VPT: An Effective and Efficient Approach for Visual Prompt Tuning.** <br>
*Cheng Han, Qifan Wang, Yiming Cui, Zhiwen Cao, Wenguan Wang, Siyuan Qi, Dongfang Liu.*<br>
[[paper]](https://arxiv.org/abs/2307.13770)
[[code]](https://github.com/ChengHan111/E2VPT)

**(*ICCV2023_PromptSwitch*) Prompt Switch: Efficient CLIP Adaptation for Text-Video Retrieval.** <br>
*Chaorui Deng, Qi Chen, Pengda Qin, Da Chen, Qi Wu.*<br>
[[paper]](https://arxiv.org/abs/2308.07648v1)
[[code]](https://github.com/bladewaltz1/PromptSwitch)

**(*arXiv2023_DePT*) DePT: Decomposed Prompt Tuning for Parameter-Efficient Fine-tuning.** <br>
*Zhengxiang Shi, Aldo Lipani.*<br>
[[paper]](https://arxiv.org/abs/2309.05173)
[[code]](https://github.com/ZhengxiangShi/DePT)

**(*arXiv2023_Black-Box*) Language Models as Black-Box Optimizers for Vision-Language Models.** <br>
*Shihong Liu, Zhiqiu Lin, Samuel Yu, Ryan Lee, Tiffany Ling, Deepak Pathak, Deva Ramanan.*<br>
[[paper]](https://arxiv.org/abs/2309.05950)
[[code]](https://github.com/shihongl1998/llm-as-a-blackbox-optimizer)

**(*arXiv2023_DePT*) DePT: Decoupled Prompt Tuning.** <br>
*Ji Zhang, Shihan Wu, Lianli Gao, Hengtao Shen, Jingkuan Song.*<br>
[[paper]](https://arxiv.org/abs/2309.07439)
[[code]](https://github.com/Koorye/DePT)

**(*arXiv2023_Point-PEFT*) Point-PEFT: Parameter-Efficient Fine-Tuning for 3D Pre-trained Models.** <br>
*Yiwen Tang, Ray Zhang, Zoey Guo, Dong Wang, Zhigang Wang, Bin Zhao, Xuelong Li.*<br>
[[paper]](https://arxiv.org/abs/2310.03059)
[[code]](https://github.com/Ivan-Tang-3D/Point-PEFT)

**(*NeurIPS2023_DG-SCT*) Cross-modal Prompts: Adapting Large Pre-trained Models for Audio-Visual Downstream Tasks.** <br>
*Haoyi Duan, Yan Xia, Mingze Zhou, Li Tang, Jieming Zhu, Zhou Zhao.*<br>
[[paper]](https://arxiv.org/abs/2311.05152)
[[code]](https://github.com/haoyi-duan/DG-SCT)

**(*AAAI2024_MmAP*) MmAP: Multi-modal Alignment Prompt for Cross-domain Multi-task Learning.** <br>
*Yi Xin, Junlong Du, Qiang Wang, Ke Yan, Shouhong Ding.*<br>
[[paper]](https://arxiv.org/abs/2312.08636)
[[code]](https://github.com/synbol/Parameter-Efficient-Transfer-Learning-Benchmark)


### ``*Adapter Tuning*``

**(*ICML2019_Adapter-BERT*) Parameter-Efficient Transfer Learning for NLP.** <br>
*Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin de Laroussilhe, Andrea Gesmundo, Mona Attariyan, Sylvain Gelly.*<br>
[[paper]](https://arxiv.org/abs/1902.00751)
[[code]](https://github.com/google-research/adapter-bert)

**(*EMNLP2019_Adapter-NMT*) Simple, Scalable Adaptation for Neural Machine Translation.** <br>
*Ankur Bapna, Naveen Arivazhagan, Orhan Firat.*<br>
[[paper]](https://arxiv.org/abs/1909.08478)

**(*NeurIPS2020_TinyTL*) TinyTL: Reduce Memory, Not Parameters for Efficient On-Device Learning.** <br>
*Han Cai, Chuang Gan, Ligeng Zhu, Song Han.*<br>
[[paper]](https://proceedings.neurips.cc/paper/2020/file/81f7acabd411274fcf65ce2070ed568a-Paper.pdf)

**(*EMNLP2020_MAD-X*) MAD-X: An Adapter-Based Framework for Multi-Task Cross-Lingual Transfer.** <br>
*Jonas Pfeiffer, Ivan Vulić, Iryna Gurevych, Sebastian Ruder.*<br>
[[paper]](https://arxiv.org/abs/2005.00052)
[[code]](https://github.com/adapter-hub/adapters)

**(*EACL2021_AdapterFusion*) AdapterFusion: Non-Destructive Task Composition for Transfer Learning.** <br>
*Jonas Pfeiffer, Aishwarya Kamath, Andreas Rücklé, Kyunghyun Cho, Iryna Gurevych.*<br>
[[paper]](https://arxiv.org/abs/2005.00247)
[[code]](https://github.com/adapter-hub/adapters)

**(*EMNLP2021_AdapterDrop*) AdapterDrop: On the Efficiency of Adapters in Transformers.** <br>
*Andreas Rücklé, Gregor Geigle, Max Glockner, Tilman Beck, Jonas Pfeiffer, Nils Reimers, Iryna Gurevych.*<br>
[[paper]](https://arxiv.org/abs/2010.11918)
[[code]](https://github.com/adapter-hub/adapters)

**(*ACL2021_Hyperformer*) Parameter-efficient Multi-task Fine-tuning for Transformers via Shared Hypernetworks.** <br>
*Rabeeh Karimi Mahabadi, Sebastian Ruder, Mostafa Dehghani, James Henderson.*<br>
[[paper]](https://arxiv.org/abs/2106.04489)
[[code]](https://github.com/rabeehk/hyperformer)

**(*NeurIPS2021_Compacter*) Compacter: Efficient Low-Rank Hypercomplex Adapter Layers.** <br>
*Rabeeh Karimi Mahabadi, James Henderson, Sebastian Ruder.*<br>
[[paper]](https://arxiv.org/abs/2106.04647)
[[code]](https://github.com/rabeehk/compacter)

**(*ICLR2022_LoRA*) LoRA: Low-Rank Adaptation of Large Language Models.** <br>
*Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen.*<br>
[[paper]](https://arxiv.org/abs/2106.09685)
[[code]](https://github.com/microsoft/LoRA)

**(*ECCV2022_Tip-Adapter*) Tip-Adapter: Training-free CLIP-Adapter for Better Vision-Language Modeling.** <br>
*Renrui Zhang, Rongyao Fang, Wei Zhang, Peng Gao, Kunchang Li, Jifeng Dai, Yu Qiao, Hongsheng Li.*<br>
[[paper]](https://arxiv.org/abs/2111.03930)
[[code]](https://github.com/gaopengcuhk/tip-adapter)

**(*CVPR2022_VL-Adapter*) VL-Adapter: Parameter-Efficient Transfer Learning for Vision-and-Language Tasks.** <br>
*Yi-Lin Sung, Jaemin Cho, Mohit Bansal.*<br>
[[paper]](https://arxiv.org/abs/2112.06825)
[[code]](https://github.com/ylsung/VL_adapter)

**(*ICASSP2023_I-Tuning*) I-Tuning: Tuning Frozen Language Models with Image for Lightweight Image Captioning.** <br>
*Ziyang Luo, Zhipeng Hu, Yadong Xi, Rongsheng Zhang, Jing Ma.*<br>
[[paper]](https://arxiv.org/abs/2202.06574)

**(*AAAI2023_KAdaptation*) Parameter-efficient Model Adaptation for Vision Transformers.** <br>
*Xuehai He, Chunyuan Li, Pengchuan Zhang, Jianwei Yang, Xin Eric Wang.*<br>
[[paper]](https://arxiv.org/abs/2203.16329)
[[code]](https://github.com/eric-ai-lab/pevit)

**(*NeurIPS2022_IA3*) Few-Shot Parameter-Efficient Fine-Tuning is Better and Cheaper than In-Context Learning.** <br>
*Haokun Liu, Derek Tam, Mohammed Muqeeth, Jay Mohta, Tenghao Huang, Mohit Bansal, Colin Raffel.*<br>
[[paper]](https://arxiv.org/abs/2205.05638)
[[code]](https://github.com/r-three/t-few)

**(*ICLR2023_ViT-Adapter*) Vision Transformer Adapter for Dense Predictions.** <br>
*Zhe Chen, Yuchen Duan, Wenhai Wang, Junjun He, Tong Lu, Jifeng Dai, Yu Qiao.*<br>
[[paper]](https://arxiv.org/abs/2205.08534)
[[code]](https://github.com/czczup/vit-adapter)

**(*EMNLP2022_AdaMix*) AdaMix: Mixture-of-Adaptations for Parameter-efficient Model Tuning.** <br>
*Yaqing Wang, Sahaj Agarwal, Subhabrata Mukherjee, Xiaodong Liu, Jing Gao, Ahmed Hassan Awadallah, Jianfeng Gao.*<br>
[[paper]](https://arxiv.org/abs/2205.12410)
[[code]](https://github.com/microsoft/AdaMix)

**(*NeurIPS2022_AdaptFormer*) AdaptFormer: Adapting Vision Transformers for Scalable Visual Recognition.** <br>
*Shoufa Chen, Chongjian Ge, Zhan Tong, Jiangliu Wang, Yibing Song, Jue Wang, Ping Luo.*<br>
[[paper]](https://arxiv.org/abs/2205.13535)
[[code]](https://github.com/ShoufaChen/AdaptFormer)

**(*NeurIPS2022_ST-Adapter*) ST-Adapter: Parameter-Efficient Image-to-Video Transfer Learning.** <br>
*Junting Pan, Ziyi Lin, Xiatian Zhu, Jing Shao, Hongsheng Li.*<br>
[[paper]](https://arxiv.org/abs/2206.13559)
[[code]](https://github.com/linziyi96/st-adapter)

**(*arXiv2022_Convpass*) Convolutional Bypasses Are Better Vision Transformer Adapters.** <br>
*Shibo Jie, Zhi-Hong Deng.*<br>
[[paper]](https://arxiv.org/abs/2207.07039)
[[code]](https://github.com/JieShibo/PETL-ViT)

**(*NeurIPS2022_Polyhistor*) Polyhistor: Parameter-Efficient Multi-Task Adaptation for Dense Vision Tasks.** <br>
*Yen-Cheng Liu, Chih-Yao Ma, Junjiao Tian, Zijian He, Zsolt Kira.*<br>
[[paper]](https://arxiv.org/abs/2210.03265)
[[code]](https://ycliu93.github.io/projects/polyhistor.html)

**(*NeurIPS2022_SSF*) Scaling & Shifting Your Features: A New Baseline for Efficient Model Tuning.** <br>
*Dongze Lian, Daquan Zhou, Jiashi Feng, Xinchao Wang.*<br>
[[paper]](https://arxiv.org/abs/2210.08823)
[[code]](https://github.com/dongzelian/ssf)

**(*AAAI2023_FacT*) FacT: Factor-Tuning for Lightweight Adaptation on Vision Transformer.** <br>
*Shibo Jie, Zhi-Hong Deng.*<br>
[[paper]](https://arxiv.org/abs/2212.03145)
[[code]](https://github.com/JieShibo/PETL-ViT)

**(*AAAI2023_Mix*) Token Mixing: Parameter-Efficient Transfer Learning from Image-Language to Video-Language.** <br>
*Yuqi Liu, Luhui Xu, Pengfei Xiong, Qin Jin.*<br>
[[paper]](https://ojs.aaai.org/index.php/AAAI/article/view/25267)
[[code]](https://github.com/yuqi657/video_language_model)

**(*arXiv2022_LAVISH*) Vision Transformers are Parameter-Efficient Audio-Visual Learners.** <br>
*Yan-Bo Lin, Yi-Lin Sung, Jie Lei, Mohit Bansal, Gedas Bertasius.*<br>
[[paper]](https://arxiv.org/abs/2212.07983)
[[code]](https://genjib.github.io/project_page/LAVISH/)

**(*arXiv2023_KronA*) KronA: Parameter Efficient Tuning with Kronecker Adapter.** <br>
*Ali Edalati, Marzieh Tahaei, Ivan Kobyzev, Vahid Partovi Nia, James J. Clark, Mehdi Rezagholizadeh.*<br>
[[paper]](https://arxiv.org/abs/2212.10650)

**(*CVPR2024_MV-Adapter*) MV-Adapter: Exploring Parameter Efficient Learning for Video Text Retrieval.** <br>
*Bowen Zhang, Xiaojie Jin, Weibo Gong, Kai Xu, Xueqing Deng, Peng Wang, Zhao Zhang, Xiaohui Shen, Jiashi Feng.*<br>
[[paper]](https://arxiv.org/abs/2301.07868)

**(*ICLR2023_AIM*) AIM: Adapting Image Models for Efficient Video Action Recognition.** <br>
*Taojiannan Yang, Yi Zhu, Yusheng Xie, Aston Zhang, Chen Chen, Mu Li.*<br>
[[paper]](https://arxiv.org/abs/2302.03024)
[[code]](https://adapt-image-models.github.io/)

**(*arXiv2023_OT*) Offsite-Tuning: Transfer Learning without Full Model.** <br>
*Guangxuan Xiao, Ji Lin, Song Han.*<br>
[[paper]](https://arxiv.org/abs/2302.04870)
[[code]](https://github.com/mit-han-lab/offsite-tuning)

**(*arXiv2023_UniAdapter*) UniAdapter: Unified Parameter-Efficient Transfer Learning for Cross-modal Modeling.** <br>
*Haoyu Lu, Mingyu Ding, Yuqi Huo, Guoxing Yang, Zhiwu Lu, Masayoshi Tomizuka, Wei Zhan.*<br>
[[paper]](https://arxiv.org/abs/2302.06605)
[[code]](https://github.com/RERV/UniAdapter)

**(*arXiv2023_RepAdapter*) Towards Efficient Visual Adaption via Structural Re-parameterization.** <br>
*Gen Luo, Minglang Huang, Yiyi Zhou, Xiaoshuai Sun, Guannan Jiang, Zhiyu Wang, Rongrong Ji.*<br>
[[paper]](https://arxiv.org/abs/2302.08106)
[[code]](https://github.com/luogen1996/repadapter)

**(*arXiv2023_T2I-Adapter*) T2I-Adapter: Learning Adapters to Dig out More Controllable Ability for Text-to-Image Diffusion Models.** <br>
*Chong Mou, Xintao Wang, Liangbin Xie, Yanze Wu, Jian Zhang, Zhongang Qi, Ying Shan, Xiaohu Qie.*<br>
[[paper]](https://arxiv.org/abs/2302.08453)
[[code]](https://github.com/TencentARC/T2I-Adapter)

**(*CVPR2023_MixPHM*) MixPHM: Redundancy-Aware Parameter-Efficient Tuning for Low-Resource Visual Question Answering.** <br>
*Jingjing Jiang, Nanning Zheng.*<br>
[[paper]](https://arxiv.org/abs/2303.01239)
[[code]](https://github.com/jingjing12110/mixphm)

**(*arXiv2023_TTC-Tuning*) Revisit Parameter-Efficient Transfer Learning: A Two-Stage Paradigm.** <br>
*Hengyuan Zhao, Hao Luo, Yuyang Zhao, Pichao Wang, Fan Wang, Mike Zheng Shou.*<br>
[[paper]](https://arxiv.org/abs/2303.07910)

**(*ICCV2023_LAE*) A Unified Continual Learning Framework with General Parameter-Efficient Tuning.** <br>
*Qiankun Gao, Chen Zhao, Yifan Sun, Teng Xi, Gang Zhang, Bernard Ghanem, Jian Zhang.*<br>
[[paper]](https://arxiv.org/abs/2303.10070)
[[code]](https://github.com/gqk/LAE)

**(*ICLR2023_AdaLoRA*) AdaLoRA: Adaptive Budget Allocation for Parameter-Efficient Fine-Tuning.** <br>
*Qingru Zhang, Minshuo Chen, Alexander Bukharin, Nikos Karampatziakis, Pengcheng He, Yu Cheng, Weizhu Chen, Tuo Zhao.*<br>
[[paper]](https://arxiv.org/abs/2303.10512)
[[code]](https://github.com/QingruZhang/AdaLoRA)

**(*ECIR2023_Adapter-SPLADE*) Parameter-Efficient Sparse Retrievers and Rerankers using Adapters.** <br>
*Vaishali Pal, Carlos Lassance, Hervé Déjean, Stéphane Clinchant.*<br>
[[paper]](https://arxiv.org/abs/2303.13220)
[[code]](https://github.com/naver/splade/tree/adapter-splade)

**(*arXiv2023_Unet-Finetune*) A Closer Look at Parameter-Efficient Tuning in Diffusion Models.** <br>
*Chendong Xiang, Fan Bao, Chongxuan Li, Hang Su, Jun Zhu.*<br>
[[paper]](https://arxiv.org/abs/2303.18181)
[[code]](https://github.com/Xiang-cd/unet-finetune)

**(*EMNLP2023_LLM-Adapters*) LLM-Adapters: An Adapter Family for Parameter-Efficient Fine-Tuning of Large Language Models.** <br>
*Zhiqiang Hu, Lei Wang, Yihuai Lan, Wanyu Xu, Ee-Peng Lim, Lidong Bing, Xing Xu, Soujanya Poria, Roy Ka-Wei Lee.*<br>
[[paper]](https://arxiv.org/abs/2304.01933)
[[code]](https://github.com/AGI-Edgerunners/LLM-Adapters)

**(*NeurIPS2023_CoDA*) Conditional Adapters: Parameter-efficient Transfer Learning with Fast Inference.** <br>
*Tao Lei, Junwen Bai, Siddhartha Brahma, Joshua Ainslie, Kenton Lee, Yanqi Zhou, Nan Du, Vincent Y. Zhao, Yuexin Wu, Bo Li, Yu Zhang, Ming-Wei Chang.*<br>
[[paper]](https://arxiv.org/abs/2304.04947)

**(*ICLR2023_Robo-Adapter*) Lossless Adaptation of Pretrained Vision Models For Robotic Manipulation.** <br>
*Mohit Sharma, Claudio Fantacci, Yuxiang Zhou, Skanda Koppula, Nicolas Heess, Jon Scholz, Yusuf Aytar.*<br>
[[paper]](https://arxiv.org/abs/2304.06600)
[[code]](https://sites.google.com/view/robo-adapters/)

**(*arXiv2023_PVP*) PVP: Pre-trained Visual Parameter-Efficient Tuning.** <br>
*Zhao Song, Ke Yang, Naiyang Guan, Junjie Zhu, Peng Qiao, Qingyong Hu.*<br>
[[paper]](https://arxiv.org/abs/2304.13639)

**(*NeurIPS2023_Aurora*) Parameter-efficient Tuning of Large-scale Multimodal Foundation Model.** <br>
*Haixin Wang, Xinlong Yang, Jianlong Chang, Dian Jin, Jinan Sun, Shikun Zhang, Xiao Luo, Qi Tian.*<br>
[[paper]](https://arxiv.org/abs/2305.08381)
[[code]](https://github.com/WillDreamer/Aurora)

**(*NeurIPS2023_QLoRA*) QLoRA: Efficient Finetuning of Quantized LLMs.** <br>
*Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, Luke Zettlemoyer.*<br>
[[paper]](https://arxiv.org/abs/2305.14314)
[[code]](https://github.com/artidoro/qlora)

**(*CVPR2023_LoRand*) 1% VS 100%: Parameter-Efficient Low Rank Adapter for Dense Predictions.** <br>
*Dongshuo Yin, Yiran Yang, Zhechao Wang, Hongfeng Yu, Kaiwen Wei, Xian Sun.*<br>
[[paper]](https://openaccess.thecvf.com/content/CVPR2023/html/Yin_1_VS_100_Parameter-Efficient_Low_Rank_Adapter_for_Dense_Predictions_CVPR_2023_paper.html)

**(*arXiv2023_LoRAPrune*) LoRAPrune: Pruning Meets Low-Rank Parameter-Efficient Fine-Tuning.** <br>
*Mingyang Zhang, Hao Chen, Chunhua Shen, Zhen Yang, Linlin Ou, Xinyi Yu, Bohan Zhuang.*<br>
[[paper]](https://arxiv.org/abs/2305.18403)

**(*arXiv2023_OFT*) Controlling Text-to-Image Diffusion by Orthogonal Finetuning.** <br>
*Zeju Qiu, Weiyang Liu, Haiwen Feng, Yuxuan Xue, Yao Feng, Zhen Liu, Dan Zhang, Adrian Weller, Bernhard Schölkopf.*<br>
[[paper]](https://arxiv.org/abs/2306.07280)
[[code]](https://github.com/Zeju1997/oft)

**(*arXiv2023_GLoRA*) One-for-All: Generalized LoRA for Parameter-Efficient Fine-tuning.** <br>
*Arnav Chavan, Zhuang Liu, Deepak Gupta, Eric Xing, Zhiqiang Shen.*<br>
[[paper]](https://arxiv.org/abs/2306.07967)
[[code]](https://github.com/Arnav0400/ViT-Slim/tree/master/GLoRA)

**(*ACMMM2023_VioLET*) VioLET: Vision-Language Efficient Tuning with Collaborative Multi-modal Gradients.** <br>
*Yaoming Wang, Yuchen Liu, Xiaopeng Zhang, Jin Li, Bowen Shi, Chenglin Li, Wenrui Dai, Hongkai Xiong, Qi Tian.*<br>
[[paper]](https://dl.acm.org/doi/abs/10.1145/3581783.3611706)
[[code]](https://github.com/Wang-Yaoming/VioLET)

**(*arXiv2023_ReLoRA*) ReLoRA: High-Rank Training Through Low-Rank Updates.** <br>
*Vladislav Lialin, Namrata Shivagunde, Sherin Muckatira, Anna Rumshisky.*<br>
[[paper]](https://arxiv.org/abs/2307.05695)
[[code]](https://github.com/guitaricet/relora)

**(*ICCV2023_ETRIS*) Bridging Vision and Language Encoders: Parameter-Efficient Tuning for Referring Image Segmentation.** <br>
*Zunnan Xu, Zhihong Chen, Yong Zhang, Yibing Song, Xiang Wan, Guanbin Li.*<br>
[[paper]](https://arxiv.org/abs/2307.11545)
[[code]](https://github.com/kkakkkka/ETRIS)

**(*ICCV2023_BI-LoRA*) Revisiting the Parameter Efficiency of Adapters from the Perspective of Precision Redundancy.** <br>
*Shibo Jie, Haoqing Wang, Zhi-Hong Deng.*<br>
[[paper]](https://arxiv.org/abs/2307.16867)
[[code]](https://github.com/jieshibo/petl-vit)

**(*arXiv2023_LoRA-FA*) LoRA-FA: Memory-efficient Low-rank Adaptation for Large Language Models Fine-tuning.** <br>
*Longteng Zhang, Lin Zhang, Shaohuai Shi, Xiaowen Chu, Bo Li.*<br>
[[paper]](https://arxiv.org/abs/2308.03303)

**(*arXiv2023_SLoRA*) SLoRA: Federated Parameter Efficient Fine-Tuning of Language Models.** <br>
*Sara Babakniya, Ahmed Roushdy Elkordy, Yahya H. Ezzeldin, Qingfeng Liu, Kee-Bong Song, Mostafa El-Khamy, Salman Avestimehr.*<br>
[[paper]](https://arxiv.org/abs/2308.06522)

**(*ICCV2023_Tem-adapter*) Tem-adapter: Adapting Image-Text Pretraining for Video Question Answer.** <br>
*Guangyi Chen, Xiao Liu, Guangrun Wang, Kun Zhang, Philip H.S.Torr, Xiao-Ping Zhang, Yansong Tang.*<br>
[[paper]](https://arxiv.org/abs/2308.08414)
[[code]](https://github.com/xliu443/tem-adapter)

**(*ICCV2023_VL-PET*) VL-PET: Vision-and-Language Parameter-Efficient Tuning via Granularity Control.** <br>
*Zi-Yuan Hu, Yanyang Li, Michael R. Lyu, Liwei Wang.*<br>
[[paper]](https://arxiv.org/abs/2308.09804)
[[code]](https://github.com/HenryHZY/VL-PET)

**(*ICCV2023_VLN-PETL*) VLN-PETL: Parameter-Efficient Transfer Learning for Vision-and-Language Navigation.** <br>
*Yanyuan Qiao, Zheng Yu, Qi Wu.*<br>
[[paper]](https://arxiv.org/abs/2308.10172)
[[code]](https://github.com/yanyuanqiao/vln-petl)

**(*arXiv2023_PE-RSITR*) Parameter-Efficient Transfer Learning for Remote Sensing Image-Text Retrieval.** <br>
*Yuan Yuan, Yang Zhan, Zhitong Xiong.*<br>
[[paper]](https://arxiv.org/abs/2308.12509)
[[code]](https://github.com/ZhanYang-nwpu/PE-RSITR)

**(*AAAI2024_SAM-PARSER*) SAM-PARSER: Fine-tuning SAM Efficiently by Parameter Space Reconstruction.** <br>
*Zelin Peng, Zhengqin Xu, Zhilin Zeng, Xiaokang Yang, Wei Shen.*<br>
[[paper]](https://arxiv.org/abs/2308.14604)

**(*NeurIPS2023_DAS*) Parameter and Computation Efficient Transfer Learning for Vision-Language Pre-trained Models.** <br>
*Qiong Wu, Wei Yu, Yiyi Zhou, Shubin Huang, Xiaoshuai Sun, Rongrong Ji.*<br>
[[paper]](https://arxiv.org/abs/2309.01479)
[[code]](https://github.com/DoubtedSteam/DAS)

**(*arXiv2023_Hydra*) Hydra: Multi-head Low-rank Adaptation for Parameter Efficient Fine-tuning.** <br>
*Sanghyeon Kim, Hyunmo Yang, Younghyun Kim, Youngjoon Hong, Eunbyung Park.*<br>
[[paper]](https://arxiv.org/abs/2309.06922)
[[code]](https://github.com/extremebird/Hydra)

**(*IJCV2023_SCT*) SCT: A Simple Baseline for Parameter-Efficient Fine-Tuning via Salient Channels.** <br>
*Henry Hengyuan Zhao, Pichao Wang, Yuyang Zhao, Hao Luo, Fan Wang, Mike Zheng Shou.*<br>
[[paper]](https://arxiv.org/abs/2309.08513)
[[code]](https://github.com/showlab/SCT)

**(*ICLR2024_LongLoRA*) LongLoRA: Efficient Fine-tuning of Long-Context Large Language Models.** <br>
*Yukang Chen, Shengju Qian, Haotian Tang, Xin Lai, Zhijian Liu, Song Han, Jiaya Jia.*<br>
[[paper]](https://arxiv.org/abs/2309.12307)
[[code]](https://github.com/dvlab-research/LongLoRA)

**(*arXiv2023_QA-LoRA*) QA-LoRA: Quantization-Aware Low-Rank Adaptation of Large Language Models.** <br>
*Yuhui Xu, Lingxi Xie, Xiaotao Gu, Xin Chen, Heng Chang, Hengheng Zhang, Zhengsu Chen, Xiaopeng Zhang, Qi Tian.*<br>
[[paper]](https://arxiv.org/abs/2309.14717)
[[code]](https://github.com/yuhuixu1993/qa-lora)

**(*ICLR2024_LyCORIS*) Navigating Text-To-Image Customization: From LyCORIS Fine-Tuning to Model Evaluation.** <br>
*Shih-Ying Yeh, Yu-Guan Hsieh, Zhidong Gao, Bernard B W Yang, Giyeong Oh, Yanmin Gong.*<br>
[[paper]](https://arxiv.org/abs/2309.14859)
[[code]](https://github.com/kohakublueleaf/lycoris)

**(*ICLR2024_NOLA*) NOLA: Networks as Linear Combination of Low Rank Random Basis.** <br>
*Soroush Abbasi Koohpayegani, KL Navaneet, Parsa Nooralinejad, Soheil Kolouri, Hamed Pirsiavash.*<br>
[[paper]](https://arxiv.org/abs/2310.02556)
[[code]](https://github.com/UCDvision/NOLA)

**(*ICLR2024_VeRA*) VeRA: Vector-based Random Matrix Adaptation.** <br>
*Dawid J. Kopiczko, Tijmen Blankevoort, Yuki M. Asano.*<br>
[[paper]](https://arxiv.org/abs/2310.11454)
[[code]](https://dkopi.github.io/vera/)

**(*ICLR2024_LoRA-rank*) The Expressive Power of Low-Rank Adaptation.** <br>
*Yuchen Zeng, Kangwook Lee.*<br>
[[paper]](https://arxiv.org/abs/2310.17513)

**(*EMNLP2023_Adapters*) Adapters: A Unified Library for Parameter-Efficient and Modular Transfer Learning.** <br>
*Clifton Poth, Hannah Sterz, Indraneil Paul, Sukannya Purkayastha, Leon Engländer, Timo Imhof, Ivan Vulić, Sebastian Ruder, Iryna Gurevych, Jonas Pfeiffer.*<br>
[[paper]](https://arxiv.org/abs/2311.11077)
[[code]](https://github.com/adapter-hub/adapters)

**(*arXiv2023_MultiLoRA*) MultiLoRA: Democratizing LoRA for Better Multi-Task Learning.** <br>
*Yiming Wang, Yu Lin, Xiaodong Zeng, Guannan Zhang.*<br>
[[paper]](https://arxiv.org/abs/2311.11501)

**(*EMNLP2023_SoRA*) Sparse Low-rank Adaptation of Pre-trained Language Models.** <br>
*Ning Ding, Xingtai Lv, Qiaosen Wang, Yulin Chen, Bowen Zhou, Zhiyuan Liu, Maosong Sun.*<br>
[[paper]](https://arxiv.org/abs/2311.11696)
[[code]](https://github.com/tsinghuac3i/sora)

**(*CVPR2024_SAM-COBOT*) Parameter Efficient Fine-tuning via Cross Block Orchestration for Segment Anything Model.** <br>
*Zelin Peng, Zhengqin Xu, Zhilin Zeng, Lingxi Xie, Qi Tian, Wei Shen.*<br>
[[paper]](https://arxiv.org/abs/2311.17112)

**(*NeurIPS2023_CAST*) CAST: Cross-Attention in Space and Time for Video Action Recognition.** <br>
*Dongho Lee, Jongseo Lee, Jinwoo Choi.*<br>
[[paper]](https://arxiv.org/abs/2311.18825)
[[code]](https://github.com/KHU-VLL/CAST)

**(*AAAI2024_VMT-Adapter*) VMT-Adapter: Parameter-Efficient Transfer Learning for Multi-Task Dense Scene Understanding.** <br>
*Yi Xin, Junlong Du, Qiang Wang, Zhiwen Lin, Ke Yan.*<br>
[[paper]](https://arxiv.org/abs/2312.08733)
[[code]](https://github.com/synbol/Parameter-Efficient-Transfer-Learning-Benchmark)

**(*arXiv2023_AdaptIR*) AdaptIR: Parameter Efficient Multi-task Adaptation for Pre-trained Image Restoration Models.** <br>
*Hang Guo, Tao Dai, Yuanchao Bai, Bin Chen, Shu-Tao Xia, Zexuan Zhu.*<br>
[[paper]](https://arxiv.org/abs/2312.08881)
[[code]](https://github.com/csguoh/AdaptIR)

**(*arXiv2023_I2V-Adapter*) I2V-Adapter: A General Image-to-Video Adapter for Diffusion Models.** <br>
*Xun Guo, Mingwu Zheng, Liang Hou, Yuan Gao, Yufan Deng, Pengfei Wan, Di Zhang, Yufan Liu, Weiming Hu, Zhengjun Zha, Haibin Huang, Chongyang Ma.*<br>
[[paper]](https://arxiv.org/abs/2312.16693)

**(*CVPR2024_ModaVerse*) ModaVerse: Efficiently Transforming Modalities with LLMs.** <br>
*Xinyu Wang, Bohan Zhuang, Qi Wu.*<br>
[[paper]](https://arxiv.org/abs/2401.06395)

**(*arXiv2024_LoTR*) LoTR: Low Tensor Rank Weight Adaptation.** <br>
*Daniel Bershatsky, Daria Cherniuk, Talgat Daulbaev, Aleksandr Mikhalev, Ivan Oseledets.*<br>
[[paper]](https://arxiv.org/abs/2402.01376)

**(*arXiv2024_LoRA+*) LoRA+: Efficient Low Rank Adaptation of Large Models.** <br>
*Soufiane Hayou, Nikhil Ghosh, Bin Yu.*<br>
[[paper]](https://arxiv.org/abs/2402.12354)
[[code]](https://github.com/nikhil-ghosh-berkeley/loraplus)

**(*arXiv2024_DiffuseKronA*) DiffuseKronA: A Parameter Efficient Fine-tuning Method for Personalized Diffusion Models.** <br>
*Shyam Marjit, Harshit Singh, Nityanand Mathur, Sayak Paul, Chia-Mu Yu, Pin-Yu Chen.*<br>
[[paper]](https://arxiv.org/abs/2402.17412)
[[code]](https://diffusekrona.github.io/)

**(*arXiv2024_Filter-Atoms*) Large Convolutional Model Tuning via Filter Subspace.** <br>
*Wei Chen, Zichen Miao, Qiang Qiu.*<br>
[[paper]](https://arxiv.org/abs/2403.00269v2)

**(*CVPR2024_DAPT*) Dynamic Adapter Meets Prompt Tuning: Parameter-Efficient Transfer Learning for Point Cloud Analysis.** <br>
*Xin Zhou, Dingkang Liang, Wei Xu, Xingkui Zhu, Yihan Xu, Zhikang Zou, Xiang Bai.*<br>
[[paper]](https://arxiv.org/abs/2403.01439)
[[code]](https://github.com/LMD0311/DAPT)

**(*arXiv2024_GaLore*) GaLore: Memory-Efficient LLM Training by Gradient Low-Rank Projection.** <br>
*Jiawei Zhao, Zhenyu Zhang, Beidi Chen, Zhangyang Wang, Anima Anandkumar, Yuandong Tian.*<br>
[[paper]](https://arxiv.org/abs/2403.03507v1)
[[code]](https://github.com/jiaweizzhao/galore)

**(*arXiv2024_Routing*) Introducing Routing Functions to Vision-Language Parameter-Efficient Fine-Tuning with Low-Rank Bottlenecks.** <br>
*Tingyu Qu, Tinne Tuytelaars, Marie-Francine Moens.*<br>
[[paper]](https://arxiv.org/abs/2403.09377)

**(*CVPR2024_MoE-Adapters4CL*) Boosting Continual Learning of Vision-Language Models via Mixture-of-Experts Adapters.** <br>
*Jiazuo Yu, Yunzhi Zhuge, Lu Zhang, Ping Hu, Dong Wang, Huchuan Lu, You He.*<br>
[[paper]](https://arxiv.org/abs/2403.11549)
[[code]](https://github.com/jiazuoyu/moe-adapters4cl)

**(*arXiv2024_SuperLoRA*) SuperLoRA: Parameter-Efficient Unified Adaptation of Multi-Layer Attention Modules.** <br>
*Xiangyu Chen, Jing Liu, Ye Wang, Pu Perry Wang, Matthew Brand, Guanghui Wang, Toshiaki Koike-Akino.*<br>
[[paper]](https://arxiv.org/abs/2403.11887)

**(*arXiv2024_LISA*) LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning.** <br>
*Rui Pan, Xiang Liu, Shizhe Diao, Renjie Pi, Jipeng Zhang, Chi Han, Tong Zhang.*<br>
[[paper]](https://arxiv.org/abs/2403.17919)
[[code]](https://github.com/optimalscale/lmflow)

**(*arXiv2024_PiSSA*) PiSSA: Principal Singular Values and Singular Vectors Adaptation of Large Language Models.** <br>
*Fanxu Meng, Zhaohui Wang, Muhan Zhang.*<br>
[[paper]](https://arxiv.org/abs/2404.02948)
[[code]](https://github.com/graphpku/pissa)

**(*ICML2024_qGOFT*) Parameter Efficient Quasi-Orthogonal Fine-Tuning via Givens Rotation.** <br>
*Xinyu Ma, Xu Chu, Zhibang Yang, Yang Lin, Xin Gao, Junfeng Zhao.*<br>
[[paper]](https://arxiv.org/abs/2404.04316)

**(*arXiv2024_MoMA*) MoMA: Multimodal LLM Adapter for Fast Personalized Image Generation.** <br>
*Kunpeng Song, Yizhe Zhu, Bingchen Liu, Qing Yan, Ahmed Elgammal, Xiao Yang.*<br>
[[paper]](https://arxiv.org/abs/2404.05674)
[[code]](https://github.com/bytedance/MoMA)

**(*ICML2024_DCRIS*) Enhancing Fine-grained Multi-modal Alignment via Adapters: A Parameter-Efficient Training Framework for Referring Image Segmentation.** <br>
*Zunnan Xu, Jiaqi Huang, Ting Liu, Yong Liu, Haonan Han, Kehong Yuan, Xiu Li.*<br>
[[paper]](https://openreview.net/pdf?id=bp8xXLi2Mp)
[[code]](https://kkakkkka.github.io/dcris/)

**(*arXiv2024_Sparse-Tuning*) Sparse-Tuning: Adapting Vision Transformers with Efficient Fine-tuning and Inference.** <br>
*Ting Liu, Xuyang Liu, Liangtao Shi, Zunnan Xu, Siteng Huang, Yi Xin, Quanjun Yin.*<br>
[[paper]](https://arxiv.org/abs/2405.14700)
[[code]](https://github.com/liuting20/Sparse-Tuning)

**(*arXiv2024_ADAPTER-X*) ADAPTER-X: A Novel General Parameter-Efficient Fine-Tuning Framework for Vision.** <br>
*Minglei Li, Peng Ye, Yongqi Huang, Lin Zhang, Tao Chen, Tong He, Jiayuan Fan, Wanli Ouyang.*<br>
[[paper]](https://arxiv.org/abs/2406.03051)

**(*arXiv2024_LoRA-Init*) The Impact of Initialization on LoRA Finetuning Dynamics.** <br>
*Soufiane Hayou, Nikhil Ghosh, Bin Yu.*<br>
[[paper]](https://arxiv.org/abs/2406.08447)


### ``*Partially Tuning*``

**(*ICML2021_CLIP*) Learning Transferable Visual Models From Natural Language Supervision.** <br>
*Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever.*<br>
[[paper]](https://arxiv.org/abs/2103.00020)
[[code]](https://github.com/OpenAI/CLIP)

**(*ACL2021_DiffPruning*) Parameter-Efficient Transfer Learning with Diff Pruning.** <br>
*Demi Guo, Alexander M. Rush, Yoon Kim.*<br>
[[paper]](https://arxiv.org/abs/2012.07463)
[[code]](https://github.com/dguo98/DiffPruning)

**(*ACL2022_BitFit*) BitFit: Simple Parameter-efficient Fine-tuning for Transformer-based Masked Language-models.** <br>
*Elad Ben Zaken, Shauli Ravfogel, Yoav Goldberg.*<br>
[[paper]](https://arxiv.org/abs/2106.10199)

**(*arXiv2022_LayerNorm-tuning*) How to Adapt Your Large-Scale Vision-and-Language Model for Downstream Image Classification.** <br>
*Konwoo Kim, Michael Laskin, Igor Mordatch, Deepak Pathak.*<br>
[[paper]](https://openreview.net/pdf?id=EhwEUb2ynIa)
[[code]](https://sites.google.com/view/adapt-large-scale-models)

**(*IJCV2024_CLIP-Adapter*) CLIP-Adapter: Better Vision-Language Models with Feature Adapters.** <br>
*Peng Gao, Shijie Geng, Renrui Zhang, Teli Ma, Rongyao Fang, Yongfeng Zhang, Hongsheng Li, Yu Qiao.*<br>
[[paper]](https://arxiv.org/abs/2110.04544)
[[code]](https://github.com/gaopengcuhk/CLIP-Adapter)

**(*NeurIPS2021_FISH-Mask*) Training Neural Networks with Fixed Sparse Masks.** <br>
*Yi-Lin Sung, Varun Nair, Colin Raffel.*<br>
[[paper]](https://arxiv.org/abs/2111.09839)
[[code]](https://github.com/varunnair18/fish)

**(*ICML2022_Head2Toe*) Head2Toe: Utilizing Intermediate Representations for Better Transfer Learning.** <br>
*Utku Evci, Vincent Dumoulin, Hugo Larochelle, Michael C. Mozer.*<br>
[[paper]](https://arxiv.org/abs/2201.03529)
[[code]](https://github.com/google-research/head2toe)

**(*NAACL2022_AdapterBias*) AdapterBias: Parameter-efficient Token-dependent Representation Shift for Adapters in NLP Tasks.** <br>
*Chin-Lun Fu, Zih-Ching Chen, Yun-Ru Lee, Hung-yi Lee.*<br>
[[paper]](https://arxiv.org/abs/2205.00305)
[[code]](https://github.com/Allen0307/AdapterBias)

**(*CVPR2023_SoLa*) Soft-Landing Strategy for Alleviating the Task Discrepancy Problem in Temporal Action Localization Tasks.** <br>
*Hyolim Kang, Hanjung Kim, Joungbin An, Minsu Cho, Seon Joo Kim.*<br>
[[paper]](https://arxiv.org/abs/2211.06023)

**(*NeurIPS2023_InCA*) Your representations are in the network: composable and parallel adaptation for large scale models.** <br>
*Yonatan Dukler, Alessandro Achille, Hao Yang, Varsha Vivek, Luca Zancato, Benjamin Bowman, Avinash Ravichandran, Charless Fowlkes, Ashwin Swaminathan, Stefano Soatto.*<br>
[[paper]](https://arxiv.org/abs/2303.04105)

**(*ICCV2023_SPT*) Sensitivity-Aware Visual Parameter-Efficient Fine-Tuning.** <br>
*Haoyu He, Jianfei Cai, Jing Zhang, Dacheng Tao, Bohan Zhuang.*<br>
[[paper]](https://arxiv.org/abs/2303.08566)
[[code]](https://github.com/ziplab/SPT)

**(*arXiv2023_LN-TUNE*) Strong Baselines for Parameter Efficient Few-Shot Fine-tuning.** <br>
*Samyadeep Basu, Daniela Massiceti, Shell Xu Hu, Soheil Feizi.*<br>
[[paper]](https://arxiv.org/abs/2304.01917)

**(*ICCV2023_DiffFit*) DiffFit: Unlocking Transferability of Large Diffusion Models via Simple Parameter-Efficient Fine-Tuning.** <br>
*Enze Xie, Lewei Yao, Han Shi, Zhili Liu, Daquan Zhou, Zhaoqiang Liu, Jiawei Li, Zhenguo Li.*<br>
[[paper]](https://arxiv.org/abs/2304.06648)
[[code]](https://github.com/vimar-gu/minimaxdiffusion)

**(*arXiv2023_ECoFLaP*) ECoFLaP: Efficient Coarse-to-Fine Layer-Wise Pruning for Vision-Language Models.** <br>
*Yi-Lin Sung, Jaehong Yoon, Mohit Bansal.*<br>
[[paper]](https://arxiv.org/abs/2310.02998)
[[code]](https://ecoflap.github.io/)

**(*CVPR2024_PELA*) PELA: Learning Parameter-Efficient Models with Low-Rank Approximation.** <br>
*Yangyang Guo, Guangzhi Wang, Mohan Kankanhalli.*<br>
[[paper]](https://arxiv.org/abs/2310.10700)
[[code]](https://github.com/guoyang9/pela)

**(*arXiv2023_EFFT*) Aggregate, Decompose, and Fine-Tune: A Simple Yet Effective Factor-Tuning Method for Vision Transformer.** <br>
*Dongping Chen.*<br>
[[paper]](https://arxiv.org/abs/2311.06749)
[[code]](https://github.com/Dongping-Chen/EFFT-EFfective-Factor-Tuning)

**(*CVPR2024_GPS*) Gradient-based Parameter Selection for Efficient Fine-Tuning.** <br>
*Zhi Zhang, Qizhe Zhang, Zijun Gao, Renrui Zhang, Ekaterina Shutova, Shiji Zhou, Shanghang Zhang.*<br>
[[paper]](https://arxiv.org/abs/2312.10136)


### ``*Side Tuning*``

**(*ECCV2020_Side-Tuning*) Side-Tuning: A Baseline for Network Adaptation via Additive Side Networks.** <br>
*Jeffrey O Zhang, Alexander Sax, Amir Zamir, Leonidas Guibas, Jitendra Malik.*<br>
[[paper]](https://arxiv.org/abs/1912.13503)
[[code]](https://github.com/jozhang97/side-tuning)

**(*arXiv2021_BD-ViT*) Benchmarking Detection Transfer Learning with Vision Transformers.** <br>
*Yanghao Li, Saining Xie, Xinlei Chen, Piotr Dollar, Kaiming He, Ross Girshick.*<br>
[[paper]](https://arxiv.org/abs/2111.11429)
[[code]](https://github.com/hustvl/mimdet)

**(*FCS2024_Y-Tuning*) Y-Tuning: An Efficient Tuning Paradigm for Large-Scale Pre-Trained Models via Label Representation Learning.** <br>
*Yitao Liu, Chenxin An, Xipeng Qiu.*<br>
[[paper]](https://arxiv.org/abs/2202.09817)

**(*NeurIPS2022_LST*) LST: Ladder Side-Tuning for Parameter and Memory Efficient Transfer Learning.** <br>
*Yi-Lin Sung, Jaemin Cho, Mohit Bansal.*<br>
[[paper]](https://arxiv.org/abs/2206.06522)
[[code]](https://github.com/ylsung/Ladder-Side-Tuning)

**(*CVPR2023_VQT*) Visual Query Tuning: Towards Effective Usage of Intermediate Representations for Parameter and Memory Efficient Transfer Learning.** <br>
*Cheng-Hao Tu, Zheda Mai, Wei-Lun Chao.*<br>
[[paper]](https://arxiv.org/abs/2212.03220)
[[code]](https://github.com/andytu28/VQT)

**(*CVPR2023_SAN*) Side Adapter Network for Open-Vocabulary Semantic Segmentation.** <br>
*Mengde Xu, Zheng Zhang, Fangyun Wei, Han Hu, Xiang Bai.*<br>
[[paper]](https://arxiv.org/abs/2302.12242)
[[code]](https://github.com/MendelXu/SAN)

**(*arXiv2023_E3VA*) Parameter-efficient is not sufficient: Exploring Parameter, Memory, and Time Efficient Adapter Tuning for Dense Predictions.** <br>
*Dongshuo Yin, Xueting Han, Bin Li, Hao Feng, Jing Bai.*<br>
[[paper]](https://arxiv.org/abs/2306.09729)

**(*arXiv2023_SAM-LST*) Ladder Fine-tuning approach for SAM integrating complementary network.** <br>
*Shurong Chai, Rahul Kumar Jain, Shiyu Teng, Jiaqing Liu, Yinhao Li, Tomoko Tateyama, Yen-wei Chen.*<br>
[[paper]](https://arxiv.org/abs/2306.12737)
[[code]](https://github.com/11yxk/SAM-LST)

**(*ICCV2023_DiST*) Disentangling Spatial and Temporal Learning for Efficient Image-to-Video Transfer Learning.** <br>
*Zhiwu Qing, Shiwei Zhang, Ziyuan Huang, Yingya Zhang, Changxin Gao, Deli Zhao, Nong Sang.*<br>
[[paper]](https://arxiv.org/abs/2309.07911)
[[code]](https://github.com/alibaba-mmai-research/DiST)

**(*arXiv2023_HST*) Hierarchical Side-Tuning for Vision Transformers.** <br>
*Weifeng Lin, Ziheng Wu, Jiayu Chen, Wentao Yang, Mingxin Huang, Jun Huang, Lianwen Jin.*<br>
[[paper]](https://arxiv.org/abs/2310.05393)
[[code]](https://github.com/AFeng-x/HST)

**(*NeurIPS2023_Res-Tuning*) Res-Tuning: A Flexible and Efficient Tuning Paradigm via Unbinding Tuner from Backbone.** <br>
*Zeyinzi Jiang, Chaojie Mao, Ziyuan Huang, Ao Ma, Yiliang Lv, Yujun Shen, Deli Zhao, Jingren Zhou.*<br>
[[paper]](https://arxiv.org/abs/2310.19859)
[[code]](https://res-tuning.github.io/)

**(*arXiv2023_Side4Video*) Side4Video: Spatial-Temporal Side Network for Memory-Efficient Image-to-Video Transfer Learning.** <br>
*Huanjin Yao, Wenhao Wu, Zhiheng Li.*<br>
[[paper]](https://arxiv.org/abs/2311.15769v1)
[[code]](https://github.com/HJYao00/Side4Video)

**(*CVPR2024_AdaTAD*) End-to-End Temporal Action Detection with 1B Parameters Across 1000 Frames.** <br>
*Shuming Liu, Chen-Lin Zhang, Chen Zhao, Bernard Ghanem.*<br>
[[paper]](https://arxiv.org/abs/2311.17241)
[[code]](https://github.com/sming256/AdaTAD)

**(*AAAI2024_DTL*) DTL: Disentangled Transfer Learning for Visual Recognition.** <br>
*Minghao Fu, Ke Zhu, Jianxin Wu.*<br>
[[paper]](https://arxiv.org/abs/2312.07856)
[[code]](https://github.com/heekhero/DTL)

**(*arXiv2024_Proxy-Tuning*) Tuning Language Models by Proxy.** <br>
*Alisa Liu, Xiaochuang Han, Yizhong Wang, Yulia Tsvetkov, Yejin Choi, Noah A. Smith.*<br>
[[paper]](https://arxiv.org/abs/2401.08565)

**(*ICLR2024_BarLeRIa*) BarLeRIa: An Efficient Tuning Framework for Referring Image Segmentation.** <br>
*Yaoming Wang, Jin Li, Xiaopeng Zhang, Bowen Shi, Chenglin Li, Wenrui Dai, Hongkai Xiong, Qi Tian.*<br>
[[paper]](https://openreview.net/pdf?id=wHLDHRkmEu)
[[code]](https://github.com/NastrondAd/BarLeRIa)

**(*CVPR2024_LoSA*) Time-, Memory- and Parameter-Efficient Visual Adaptation.** <br>
*Otniel-Bogdan Mercea, Alexey Gritsenko, Cordelia Schmid, Anurag Arnab.*<br>
[[paper]](https://arxiv.org/abs/2402.02887)

**(*arXiv2024_LAST*) Low-rank Attention Side-Tuning for Parameter-Efficient Fine-Tuning.** <br>
*Ningyuan Tang, Minghao Fu, Ke Zhu, Jianxin Wu.*<br>
[[paper]](https://arxiv.org/abs/2402.04009)

**(*arXiv2024_R2-Tuning*) R2-Tuning: Efficient Image-to-Video Transfer Learning for Video Temporal Grounding.** <br>
*Ye Liu, Jixuan He, Wanhua Li, Junsik Kim, Donglai Wei, Hanspeter Pfister, Chang Wen Chen.*<br>
[[paper]](https://arxiv.org/abs/2404.00801)
[[code]](https://github.com/yeliudev/R2-Tuning)

**(*arXiv2024_LoSA*) LoSA: Long-Short-range Adapter for Scaling End-to-End Temporal Action Localization.** <br>
*Akshita Gupta, Gaurav Mittal, Ahmed Magooda, Ye Yu, Graham W. Taylor, Mei Chen.*<br>
[[paper]](https://arxiv.org/abs/2404.01282)

**(*CVPR2024_UniPT*) UniPT: Universal Parallel Tuning for Transfer Learning with Efficient Parameter and Memory.** <br>
*Haiwen Diao, Bo Wan, Ying Zhang, Xu Jia, Huchuan Lu, Long Chen.*<br>
[[paper]](https://arxiv.org/abs/2308.14316)
[[code]](https://github.com/Paranioar/UniPT)

**(*ECCV2024_SynQT*) Parameter-Efficient and Memory-Efficient Tuning for Vision Transformer: A Disentangled Approach.** <br>
*Taolin Zhang, Jiawang Bai, Zhihe Lu, Dongze Lian, Genping Wang, Xinchao Wang, Shu-Tao Xia.*<br>
[[paper]](https://arxiv.org/abs/2407.06964)

**(*ECCV2024_SHERL*) SHERL: Synthesizing High Accuracy and Efficient Memory for Resource-Limited Transfer Learning.** <br>
*Haiwen Diao, Bo Wan, Xu Jia, Yunzhi Zhuge, Ying Zhang, Huchuan Lu, Long Chen.*<br>
[[paper]](https://arxiv.org/abs/2407.07523)
[[code]](https://github.com/Paranioar/SHERL)


### ``*Unified Tuning*``

**(*ICLR2022_UnifiedPET*) Towards a Unified View of Parameter-Efficient Transfer Learning.** <br>
*Junxian He, Chunting Zhou, Xuezhe Ma, Taylor Berg-Kirkpatrick, Graham Neubig.*<br>
[[paper]](https://arxiv.org/abs/2110.04366)
[[code]](https://github.com/jxhe/unify-parameter-efficient-tuning)

**(*ACL2022_UniPELT*) UniPELT: A Unified Framework for Parameter-Efficient Language Model Tuning.** <br>
*Yuning Mao, Lambert Mathias, Rui Hou, Amjad Almahairi, Hao Ma, Jiawei Han, Wen-tau Yih, Madian Khabsa.*<br>
[[paper]](https://arxiv.org/abs/2110.07577)
[[code]](https://github.com/morningmoni/unipelt)

**(*arXiv2022_NOAH*) Neural Prompt Search.** <br>
*Yuanhan Zhang, Kaiyang Zhou, Ziwei Liu.*<br>
[[paper]](https://arxiv.org/abs/2206.04673)
[[code]](https://github.com/ZhangYuanhan-AI/NOAH)

**(*arXiv2023_V-PETL*) Towards a Unified View on Visual Parameter-Efficient Transfer Learning.** <br>
*Bruce X.B. Yu, Jianlong Chang, Lingbo Liu, Qi Tian, Chang Wen Chen.*<br>
[[paper]](https://arxiv.org/abs/2210.00788)
[[code]](https://github.com/bruceyo/V-PETL)

**(*arXiv2023_PETL-DS*) Parameter-Efficient Fine-Tuning Design Spaces.** <br>
*Jiaao Chen, Aston Zhang, Xingjian Shi, Mu Li, Alex Smola, Diyi Yang.*<br>
[[paper]](https://arxiv.org/abs/2301.01821)
[[code]](https://github.com/amazon-science/peft-design-spaces)

**(*arXiv2023_AutoPEFT*) AutoPEFT: Automatic Configuration Search for Parameter-Efficient Fine-Tuning.** <br>
*Han Zhou, Xingchen Wan, Ivan Vulić, Anna Korhonen.*<br>
[[paper]](https://arxiv.org/abs/2301.12132)
[[code]](https://github.com/cambridgeltl/autopeft)

**(*arXiv2023_U-Tuning*) Rethinking Efficient Tuning Methods from a Unified Perspective.** <br>
*Zeyinzi Jiang, Chaojie Mao, Ziyuan Huang, Yiliang Lv, Deli Zhao, Jingren Zhou.*<br>
[[paper]](https://arxiv.org/abs/2303.00690)

**(*arXiv2023_GIST*) GIST: Improving Parameter Efficient Fine Tuning via Knowledge Interaction.** <br>
*Jiacheng Ruan, Jingsheng Gao, Mingye Xie, Suncheng Xiang, Zefang Yu, Ting Liu, Yuzhuo Fu.*<br>
[[paper]](https://arxiv.org/abs/2312.07255)
[[code]](https://github.com/JCruan519/GIST)


### ``*Posted in*``

**(*IEEE2020_Survey*) A Comprehensive Survey on Transfer Learning.** <br>
*Fuzhen Zhuang, Zhiyuan Qi, Keyu Duan, Dongbo Xi, Yongchun Zhu, Hengshu Zhu, Hui Xiong, Qing He.*<br>
[[paper]](https://arxiv.org/abs/1911.02685)

**(*ACL2021_Intrinsic-SAID*) Intrinsic Dimensionality Explains the Effectiveness of Language Model Fine-Tuning.** <br>
*Armen Aghajanyan, Luke Zettlemoyer, Sonal Gupta.*<br>
[[paper]](https://arxiv.org/abs/2012.13255)
[[code]](https://github.com/rabeehk/compacter)

**(*AAAI2023_MobileTL*) MobileTL: On-device Transfer Learning with Inverted Residual Blocks.** <br>
*Hung-Yueh Chiang, Natalia Frumkin, Feng Liang, Diana Marculescu.*<br>
[[paper]](https://arxiv.org/abs/2212.03246)

**(*arXiv2023_G-BAIR*) Gradient-Based Automated Iterative Recovery for Parameter-Efficient Tuning.** <br>
*Maximilian Mozes, Tolga Bolukbasi, Ann Yuan, Frederick Liu, Nithum Thain, Lucas Dixon.*<br>
[[paper]](https://arxiv.org/abs/2302.06598)

**(*EMNLP2023_VL-merging*) An Empirical Study of Multimodal Model Merging.** <br>
*Yi-Lin Sung, Linjie Li, Kevin Lin, Zhe Gan, Mohit Bansal, Lijuan Wang.*<br>
[[paper]](https://arxiv.org/abs/2304.14933)
[[code]](https://github.com/ylsung/vl-merging)

**(*NeurIPS2023_MEFT*) Make Pre-trained Model Reversible: From Parameter to Memory Efficient Fine-Tuning.** <br>
*Baohao Liao, Shaomu Tan, Christof Monz.*<br>
[[paper]](https://arxiv.org/abs/2306.00477)
[[code]](https://github.com/BaohaoLiao/mefts)

**(*arXiv2023_LOMO*) Full Parameter Fine-tuning for Large Language Models with Limited Resources.** <br>
*Kai Lv, Yuqing Yang, Tengxiao Liu, Qinghui Gao, Qipeng Guo, Xipeng Qiu.*<br>
[[paper]](https://arxiv.org/abs/2306.09782)
[[code]](https://github.com/openlmlab/lomo)

**(*CVPR2024_Dr2Net*) Dr2Net: Dynamic Reversible Dual-Residual Networks for Memory-Efficient Finetuning.** <br>
*Chen Zhao, Shuming Liu, Karttikeya Mangalam, Guocheng Qian, Fatimah Zohra, Abdulmohsen Alghannam, Jitendra Malik, Bernard Ghanem.*<br>
[[paper]](https://arxiv.org/abs/2401.04105)

**(*arXiv2024_Survey*) Parameter-Efficient Fine-Tuning for Pre-Trained Vision Models: A Survey.** <br>
*Yi Xin, Siqi Luo, Haodi Zhou, Junlong Du, Xiaohong Liu, Yue Fan, Qing Li, Yuntao Du.*<br>
[[paper]](https://arxiv.org/abs/2402.02242)
[[code]](https://github.com/synbol/Awesome-Parameter-Efficient-Transfer-Learning)
